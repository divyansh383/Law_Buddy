{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "groq_api_key=os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"lala\",\n",
    "    openai_api_version=\"2024-03-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,\n",
    "             model_name=\"Llama3-8b-8192\")\n",
    "database = FAISS.load_local('first__vector',embeddings, allow_dangerous_deserialization= True)\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=database.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiquert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', '', 'Tell me about different IPC sections', '--------------------------------------------------', '', 'Alternative 1:', 'What are the main categories of intellectual property law in the Indian Patent Act?', '-------------------------------------------------', '', 'Alternative 2:', 'Provide information on the various sections of the Indian Patents Act, including the scope and application of each.', '-------------------------------------------------', '', 'Alternative 3:', 'Can you give me an overview of the various sections of the Indian Patent Act, such as patentability, patent infringement, and patent licensing?', '-------------------------------------------------', '', 'These alternative questions aim to capture different aspects of the original question, allowing for a more comprehensive search of the vector database and potentially retrieving relevant documents that may not have been retrieved by the original question alone.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs = retriever_from_llm.invoke(\"Tell me about diff IPC Sections\")\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(input):\n",
    "    llm=ChatGroq(groq_api_key=groq_api_key,\n",
    "             model_name=\"Llama3-8b-8192\")\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Answer the following question based only on the provided context. \n",
    "    Think step by step before providing a detailed answer. \n",
    "    I will tip you $1000 if the user finds the answer helpful. \n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    Question: {input}\"\"\")\n",
    "\n",
    "    document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "    database = FAISS.load_local('first__vector',embeddings, allow_dangerous_deserialization= True)\n",
    "    retriever=database.as_retriever()\n",
    "    retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "    response=retrieval_chain.invoke({\"input\":input})\n",
    "\n",
    "    return response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = FAISS.load_local('first__vector',embeddings, allow_dangerous_deserialization= True)\n",
    "retriever=database.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(groq_api_key=groq_api_key,\n",
    "             model_name=\"gemma2-9b-it\")\n",
    "\n",
    "# gemma-7b-it\n",
    "# mixtral-8x7b-32768\n",
    "# gemma2-9b-it\n",
    "# Llama3-8b-8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      ">>>\n",
      "Rupees Twenty five thousand. 379 -B. Snatching with hurt, wrongful restraint or fear of hurt. \n",
      "Whoever, in order to commit snatching, or in committing the s natching, causes hurt or wrongful \n",
      "restraint or fear of hurt; or after  committing the offence of snatching, causes hurt or wrongful \n",
      "restraint or fear of hurt in order to effect his escape, shall be punished with rigorous \n",
      "imprisonment which shall not be less than ten years but which may extend to fourteen years, \n",
      "and shall also  be liable to fine of Rupees Twenty five thousand.” [Vide G.S.R. 383(E), dated 29 -\n",
      "5-2019 (w.e.f. 29 -5-2019).]  \n",
      " \n",
      "IPC Section 380. Theft in dwelling house, etc .—Whoever commits theft in any building, tent or \n",
      "vessel, which building, tent or vessel is used as a human dwelling, or used for the custody of \n",
      "property, shall be punished with imprisonment of either description for a term which may \n",
      "extend to seven years, and shall also be liable to fine.\n",
      ">>>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      ">>>\n",
      "Rupees Twenty five thousand. 379 -B. Snatching with hurt, wrongful restraint or fear of hurt. \n",
      "Whoever, in order to commit snatching, or in committing the s natching, causes hurt or wrongful \n",
      "restraint or fear of hurt; or after  committing the offence of snatching, causes hurt or wrongful \n",
      "restraint or fear of hurt in order to effect his escape, shall be punished with rigorous \n",
      "imprisonment which shall not be less than ten years but which may extend to fourteen years, \n",
      "and shall also  be liable to fine of Rupees Twenty five thousand.” [Vide G.S.R. 383(E), dated 29 -\n",
      "5-2019 (w.e.f. 29 -5-2019).]  \n",
      " \n",
      "IPC Section 380. Theft in dwelling house, etc .—Whoever commits theft in any building, tent or \n",
      "vessel, which building, tent or vessel is used as a human dwelling, or used for the custody of \n",
      "property, shall be punished with imprisonment of either description for a term which may \n",
      "extend to seven years, and shall also be liable to fine.\n",
      ">>>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "Rupees Twenty five thousand. 379 -B. Snatching with hurt, wrongful restraint or fear of hurt. \n",
      "Whoever, in order to commit snatching, or in committing the s natching, causes hurt or wrongful \n",
      "restraint or fear of hurt; or after  committing the offence of snatching, causes hurt or wrongful \n",
      "restraint or fear of hurt in order to effect his escape, shall be punished with rigorous \n",
      "imprisonment which shall not be less than ten years but which may extend to fourteen years, \n",
      "and shall also  be liable to fine of Rupees Twenty five thousand.” [Vide G.S.R. 383(E), dated 29 -\n",
      "5-2019 (w.e.f. 29 -5-2019).]  \n",
      " \n",
      "IPC Section 380. Theft in dwelling house, etc .—Whoever commits theft in any building, tent or \n",
      "vessel, which building, tent or vessel is used as a human dwelling, or used for the custody of \n",
      "property, shall be punished with imprisonment of either description for a term which may \n",
      "extend to seven years, and shall also be liable to fine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "Rupees Twenty five thousand. 379 -B. Snatching with hurt, wrongful restraint or fear of hurt. \n",
      "Whoever, in order to commit snatching, or in committing the s natching, causes hurt or wrongful \n",
      "restraint or fear of hurt; or after  committing the offence of snatching, causes hurt or wrongful \n",
      "restraint or fear of hurt in order to effect his escape, shall be punished with rigorous \n",
      "imprisonment which shall not be less than ten years but which may extend to fourteen years, \n",
      "and shall also  be liable to fine of Rupees Twenty five thousand.” [Vide G.S.R. 383(E), dated 29 -\n",
      "5-2019 (w.e.f. 29 -5-2019).]  \n",
      " \n",
      "IPC Section 380. Theft in dwelling house, etc .—Whoever commits theft in any building, tent or \n",
      "vessel, which building, tent or vessel is used as a human dwelling, or used for the custody of \n",
      "property, shall be punished with imprisonment of either description for a term which may \n",
      "extend to seven years, and shall also be liable to fine.\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"Tell me about all IPC Sections\"\n",
    ")\n",
    "pretty_print_docs(compressed_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for printing docs\n",
    "\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Rupees Twenty five thousand. 379 -B. Snatching with hurt, wrongful restraint or fear of hurt. \n",
      "Whoever, in order to commit snatching, or in committing the s natching, causes hurt or wrongful \n",
      "restraint or fear of hurt; or after  committing the offence of snatching, causes hurt or wrongful \n",
      "restraint or fear of hurt in order to effect his escape, shall be punished with rigorous \n",
      "imprisonment which shall not be less than ten years but which may extend to fourteen years, \n",
      "and shall also  be liable to fine of Rupees Twenty five thousand.” [Vide G.S.R. 383(E), dated 29 -\n",
      "5-2019 (w.e.f. 29 -5-2019).]  \n",
      " \n",
      "IPC Section 380. Theft in dwelling house, etc .—Whoever commits theft in any building, tent or \n",
      "vessel, which building, tent or vessel is used as a human dwelling, or used for the custody of \n",
      "property, shall be punished with imprisonment of either description for a term which may \n",
      "extend to seven years, and shall also be liable to fine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Rupees Twenty five thousand. 379 -B. Snatching with hurt, wrongful restraint or fear of hurt. \n",
      "Whoever, in order to commit snatching, or in committing the s natching, causes hurt or wrongful \n",
      "restraint or fear of hurt; or after  committing the offence of snatching, causes hurt or wrongful \n",
      "restraint or fear of hurt in order to effect his escape, shall be punished with rigorous \n",
      "imprisonment which shall not be less than ten years but which may extend to fourteen years, \n",
      "and shall also  be liable to fine of Rupees Twenty five thousand.” [Vide G.S.R. 383(E), dated 29 -\n",
      "5-2019 (w.e.f. 29 -5-2019).]  \n",
      " \n",
      "IPC Section 380. Theft in dwelling house, etc .—Whoever commits theft in any building, tent or \n",
      "vessel, which building, tent or vessel is used as a human dwelling, or used for the custody of \n",
      "property, shall be punished with imprisonment of either description for a term which may \n",
      "extend to seven years, and shall also be liable to fine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "Rupees Twenty five thousand. 379 -B. Snatching with hurt, wrongful restraint or fear of hurt. \n",
      "Whoever, in order to commit snatching, or in committing the s natching, causes hurt or wrongful \n",
      "restraint or fear of hurt; or after  committing the offence of snatching, causes hurt or wrongful \n",
      "restraint or fear of hurt in order to effect his escape, shall be punished with rigorous \n",
      "imprisonment which shall not be less than ten years but which may extend to fourteen years, \n",
      "and shall also  be liable to fine of Rupees Twenty five thousand.” [Vide G.S.R. 383(E), dated 29 -\n",
      "5-2019 (w.e.f. 29 -5-2019).]  \n",
      " \n",
      "IPC Section 380. Theft in dwelling house, etc .—Whoever commits theft in any building, tent or \n",
      "vessel, which building, tent or vessel is used as a human dwelling, or used for the custody of \n",
      "property, shall be punished with imprisonment of either description for a term which may \n",
      "extend to seven years, and shall also be liable to fine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "Rupees Twenty five thousand. 379 -B. Snatching with hurt, wrongful restraint or fear of hurt. \n",
      "Whoever, in order to commit snatching, or in committing the s natching, causes hurt or wrongful \n",
      "restraint or fear of hurt; or after  committing the offence of snatching, causes hurt or wrongful \n",
      "restraint or fear of hurt in order to effect his escape, shall be punished with rigorous \n",
      "imprisonment which shall not be less than ten years but which may extend to fourteen years, \n",
      "and shall also  be liable to fine of Rupees Twenty five thousand.” [Vide G.S.R. 383(E), dated 29 -\n",
      "5-2019 (w.e.f. 29 -5-2019).]  \n",
      " \n",
      "IPC Section 380. Theft in dwelling house, etc .—Whoever commits theft in any building, tent or \n",
      "vessel, which building, tent or vessel is used as a human dwelling, or used for the custody of \n",
      "property, shall be punished with imprisonment of either description for a term which may \n",
      "extend to seven years, and shall also be liable to fine.\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "\n",
    "_filter = LLMChainFilter.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=_filter, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"Tell me about all IPC sections\"\n",
    ")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'58626aeb-8f45-4be2-b37b-0d9d16755510': Document(page_content='•President INK-IT Publication Club, GGSIPU EDC Nov 2021 - March 2023\\n–Led a team of over 50 members in annual newspaper publication, ensuring adherence to deadlines and high-quality\\nstandards.\\nCERTIFICATIONS\\n•Microsoft Azure issued on June 2024\\nAzure AI Fundamentals\\n•Deeplearning.AI issued on Feb 2024\\nMachine Learning Modeling Pipelines in Production', metadata={'source': 'uploaded_pdfs\\\\Avinash_Kumar_Srivastava_USAR.pdf', 'page': 0}),\n",
       " '504d8bac-f72b-47ec-a486-0eda259a81ab': Document(page_content='Avinash Kumar Srivastava +91-9289690679\\nBachelor of Technology avinashsri1605@gmail.com\\nIndustrial Internet of Things GitHub\\nGuru Gobind Singh Indraprastha University EDC Delhi LeetCode\\nEnrollment No: 02719011721 LinkedIn\\nEducation\\n•Bachelor of Technology in Industrial Internet of Things 2021-25\\nUNIVERSITY SCHOOL OF AUTOMATION AND ROBOTICS Surajmal Vihar New Delhi CGPA: 9.14\\n•Intermediate Year: 2021\\nIndira Ideal School, New Delhi Percentage: 94%\\nPersonal Projects\\n•HealthCare AI Companion July 2023 - Aug 2023\\nGitHub Link\\n–Trained a pneumonia classifier and disease predictor utilizing a CNN framework for the Intel oneAPI Ambassador’s\\nHackathon.\\n–Improved training efficiency by up to 20% using Intel’s AI Reference kits.\\n–Conducted data cleaning and visualization on a dataset containing over 25,000 X-ray images, resulting in a 95%\\naccuracy rate.\\n•Smart Cart May 2023 - June 2023\\nGitHub Link', metadata={'source': 'uploaded_pdfs\\\\Avinash_Kumar_Srivastava_USAR.pdf', 'page': 0}),\n",
       " '3b3bdadb-4ff2-4875-97f1-386fd2d701e2': Document(page_content='GitHub Link\\n–Developed a Raspberry Pi IoT system for efficient product scanning and real time billing at shopping carts using\\nComputer Vision, decreasing the billing time by 90%.\\n–Integrated with the SmartCart App using Django Rest Framework, providing users with a virtual cart view and\\nsimplified payment options.\\nExperience\\n•Junior AIML Intern Feb 2024 - Present\\nAcencore\\n–Implemented and integrated NLP models and fine-tuned llm, deployed on both GCP and AWS Sagemaker.\\n–Optimized GenAI models by using langchain and llamaindex, improved efficiency by 60%.\\n–DevisedaresumeparsingsystemusingtheYolov5modelandanobjectivityalgorithmforresumescoring, achieving\\na 90% accuracy rate in evaluations.\\n•AI Intern July 2024 - Present\\nIndian Law Institute (IT Section, Supreme Court of India)\\n–Developed a generative AI Q&A chatbot using the open-source Ollama LLM, providing legal advice to users.', metadata={'source': 'uploaded_pdfs\\\\Avinash_Kumar_Srivastava_USAR.pdf', 'page': 0}),\n",
       " 'b11797d4-1992-4cf1-80e7-9ab268072c10': Document(page_content='–Enhanced the realism of legal advice by implementing Retrieval-Augmented Generation (RAG) on over 200 real-life\\nSupreme Court cases.\\n–Accelerated AI inferencing by 80% through the integration of Azure OpenAI embeddings and Groq technology.\\nTechnical Skills\\nLanguages : C, Java, Python, SQL\\nLibraries and Tools : SQLite , Git, VS Code, TensorFlow, Numpy, Pandas, Scikit-learn, Matplotlib, Seaborn, NLTK,\\nTableau, AWS Sagemaker, Azure AI Services\\nFrameworks :Django, Django Rest Framework, FastAPI, Langchain, LlamaIndex\\nCloud Platforms :Google Cloud Platform (GCP), Amazon Web Services(AWS), Microsoft Azure\\nOther Skills :IOT, Machine Learning, Data Analytics,Deep Learning,NLP, Generative AI\\nPositions of Responsibility\\n•Co-Lead Intel oneAPI Student Community June 2023 - Present\\n–Successfully orchestrated a tech-fest and hackathon, drawing an impressive footfall of 600 participants.\\n–Presented sessions on Intel’s oneAPI and associated toolkits, guiding 100 club members.', metadata={'source': 'uploaded_pdfs\\\\Avinash_Kumar_Srivastava_USAR.pdf', 'page': 0}),\n",
       " '3934b48c-08fd-49ec-a0c9-2a383c644747': Document(page_content='•President INK-IT Publication Club, GGSIPU EDC Nov 2021 - March 2023\\n–Led a team of over 50 members in annual newspaper publication, ensuring adherence to deadlines and high-quality\\nstandards.\\nCERTIFICATIONS\\n•Microsoft Azure issued on June 2024\\nAzure AI Fundamentals\\n•Deeplearning.AI issued on Feb 2024\\nMachine Learning Modeling Pipelines in Production', metadata={'source': 'uploaded_pdfs\\\\Avinash_Kumar_Srivastava_USAR.pdf', 'page': 0}),\n",
       " '893ae15f-0d57-43b8-beb4-812ab5fd927e': Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " 'cdd90760-d218-425c-aeed-d078c974a9f5': Document(page_content='entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '22de34d9-3e76-4533-98b9-42151e6334b6': Document(page_content='has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '91341b56-36ab-4bb8-8a95-f2feb8f1ed0f': Document(page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '8594f9c7-d62e-4470-9037-12442e06a86a': Document(page_content='significant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'bbd85803-19d8-48f1-bb37-9729eb381b8a': Document(page_content='2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '0af87bd6-5005-4e59-a4e5-0563f62dcda0': Document(page_content='of a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'b47dceef-cc0c-4d06-a9a4-058e746230db': Document(page_content='Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'c5c456b2-46d7-448a-b6eb-2300171baf01': Document(page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " '005e47d0-f41c-406a-814c-c724237ce0fa': Document(page_content='Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " 'dfca1445-6f97-4725-90b5-96215619a557': Document(page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '5d939a13-e324-4603-9a84-f9ca9f14fbb1': Document(page_content='√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '0765a072-7653-4a73-a3c1-cde8e84ec62c': Document(page_content='Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '7f632bbe-9682-42f6-b693-ed4c45df35c1': Document(page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " 'd09c61a5-b3ef-4436-9475-94454949da92': Document(page_content='and the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '4cc3351b-429b-4da9-a7c1-4d62846426d9': Document(page_content='of the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '5653da3b-1b82-402b-a705-ebc02f3fff6e': Document(page_content='mation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '0b18fec7-24c6-43b2-9648-bcfc2a913d14': Document(page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " 'bdc2b58e-618a-41b5-8fcc-23a38686d0c5': Document(page_content='as the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " 'ff9c501c-56a1-4af4-8e53-f2a04dba30f4': Document(page_content='In this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '094dce5f-2179-4048-bab7-226e6aa5bff1': Document(page_content='and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '004e9fef-717e-4f7d-8bac-830b2c3c84c7': Document(page_content='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '4547a486-9a12-435d-bb6e-eb8e49e760f7': Document(page_content='recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " 'd241f119-2dba-42ee-a30e-4b89cb318b45': Document(page_content='target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '29f3cb59-1e75-4d5a-b389-bdbeb7c9b33d': Document(page_content='lrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " 'ac50a86e-5eac-4604-8c97-c81749adaa34': Document(page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '111edc7d-e1f2-4459-a78a-e124fc44285f': Document(page_content='Pdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '64ada126-5818-4559-9bd3-a40b98a7bb3b': Document(page_content='previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '84ee2084-cbc3-4273-8f4d-9868a288483b': Document(page_content='To evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " 'f10fc757-d4f0-4757-a84b-023dd26271ec': Document(page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " 'b09ad48e-2c72-43a2-b82f-2dde21af53d6': Document(page_content='checkpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '477e80c9-42c3-479c-b7c8-e67042cfd00c': Document(page_content='To evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " 'bc7f05c9-575c-4499-af48-00957877cddc': Document(page_content='(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '15b75b3d-62bc-4c0c-8e20-1fa834aa08ec': Document(page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '38de5ccb-2c0f-45e5-8611-9140a39e5b07': Document(page_content='prisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '8a6e2982-b37b-48b3-8418-fc3130b10994': Document(page_content='plan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'a111d2bc-c11c-47cf-9dee-5ee2f4aee925': Document(page_content='[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '6a239a94-8a9f-4b3d-8ddf-7ad50e943b51': Document(page_content='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '43d61ff8-a2a2-47ff-8bf1-2005d7f43f1e': Document(page_content='[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " 'dc18e2e9-431b-4020-9783-f7be8b856c92': Document(page_content='Information Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '6e576912-83a6-4d8b-8fa3-cb1c4abf0ccd': Document(page_content='[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '1d51e83d-762b-4413-bbb9-7d2dedba1e90': Document(page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '6e4e4fd9-bb67-4ce7-88d6-a4786dc2381d': Document(page_content='2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '06cfcb2b-f85f-4a80-921e-75b158450cef': Document(page_content='Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '7f901488-e19e-45fe-b236-2c6749f9fab3': Document(page_content='fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " 'bd0087fa-c26a-4d94-88dc-f048f3ddf1be': Document(page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 12}),\n",
       " '3e321e4c-1c77-4dca-8567-9d1489b7faaa': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 13}),\n",
       " 'f82143d9-263b-442d-a444-5b7cdab9a3cd': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 14}),\n",
       " 'c2e1d7f5-96d0-4e11-9247-ce9b37619ecb': Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '956e9439-2c45-4956-8b82-1deec750ee07': Document(page_content='entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '30a626cc-3964-443f-b51e-3a3eb71c845c': Document(page_content='has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " 'c0115c7d-024a-4cd0-aa2d-02c1699dc20a': Document(page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '01e170ed-b896-4f4b-b419-47be7415c3e5': Document(page_content='significant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '54207502-c426-4fea-9e3a-61957d20dec4': Document(page_content='2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '717e7872-9d0e-43ab-98c0-6d8836ac31d9': Document(page_content='of a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'bd1e68a4-f158-4ae0-b649-df4e10f89cf8': Document(page_content='Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'bb48e0c4-244f-4f7c-9ffa-970063e0b146': Document(page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " 'f71c91b6-7570-4836-a6c3-48c309176c78': Document(page_content='Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " 'cd3ff585-a359-4ff3-b34a-571d76ac2484': Document(page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '2f18e4dc-06ab-4e3d-9db2-42a2c5177fc4': Document(page_content='√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '7137a89e-1d50-4e8f-a8e0-16023f0e226a': Document(page_content='Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " 'b50f2b65-b853-4dcf-8185-e6eb0f931034': Document(page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '4dfbd78d-aae9-4126-a430-b60d06857e71': Document(page_content='and the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '2d69b41c-3f86-468b-b5b6-d9484f200104': Document(page_content='of the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '22b34236-ce8e-419e-ba52-9309b4a6aff0': Document(page_content='mation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '48ecb9dd-3b45-43c7-8dca-9b1317283726': Document(page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '99d7d2c9-98e9-43f3-9dc7-11b029509c5a': Document(page_content='as the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '053395ad-c911-4a0f-8603-cfdc43c742ed': Document(page_content='In this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " 'b45224ca-5869-4892-9102-9e43f30a63e2': Document(page_content='and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '417650c3-da94-4351-a1d0-20d190d91088': Document(page_content='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " 'e8604566-98e9-493b-be62-02d34f863f7a': Document(page_content='recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '259fb5a5-356f-4e2e-b0ba-9c6d000c8c04': Document(page_content='target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " 'b3333257-d50d-4f87-af4c-4aaeb8ef2ad2': Document(page_content='lrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " 'e240c8ff-d547-4fc1-a140-48e53ed98e79': Document(page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '2c278890-d238-4a08-9b88-8e6df1a6f311': Document(page_content='Pdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '344dd1f5-58bd-44c5-aed5-de1400f71a6f': Document(page_content='previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " 'd1f83b97-c8d5-4bb4-bb7d-a2cf6aaff637': Document(page_content='To evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " 'f475ce78-cc7d-4344-9c0d-93b1a3090fc2': Document(page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " 'dbba8e83-b569-41ca-86f4-522cb32dc4dc': Document(page_content='checkpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '083da622-159f-49e3-8d9b-12b5c69e4df4': Document(page_content='To evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " 'b03e0f77-3cfb-4dd2-981f-4a08bf3aef82': Document(page_content='(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '8e1020a8-ee36-48af-b4f5-f542c6d76001': Document(page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '57d0d8a4-00e9-4d7d-9adb-703a835206c5': Document(page_content='prisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'aaf43d72-9781-4c95-b237-2aeda751f056': Document(page_content='plan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'db65ac15-15cd-4306-af41-3b66352152c6': Document(page_content='[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '4c835dc0-df12-44e9-a902-54251d0adb76': Document(page_content='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " 'e4aef05d-11b1-4a0f-841a-07c62ed40e3c': Document(page_content='[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " 'e1c28200-8a90-4ef3-adcc-d2b960a358c3': Document(page_content='Information Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '60416073-6e95-4b0e-8247-01bc25db8e24': Document(page_content='[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '693a8182-0715-4b7a-bd33-fac317802a75': Document(page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " 'e7416507-773d-497b-9d85-b49a08d5966b': Document(page_content='2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '9403dc06-e776-42a3-a799-1d4e27b7084c': Document(page_content='Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " 'bfcf3961-03ea-424c-b9a4-553c8fb8f75c': Document(page_content='fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '85229ba9-e18f-4fb3-8543-ab023832acd5': Document(page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 12}),\n",
       " '9b99ac18-9b6b-46fe-985e-33b67bc5fe83': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 13}),\n",
       " '03060336-0e7e-4534-99f9-ae56d408dc1e': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 14}),\n",
       " '9577031a-a810-4e16-88ae-f0fcf4190d7f': Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " 'b22ddce4-4b44-46da-b539-77bc1a8478ca': Document(page_content='entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '89bf421f-8627-4375-b6c7-18a2a5b328d6': Document(page_content='has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '5dec24e4-4788-464a-b130-cde1dc3471fa': Document(page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '61002609-38da-471a-8f38-ce17db69a1b5': Document(page_content='significant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'be9b4fee-0f68-4e99-bb81-42ca23fbb8fd': Document(page_content='2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'b0553d8e-c9eb-423b-bb1e-c64c5407cdd5': Document(page_content='of a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'b418df7a-0fcc-41d4-bbf0-aba8a7c3f1f2': Document(page_content='Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '80e07537-747c-4f2b-9ff0-e1b3f7372b76': Document(page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " '5593efa4-e585-4f55-a60e-62cf752db6c9': Document(page_content='Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " 'cde1d14c-0b47-4011-af4a-86bbcce5fa4e': Document(page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '3c26fd3a-4ea4-4dd2-a08e-c893ba0ef1ef': Document(page_content='√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '26facdc7-35dc-4e22-9c63-fa9cca56d28a': Document(page_content='Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " 'f8fd753a-e86e-4d67-b09d-870b3b00f555': Document(page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '47a06a11-dd6a-4dd0-a5d1-ac1f8c82763f': Document(page_content='and the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '10dfcac4-2285-4c82-8955-e0f91b73fe87': Document(page_content='of the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " 'dc55a7f6-bd7b-4d2c-93ac-7a3366eadd75': Document(page_content='mation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '2241b081-1af3-4d67-86a9-69f307d4871d': Document(page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " 'fcd9f77f-5302-454d-9bb2-f1503a1c45aa': Document(page_content='as the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '5de925ee-cc2d-48dd-9ddd-a8d4cc837ac9': Document(page_content='In this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " 'c579f4d5-56fa-4995-ba06-2ba858a09f77': Document(page_content='and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '4ce206de-057d-40e7-96c3-93597ac0ce73': Document(page_content='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '324da78c-3b2a-4457-b3c7-6d96581d01b5': Document(page_content='recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '78b57927-3a6c-4cfb-979b-68920a4b5488': Document(page_content='target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " 'acf3fa7d-c837-4f28-90da-5481e5b1b9b8': Document(page_content='lrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '60074435-9a55-4694-85ed-4d60c2f7c837': Document(page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " 'be260589-587f-42e6-ad10-048cd4218ff7': Document(page_content='Pdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " 'af18997a-5a6d-4708-954f-6e56ce13f9de': Document(page_content='previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '369aa03d-01d1-41f5-a0f4-ba4acaf975d1': Document(page_content='To evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '4993d564-186a-4a09-a877-bd1ffaf25ecd': Document(page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '2a4008b9-b5a1-4b9a-8146-49f09df70261': Document(page_content='checkpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " 'c2c9e436-7cf7-4b08-aa58-daf498e72928': Document(page_content='To evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '0c83f804-7c82-4dcb-9808-219987fc135b': Document(page_content='(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '4f78f8c1-2f58-48c6-a3a7-c08a18837814': Document(page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'f3868ab9-cdf8-4c84-8a80-dcd1598aa0c7': Document(page_content='prisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '7a99f954-32a3-4e85-8086-c68510d308f5': Document(page_content='plan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '92b0b3ce-48c3-4a35-b3f4-8fd457a72995': Document(page_content='[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '218003e2-de42-438a-8313-4f60bd262231': Document(page_content='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '0ebd034a-f17a-44e8-b1bd-bc38a26de81f': Document(page_content='[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '99d412ce-7ec0-4081-a2ea-27b2edf5b2b6': Document(page_content='Information Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '6fd0a9aa-06d0-46f2-9234-acf04fea25d7': Document(page_content='[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " 'eccc9abd-6c28-4f44-93d6-be2081e87529': Document(page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '122a13fb-f813-4a6a-aef5-022932cff7b3': Document(page_content='2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '4acc3ff8-2660-465e-b361-3965121f2179': Document(page_content='Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " 'aa7d70e6-4e15-4e05-9392-5a5967d93192': Document(page_content='fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '1a0011b3-abbf-4e30-8e21-68ba6fbfe0c0': Document(page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 12}),\n",
       " '5b336b4a-b93c-4a3f-8a14-c6b641cf9a8c': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 13}),\n",
       " '37773565-cbab-4661-84da-99d641d48f36': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 14}),\n",
       " '52dcc8aa-d919-4282-807e-1fd3c49f201e': Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " 'e179df82-b327-43ac-a4d5-461e4c493597': Document(page_content='entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '36c0e7ee-1102-4059-957a-8cf656611fc7': Document(page_content='has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '5ed1833d-5115-4696-8acd-a26d7a9e1498': Document(page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '2b78da3a-71de-4493-b4d2-bee4b0f6346c': Document(page_content='significant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '7a3e6beb-4103-4f95-ad8c-8e5ad1637701': Document(page_content='2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '8833c062-eeed-4125-8d21-85cc394eeeb2': Document(page_content='of a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '58aebbf5-dd87-4fd7-be4d-4186180306c9': Document(page_content='Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '950895c5-8da2-4989-ba32-51b38a82e174': Document(page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " 'b8eae3d1-d324-4e25-b446-cee426eb8814': Document(page_content='Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " '09ae565e-a2cb-4668-8246-c6d1fd460fbf': Document(page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " 'f0930991-afa6-4fb0-8a91-cb32f3b9d94d': Document(page_content='√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '8306acf6-b8b0-4310-a22b-006328f65c4c': Document(page_content='Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " 'ed0fc7f3-bfc0-4a30-847a-9d38657a1720': Document(page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '8d5ac3cb-c4e0-4e3b-9c1a-0ecac9192b8b': Document(page_content='and the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " 'bbb1a437-2624-494e-98fc-2ae9cc816774': Document(page_content='of the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '67ad7143-7300-45d5-8725-4c019f6c6be9': Document(page_content='mation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " 'fefac4d3-a080-446d-91cd-c89391266d87': Document(page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '696ee222-5625-47d2-b403-618603bd3ade': Document(page_content='as the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " 'a4923f48-e754-464a-8734-95f44c50549a': Document(page_content='In this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '9f13e0e7-4a5e-4c6e-8d81-eaf2178be250': Document(page_content='and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '3fa9a1f7-02d4-48d3-9681-bd89752c6237': Document(page_content='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '6c58247c-80bd-4461-94aa-d5ef2606a210': Document(page_content='recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '4577ed19-d193-4d1d-81a3-a29c73d4770a': Document(page_content='target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '95ae61f2-ea1f-4db2-9cc3-2a5d0ecad65e': Document(page_content='lrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '18ca4a88-20b2-457f-8b7f-707818d4af4e': Document(page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " 'c5783fa1-f0fa-4593-be30-a48319316f9e': Document(page_content='Pdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '43091dde-7184-44be-869c-6e8eb30f00cc': Document(page_content='previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " 'df4a4e14-00ec-4139-ba2f-3a14741ada9a': Document(page_content='To evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " 'b971d1b6-b773-4e5d-8b2b-6d1220c3e98b': Document(page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " 'c9ffd9d3-baec-4d6b-aee4-1c3fb21bda44': Document(page_content='checkpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '16887982-bdc6-42a9-9125-9855166853d6': Document(page_content='To evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " 'c4dd5853-21ea-4687-809d-5f6963178a10': Document(page_content='(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " 'ea86aa24-a2e7-4a6e-8d2d-98dfce36d66e': Document(page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'd496ec1f-1315-498d-9751-6a135c7fba5b': Document(page_content='prisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '7cc38e5f-5d87-42d4-98bd-f4f9b9cbe2cb': Document(page_content='plan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '39e4f593-a65c-43d8-9f15-e55466df29ee': Document(page_content='[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '30819d2f-cc1c-4623-bf84-70554669ad99': Document(page_content='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '7fbff810-b231-4a6f-8c87-334e980adafa': Document(page_content='[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " 'cf598af4-e070-436d-b5a2-58cba3c3217f': Document(page_content='Information Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '458af25f-6a58-41b2-b95d-7008a341a51a': Document(page_content='[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " 'b7e1e62c-3306-4374-9327-f4cd5b47185b': Document(page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '06476b76-5798-4204-af67-057bad89ec9e': Document(page_content='2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '35877662-4a08-481f-91c4-6e8e8f8674e5': Document(page_content='Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '5e0f914b-22bb-413b-8ddf-b7088b25cab2': Document(page_content='fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '2cd083cf-1f4c-42d0-866c-7e9fefa4f046': Document(page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 12}),\n",
       " 'a2f974f7-194f-4114-af99-d68640bd0787': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 13}),\n",
       " 'ce12bcd0-a1cc-46e2-b2d0-338e67ac9012': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 14}),\n",
       " '0a15a428-b195-4447-bda3-4a3b2094cfe9': Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '6ee4f325-182d-42a2-bfae-31be9f99d965': Document(page_content='entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " 'b6b29ad9-901e-4e29-b636-a58367937f11': Document(page_content='has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " 'fc6288ad-9bf1-4478-91b3-7a5d1d90269c': Document(page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '6b2649bb-09e0-4d61-ad26-81c9705114b2': Document(page_content='significant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'eded4126-8dec-4971-bc02-4dcd672dc89e': Document(page_content='2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'af15c453-d984-408f-8292-12bee68c191e': Document(page_content='of a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'bd58fba3-689b-4ca8-80ee-36e49ad56abb': Document(page_content='Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '1b1c53e7-8f24-400f-99cc-9164120684c0': Document(page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " '455e37d7-221a-4ef3-ab1f-fe1484d95ca1': Document(page_content='Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " '39d36fff-8cc7-42a3-9f56-ed240a076ff2': Document(page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '0da3244e-27f5-49ff-b528-e5aa6631e7ad': Document(page_content='√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '052ba440-98ca-4dbe-9d96-80a886828aca': Document(page_content='Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " 'e576f2d4-9f5c-43b0-a642-d11984bc34ab': Document(page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '53fafb59-fd7a-4ff6-b187-2026e12c409c': Document(page_content='and the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '40510eef-571a-495b-92c7-755f0ee8d9e9': Document(page_content='of the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '23a038b6-e632-4915-a8c9-783d3cb4e228': Document(page_content='mation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '64487686-1de2-4bcf-95ab-66627e893b54': Document(page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '4f00b520-a05f-47f9-aeee-2eb881c79e23': Document(page_content='as the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '4dbb08c6-cd66-4167-a96d-835e71790459': Document(page_content='In this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '61a96ce8-7f83-4449-96bd-ff63131b0699': Document(page_content='and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '96c234b9-4f4e-40c0-8aa0-f7d925527d07': Document(page_content='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '7eec43bf-22fc-4150-8e40-12ca309a20f9': Document(page_content='recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '8115ce91-b346-4686-8c30-b443fc28f7e9': Document(page_content='target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '86ef01d2-72dd-471d-8167-e9b3eb3249f5': Document(page_content='lrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '72e98473-8cc2-4c1b-b89f-eb0c063e1eba': Document(page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " 'd8db8a20-aa27-49fb-ada1-63821c18844a': Document(page_content='Pdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '8b5aa9fa-982b-4243-8b61-1482b1c780ff': Document(page_content='previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '38b7dcd3-65a4-49c2-8fd9-2db48395d76b': Document(page_content='To evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '62b57315-8bc4-41d4-be6e-401cb7b1f802': Document(page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " 'f7a4f881-2332-467b-8263-785a1b2b1479': Document(page_content='checkpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '610c6e83-be76-415d-837f-9b232faeda03': Document(page_content='To evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " 'f9abc18a-9140-4ce1-9a54-2a31b7d74d98': Document(page_content='(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '6b23f5b2-eb03-4631-9c09-d018846925fe': Document(page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'd3957c0f-5d2b-4bfe-92e1-3e9374169414': Document(page_content='prisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'bc335a99-3a49-4cb0-a229-e84dc8e96cc0': Document(page_content='plan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'ccaaa2fe-9c55-4869-843a-0f73e8e98a2a': Document(page_content='[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '3bc63a0e-e0a5-4522-aa9f-ac9c5b594f3d': Document(page_content='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '645871aa-c3c9-4fc0-88c9-7175906b1720': Document(page_content='[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '60c53602-2c68-4a76-982c-14087facfd36': Document(page_content='Information Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " 'd87081f6-deba-4f22-b7c7-869ecf75b228': Document(page_content='[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '8a0358f6-5291-49e8-bbda-51590c041991': Document(page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '9062725d-a73a-4503-8c14-9f39897ac8a4': Document(page_content='2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " 'ba7c4cf6-3861-4620-8d39-71919033c03d': Document(page_content='Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " 'b3e2bda2-82c1-4cc1-9fe3-27a7265e0abe': Document(page_content='fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '8f05a6e9-1882-4fb4-a572-0327c80cf324': Document(page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 12}),\n",
       " 'be22c8a5-6e2b-4063-91d3-981114c62eef': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 13}),\n",
       " 'fcd97e93-b60e-4cc9-b3be-fb65f8b600dc': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 14}),\n",
       " 'ad9bd4e7-94c4-47df-9d69-91a2c45a667c': Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " 'f5ba6891-925d-4c7b-af10-7dad9a9fa695': Document(page_content='entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '5fc5fdd6-e4cf-4e87-ba28-3b3898f8486a': Document(page_content='has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '43f012d3-77c9-418b-9569-b54bcd753daf': Document(page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '1a10eedd-ad86-4a9e-ade4-24255381f4bd': Document(page_content='significant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '5a0d23a1-31a8-4ca2-a97f-c09180311061': Document(page_content='2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'e629e6dc-1677-448a-8a8a-52c7a90bb674': Document(page_content='of a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '09752eaf-11f5-4cf4-b414-a3875071c94c': Document(page_content='Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '5fe43334-a8d3-41d7-a68c-2d34489ad4c7': Document(page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " 'dae55f3e-335d-4fa0-99c8-87e9fd5da170': Document(page_content='Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " 'c85d1dc3-7673-4b53-b55f-02e9f8bb66d9': Document(page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '462fa09d-e035-4c71-96ae-d2c4a59f0c8b': Document(page_content='√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " 'aacd17a8-0a64-4089-8c78-75282854bb91': Document(page_content='Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '41aef0e5-fa5d-45dd-8471-1f3769660bb8': Document(page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " 'c1b8d65c-8d2e-48aa-b6ff-aa8a9a25b33d': Document(page_content='and the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '59a21938-387c-4f82-a7ff-9a0ca9da3e0a': Document(page_content='of the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '0c399f2c-3c3a-4fcc-a002-e9b2d882e5a5': Document(page_content='mation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '5ca3d075-635f-4120-bb51-f4fd1a22b709': Document(page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '6f9fc1fb-be7b-4b53-9501-9a06d0408066': Document(page_content='as the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '30983bd4-4310-4c9d-ab4b-1a347e79f99f': Document(page_content='In this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '6b731797-adc2-4dc8-a69d-803c1481b7c2': Document(page_content='and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " 'adf1301d-05fd-403f-a123-e2ec2add7f0a': Document(page_content='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " 'c5ca3718-ad5f-46f0-b94e-56da9dcf8924': Document(page_content='recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " 'd40300db-26ea-496f-a93e-62acf8b1a80c': Document(page_content='target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '32a8e626-ad3f-4a9c-8c88-fcd9a2ea96e6': Document(page_content='lrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '2fbddab3-d506-417e-a35b-37187d27e232': Document(page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '1077f9c6-7d5c-4d5f-aa05-2a3b6c2266fa': Document(page_content='Pdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '43b8c33e-b95b-4bff-8939-1ece7aa7fcbb': Document(page_content='previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '67d94398-a859-4896-99a5-b3a403148e77': Document(page_content='To evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '6bd32055-92fe-48c9-8e21-c3f10fbb10e7': Document(page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '1d440145-3d08-4b05-a395-be4842723e53': Document(page_content='checkpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '0d8e3f09-8e1a-483c-a5ee-6bb4fb6b373b': Document(page_content='To evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '25f55efe-e34f-4ba2-8e1c-d1f830b15f77': Document(page_content='(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '844a8750-212e-4ba3-a046-c07614046a19': Document(page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '93236802-e653-4418-ba08-1f40f17ead94': Document(page_content='prisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '9f8542d8-9c9b-49dd-9194-ef4815cc6726': Document(page_content='plan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'c8a5588d-2608-4d74-81f7-7e12f0e09ffe': Document(page_content='[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '8238d715-fee1-4fcb-9c93-26364dc357fe': Document(page_content='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '26fdfdc0-73f5-4069-ad53-82e37cf758ae': Document(page_content='[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '2cb0d9aa-1339-49f9-b7a0-3d8d188a7d4c': Document(page_content='Information Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '805e26b8-721d-43e1-83db-3e3bac7dfc9d': Document(page_content='[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '052a8880-37a4-44db-ae90-4122bdf800db': Document(page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " 'bcdd954b-831a-4c81-ac38-07467cc17fb0': Document(page_content='2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " 'f09bb1c8-5c58-454a-9cb8-937e0cd56500': Document(page_content='Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " 'e4ebf279-4a4e-4c20-8927-47a1d523e118': Document(page_content='fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " 'e6021ae1-ad3d-4996-b45f-004ed948008d': Document(page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 12}),\n",
       " '69924e3c-9607-4643-92bf-93126af3be1b': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 13}),\n",
       " 'ce044195-5e22-4bc6-8547-421e0514765e': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 14}),\n",
       " 'e20e619f-1dfa-4f1a-b1bd-8242747f44d3': Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " 'a4ef67c5-3c39-4e1a-a0cc-4fd8745dc0e2': Document(page_content='entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '2268acb8-ec4a-4a12-897d-9d1aa9192514': Document(page_content='has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '0c30d1ee-a2b3-4913-8dc2-37744437bdba': Document(page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'f84d7c5a-be11-4a81-97c5-37789727bdbd': Document(page_content='significant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'c74e1b62-cb1a-4018-989e-b5ca528a68fd': Document(page_content='2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '61064687-8988-4faf-b473-96e045caa770': Document(page_content='of a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'b670816c-d838-4bb5-89e9-d439d5a7f3e3': Document(page_content='Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'e03a2d86-1d40-4040-bc09-0b963b1fb5dc': Document(page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " '3397eba5-d9be-41cb-aff7-7ef5d44f4530': Document(page_content='Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " '2c71159b-87c6-452f-a2b9-f803388705b7': Document(page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '9b9304fe-047a-4a45-919a-377207072bdd': Document(page_content='√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " 'ac661a56-aadc-47f8-8645-87596c482728': Document(page_content='Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " 'a6fb4714-669d-4b1f-8369-3dc8ebe85cc2': Document(page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '92345892-e626-49f0-be30-024809f64e64': Document(page_content='and the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '535ed7bd-f1ba-4856-a9c0-2ddfa0d02241': Document(page_content='of the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '6bb9583a-7132-4005-882c-db8458bf3e9d': Document(page_content='mation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " 'cb630885-352c-4557-b82c-abc22641f57e': Document(page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " 'd67daf08-6fc3-44f2-b62e-e7a61657ecbe': Document(page_content='as the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " 'd59593ee-fea9-4326-8537-95ae48dd25c2': Document(page_content='In this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " 'a6d7f5b9-a756-4f75-a2a9-f47410228281': Document(page_content='and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '43694f45-db9c-4744-a274-9b9eb44c35f2': Document(page_content='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '82eaafe4-372f-413a-9120-84ddd33e4054': Document(page_content='recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " 'eddc4d55-fc11-4597-b4ad-b7bbadfeec34': Document(page_content='target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " 'c7df8580-0b48-4dca-939b-6d1de99f8dba': Document(page_content='lrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " 'eb3b003f-4b54-4242-ab45-6feafb36d263': Document(page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '3c03c72a-940e-4f16-b374-754fb909735a': Document(page_content='Pdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '1521f301-f05a-4d5e-a1cc-2e26ccd476c5': Document(page_content='previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '5fb79770-0e6a-4be9-907e-05cd3cb8bcb2': Document(page_content='To evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '4ae386a3-af52-4a13-bc10-54badbe0349c': Document(page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " 'c8569b5e-8bb4-4dfe-873c-4bd0201a4ed4': Document(page_content='checkpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '0d4238cc-f50d-40a2-90ad-99c6c7c9e501': Document(page_content='To evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " 'e2e19374-fe2b-4d51-80f5-eedfe48b6652': Document(page_content='(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " 'ea5c56eb-1c43-46b3-bc40-67f6551e2682': Document(page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '3da1e440-3ce7-42da-9d6c-178d42955ce3': Document(page_content='prisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'b6c0030f-d23a-4de8-aeb6-5bcb8c0ded9b': Document(page_content='plan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'fe7b81d5-8bbf-46df-92ed-d77429f7deaf': Document(page_content='[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '59e3cad5-f150-47d7-9615-119baac96fd0': Document(page_content='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " 'e33fa5ca-8fb3-4576-b8e9-ea64f2a38c80': Document(page_content='[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '0dc4e734-4144-453f-9434-09f385aa851c': Document(page_content='Information Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '1151afb4-831a-4beb-a951-7d6ff2ceb824': Document(page_content='[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '7016f72d-c8d3-4d31-94fb-e6dd386785b3': Document(page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " 'b888bbdd-c96f-487e-9d60-a22140b67a41': Document(page_content='2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " 'a411ffbc-82e2-45c4-8690-2173a33de4d8': Document(page_content='Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " 'e7c015a7-7881-4d1a-b835-cbcb3f35f00b': Document(page_content='fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " 'cf984bcd-3590-4a0e-b67f-66b6bb2d23b2': Document(page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 12}),\n",
       " 'a8f90a0f-6025-47af-8524-79f7264df007': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 13}),\n",
       " 'c495e688-1af5-4f93-bf6f-fe5948b7a5e3': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 14}),\n",
       " 'd9492259-b134-449b-ba76-f03b95d83244': Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '7cf26d97-54ad-464e-97f8-c6ce62774936': Document(page_content='entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " 'b1757d4d-52a4-4897-9884-141bad3957dd': Document(page_content='has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " 'caa5dbcc-fa8b-4b46-8754-e6723d80b1bd': Document(page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '6ca09cdf-eb9a-4ed2-8a48-3a0a260255bf': Document(page_content='significant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '530a8b6e-fdd4-43ed-aa94-4f7fa9a9215c': Document(page_content='2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'a062c98d-353b-44ad-9583-07e281c35bf9': Document(page_content='of a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '9f4bab76-4d7b-407c-ad4e-748202fec603': Document(page_content='Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '33c18247-4312-44d6-9385-89d652f21606': Document(page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " 'f088d49e-a9fc-465a-b863-00f5bdb17b02': Document(page_content='Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " 'eb24b63e-fc2c-44e8-8125-9a2356d530fb': Document(page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " 'df85b104-b2b6-44b9-8b5e-1b489b85fbfb': Document(page_content='√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '291f6b23-ac8d-4b37-8867-34237847e54e': Document(page_content='Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '12016d13-9924-4237-82bd-b94e0ae0d8a1': Document(page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " 'f599f811-06d9-48c5-8693-ad107ca3b80c': Document(page_content='and the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '2db486ec-d124-44bc-b760-86d9f75f44dd': Document(page_content='of the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '1158d185-3d3f-4268-bc44-b7e9d414f406': Document(page_content='mation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " 'd8de9f88-ae2f-438d-bd67-37ece3842b59': Document(page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '57028802-8009-435d-b227-9b639055e6d8': Document(page_content='as the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " 'c15fc7a1-ecfd-4b94-b901-620728fc0773': Document(page_content='In this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '5889438e-c779-4457-ae1b-b21ddfbc8303': Document(page_content='and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " 'e77d0257-9256-4b05-af02-f0f5f35e3602': Document(page_content='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '508e6a86-3624-48a1-baef-bcefb9059046': Document(page_content='recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " 'b1710967-fef2-4126-8fd5-bdf8121473c2': Document(page_content='target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '2ac06a9c-27d3-4433-b4cd-928d736a95b4': Document(page_content='lrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '44a1be80-e69d-42ab-ad40-bc00c84fa678': Document(page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " 'e3a5c8d3-103a-4156-bfad-a3297e2181ad': Document(page_content='Pdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '7c0be314-78c1-454a-adb6-35cf6b4057d3': Document(page_content='previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " 'bc8cba90-0570-436b-8f64-c7c3c384f43f': Document(page_content='To evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '017235fc-1878-4b30-b7e4-84fed190ae08': Document(page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '4b673e04-0e88-4542-b6d3-f9992c96904c': Document(page_content='checkpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '6a1c5427-e970-4342-bc3e-a486659bfe11': Document(page_content='To evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " 'db8d61c7-3745-4804-9053-a60e8a6089cb': Document(page_content='(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '65017dd5-d832-4eee-91ff-ee4d755e4732': Document(page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '54ca2463-2b73-4aa2-aee1-bad70fd61f5e': Document(page_content='prisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'e83f9015-8a34-407c-8ee6-a92359d0833b': Document(page_content='plan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '9cacb015-04b3-433d-bb11-656a38a5d077': Document(page_content='[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '8fe170df-c47b-4f70-ae43-d06ab756ccbc': Document(page_content='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " 'cd3190db-c726-4642-810e-eb57c0226138': Document(page_content='[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " 'f9b66d83-2585-4de8-aec7-4b827202bb6a': Document(page_content='Information Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '1b5716d8-8815-45cf-aaa4-b7219effcc90': Document(page_content='[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '989b102a-8e5e-45d7-8a06-84a05fbe7e09': Document(page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '6a190b45-c257-4055-80f9-63397a869682': Document(page_content='2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " 'b7db050e-dabf-43f2-91d3-ceabd82aa231': Document(page_content='Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " 'fb51d174-3910-45a9-bb08-104244ce0ec3': Document(page_content='fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '02bcdeee-66d7-48bd-9392-75d2f608e998': Document(page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 12}),\n",
       " 'ff27134f-c0c5-4b6c-9fdf-4fdd4cd2e272': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 13}),\n",
       " '2d88affd-ad86-4952-8358-90c2a3879416': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 14}),\n",
       " '03dca886-9cf0-4835-a167-0657cfa6f2e6': Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '5d1817a0-fc73-45a7-9705-b71c879880f4': Document(page_content='entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " 'e085a07c-3357-4d4e-83b6-d9e4f7eb8fd0': Document(page_content='has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '8ab6937d-e46c-44ba-b7ba-e8c60e4d06e7': Document(page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'bb121861-20cc-4e22-a78c-17026d5321ef': Document(page_content='significant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'f0405514-2200-42a4-bb9b-8f29ba459768': Document(page_content='2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'e5d4d7eb-1f98-4490-802f-e7b583dea635': Document(page_content='of a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '2e91cc79-4ebd-4a1d-914b-833ee7bb7fe7': Document(page_content='Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'b4c0b61d-318a-433b-900e-20638b6bbdd1': Document(page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " 'a904e133-4c12-485b-b665-17ad3156e1e9': Document(page_content='Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " '1374b8ff-6225-4cc3-9a67-4cdfe9810d6a': Document(page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " 'b929e097-a711-4c32-a0bc-397e7c7c4fe8': Document(page_content='√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '1d22a84c-a94c-4ea5-a79a-8e8fdc9b273e': Document(page_content='Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '9eea3241-0eb3-4d34-a952-8da556b40565': Document(page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '8644d501-63ff-481c-b3ba-b3abf97ead1a': Document(page_content='and the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '48cdc0ef-8a1b-4f12-bcb2-87355a31a036': Document(page_content='of the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " 'dc5d0344-61b7-48a2-ae9a-444b8c50e9d4': Document(page_content='mation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '79cfcdd3-e165-4fb6-ab70-a576030f11fe': Document(page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " 'e6f09c8c-e0ef-46ca-b878-202ebf9fe71f': Document(page_content='as the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " 'c6e8899c-74d2-4e78-9669-d0f110bb0218': Document(page_content='In this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '5eefa378-ebff-4058-94dc-816574cde3b1': Document(page_content='and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " 'dad3ac87-5d02-4eb1-b5f8-eb9229a6db59': Document(page_content='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " 'ee1a790b-0d49-47d0-8000-ea2004f81c18': Document(page_content='recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '18a84606-4f4b-4d60-ae12-0b77c3e30a67': Document(page_content='target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " 'db07980d-0227-4a8e-b490-6e0eb102dbff': Document(page_content='lrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " 'b77441c0-9803-4987-84dd-118bc41ba2c9': Document(page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '630e62b9-ecf9-4333-b0b3-2b68ee7cd590': Document(page_content='Pdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " 'bc40b40f-16f0-459d-a9f8-b2c90b9c6bae': Document(page_content='previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '94f36c91-7ffd-4d83-8c6f-5fde1658d3e4': Document(page_content='To evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '28c2fb79-418b-4579-aeec-60e1d2d53e2f': Document(page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '03bba446-b13d-4906-829c-c0d1e89a7453': Document(page_content='checkpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " 'cef49626-3f29-439a-a800-d860494ea5f0': Document(page_content='To evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '25a03a25-bef7-4aac-b5f0-499b4bbb01c5': Document(page_content='(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '1b14ba41-f081-4569-a6d7-fce50005ce92': Document(page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'c84178e4-af25-4c1f-9570-f1f4f725cda1': Document(page_content='prisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '9c9a0b27-0746-4637-be02-b2b520ed3f32': Document(page_content='plan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'fed06b30-3ab5-4972-b361-635c0d2eb7b1': Document(page_content='[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '8d36d58e-0dc5-4bbe-9efb-57ff18616f9c': Document(page_content='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '1dc7656c-accb-4675-a628-cd28e3ae4ea3': Document(page_content='[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '231f507b-b2f8-4249-ad99-7e249e5bb6af': Document(page_content='Information Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " 'b57ddf5f-a8d5-4c7d-b962-b67bb7b0a7a2': Document(page_content='[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '5efc555a-31df-4a90-b750-13f56e5ade18': Document(page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '54d4e279-a604-44d7-bdc0-149e4342334e': Document(page_content='2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '27d29a0f-ace7-4412-98b1-b4bd6df90461': Document(page_content='Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '1de9de5e-11a1-4f06-a66e-f7413d221363': Document(page_content='fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '211a90a4-ff89-42a8-891e-d97511d43e56': Document(page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 12}),\n",
       " 'bf040927-cddc-4de6-9c83-8523e81a5820': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 13}),\n",
       " '6b10cb80-1ace-471d-93e0-c256a6431642': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 14}),\n",
       " 'cc4980b8-6031-42a2-b981-2c040b85430a': Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " 'bd7796a6-5073-4ae8-a5c9-bc0e1c4203ec': Document(page_content='entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '67a7a842-57a9-4be8-996a-f30adb09e8ad': Document(page_content='has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " 'dcebba68-5cc6-4107-a96e-4e7c71cb0856': Document(page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'ea72c5ca-174d-478c-9460-44c19e474c08': Document(page_content='significant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '021cd153-69cc-4094-922e-db3ec322de33': Document(page_content='2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '15380aeb-9138-4894-a494-402e7b8bc6d2': Document(page_content='of a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '86356f15-3319-49c1-a1f7-a18527cd857a': Document(page_content='Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'a21045b4-5977-48da-9656-b82525fd28ae': Document(page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " 'ec55dd94-e20e-464e-b0af-477ea8d7753b': Document(page_content='Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " '19c2852a-0efa-4f77-b7b5-3743d49c98b2': Document(page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " 'b5f4996b-7e70-47b2-923f-c68e88f0f451': Document(page_content='√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '784ee5b3-a530-4317-8f09-57d89e5413b3': Document(page_content='Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '381f9780-d6ef-479b-8305-dd4804ad339a': Document(page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " 'd696856e-7889-4eaa-ac21-3d05f078c558': Document(page_content='and the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " '19f221fa-04e6-4630-b186-7762bc106c87': Document(page_content='of the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " 'bb2e6cec-9866-4ccb-a104-01291ebfb69e': Document(page_content='mation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " 'bb992b28-06e8-4445-b07d-0ec88e6fb2bd': Document(page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '2734cd9d-e888-4215-80be-5da44bdd1b6c': Document(page_content='as the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '9f18f2b0-ed55-4c90-9092-6728387f70d5': Document(page_content='In this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '48b19cb3-c408-456d-aba7-e2a54eff63cf': Document(page_content='and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '281b84d2-2a7f-4790-a651-e23ec6aaceeb': Document(page_content='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " 'a9507260-849b-4136-a0ac-ea077d853e0d': Document(page_content='recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '3d0c93a7-7e80-4d25-88ee-e4bc93f25eeb': Document(page_content='target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '11a5eada-5f95-4a16-843e-d808f60ff6f6': Document(page_content='lrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '9fe3a90c-a75c-4475-ac19-b689e8efe587': Document(page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '623f514e-8dfd-4540-930f-702f6db22b37': Document(page_content='Pdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '4cda40d4-c9fe-4ea5-913f-20a9b4e2a499': Document(page_content='previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " 'bb203c04-77b9-4bbb-b114-b15f3fb1b091': Document(page_content='To evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " 'c128665b-4b80-4067-b3e6-61aa170ab1af': Document(page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '77339e00-af7a-4287-bece-5e707c485936': Document(page_content='checkpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '613f66ff-3f06-4b8b-b9c5-681a3768ea49': Document(page_content='To evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " 'f557bfd2-df83-4334-b630-612ddf257752': Document(page_content='(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '4aeb2a4c-45b1-46da-b2f2-ab3078bc24fe': Document(page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'cdcd20cd-b298-4d23-a264-388ed7c16e9e': Document(page_content='prisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '39d31db7-b52e-4d4c-96d4-a0393a6f34bc': Document(page_content='plan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'b37f3f8a-5dfc-4467-942b-1f0134f1f847': Document(page_content='[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'feb9162a-204f-47c8-8eb4-477a9850492f': Document(page_content='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '0485485a-bef2-49d6-8cbe-d252fde433e9': Document(page_content='[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '30e43a60-db67-4d45-b9ea-080545998dd6': Document(page_content='Information Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " 'da33d4f7-2545-496d-b8b5-f638072a6c3d': Document(page_content='[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " 'b0dd5843-f0b5-407c-a807-1790e3d55833': Document(page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '6da771f0-816c-4c96-8873-8a4ac3570ac8': Document(page_content='2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '3d0c7b2e-0e40-4b36-9ab7-8267cfc00dfe': Document(page_content='Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '1b0d8ff4-0fd5-45a2-92db-c05b89cdc520': Document(page_content='fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '42e4598c-9d02-4f3a-bca8-98656f074fca': Document(page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 12}),\n",
       " '9fe00822-4fac-40b9-b0a1-2c353caa3f7c': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 13}),\n",
       " '5b84ca04-b94e-46e9-bf2f-f879abd1aafc': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 14}),\n",
       " '19c40d28-9fe3-47b5-98f7-56c021eaddeb': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_Resume__july2024.pdf', 'page': 0}, page_content='DIVYANSH NAUTIYAL\\nćndivyansh29@gmail.com |\\x99New Delhi, India |Ħ+91 9599549382\\naGitHub |ĲLeetCode |ǑKaggle |]linkedin\\nEducation\\nGuru Gobind Singh Indraprastha University Expected graduation: 2025\\nUniversity School of Automation and Robotics CGPA: 9.32/10\\nB. Tech in Industrial IoT\\nExperience\\nAcencore Technologies Feb 2024 - present\\nAI-ML Intern\\n•Developed LLM applications using prompt engineering and fine-tuning, attained an error rate below 2% in ATS algorithm\\n•Engineered and deployed interview scoring systems, tuning BERT Transformers on AWS SageMaker, achieving a\\nMean Absolute Error (MAE) below 4.\\n•Designed resume parser incorporating YOLOv8, PyTesseract and open-source LLM for extraction and formatting,\\nachieving a parse rate of 90%.\\nMedical Imaging and Signal Analysis Hub Sep 2023 - Nov 2023\\nDeep Learning Research Intern\\n•Researched and integrated object detection and semantic segmentation models, achieving a remarkable 97% accuracy'),\n",
       " '2ffb864a-3c56-4afc-9b14-15908003a32e': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_Resume__july2024.pdf', 'page': 0}, page_content='rate on Video Capusle Endoscopy (VCE) frames dataset.\\n•Quantized deep learning models and reduced size by 50% for deployment on mobile devices.\\n•Annotated and benchmarked the VCE dataset containing 3500 images for multi-region bleeding detection.\\nIntel oneAPI Student’s Club (IoSC GGSIPU-EDC) Jul 2023 - Aug 2023\\nWeb Development Intern\\n•Engineered HealthCare Companion AI for Intel oneAPI Ambassador’s Hackathon, performing above 90% mark for\\ndisease prediction from symptom descriptions.\\n•Enhanced model performance by integrating Intel’s AI Reference kits, showcasing a notable 20% increase in training\\nspeeds.\\n•Mentored a team of 10 students in the collaborative development of the official club website for IoSC USAR 2023-24.\\nProjects\\nLegal Buddy Aug 2023 - Sep 2023\\n•Implemented a Django-based law prediction system using NLP, achieving 85% accuracy in predicting applicable laws\\nfrom case descriptions.'),\n",
       " '7c91898e-54a2-4f44-bce3-2450c40dc3c9': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_Resume__july2024.pdf', 'page': 0}, page_content='•Compiled a dataset from 200 case studies sourced from news articles and Supreme Court records, enhanced using\\ntext augmentation techniques.\\nAdvertisement Optimizer Apr 2023 - May 2023\\n•Utilized Reinforcement Learning to predict the Click-Through Rates of advertisements and give personalised recom-\\nmendation within 100 interactions with the RL system.\\n•Resolved exploration-exploitation tradeoffs using the Upper Confidence Bound and Thompson Sampling algorithms .\\nSkills\\nLanguages: Python, Java, C/C++, SQL\\nWeb Development: Django, Django-REST Framework, FastAPI, PostgreSQL, SQLalchemy, React\\nData Science: Machine Learning, Data analysis, Deep Learning, Computer Vision , NLP , Generative AI, MLops\\nTools and Frameworks: TensorFlow, PyTorch, Scikit-Learn,Tableau, MATLAB, NLTK, Hugging Face, LangChain\\nDeployment: AWS, AWS SageMaker, TensorFlow Extended , Docker, TFlite, Kubeflow, Microsoft Azure AI services, Git\\nPositions of Responsibility\\nEvaluation Team Member -WCEBleedGen Challenge'),\n",
       " 'a523165b-452a-487f-b170-de57fab6583f': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_Resume__july2024.pdf', 'page': 0}, page_content='•Spearheaded the Evaluation Team for the WCEBleedGen Challenge, leading the development of an annotated dataset\\nof 2500 images.\\n•Pioneered pipelines that automated submission scoring, resulting in a significant reduction of evaluation time by 30%.\\nTech-Lead, Intel oneAPI Student’s Club, GGSIPU\\n•Conducted seminars on Intel oneAPI toolkits, oneDNN, and Intel DevCloud, engaging over 100 attendees.\\n•Orchestrated a tech-fest with 600 participants, coordinating coding competitions and hackathons in collaboration with\\nCoding Ninjas and Devfolio.\\nCertifications\\nMicrosoft Certified: Azure AI Fundamentals Issued on: June 2024\\nDeepLearning.AI - Machine Learning Engineering for Production (MLOps) Specialization Issued on: March 2024'),\n",
       " '72a6611d-19f9-4ade-81c6-c0e757f6900a': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_Resume__july2024.pdf', 'page': 0}, page_content='DIVYANSH NAUTIYAL\\nćndivyansh29@gmail.com |\\x99New Delhi, India |Ħ+91 9599549382\\naGitHub |ĲLeetCode |ǑKaggle |]linkedin\\nEducation\\nGuru Gobind Singh Indraprastha University Expected graduation: 2025\\nUniversity School of Automation and Robotics CGPA: 9.32/10\\nB. Tech in Industrial IoT\\nExperience\\nAcencore Technologies Feb 2024 - present\\nAI-ML Intern\\n•Developed LLM applications using prompt engineering and fine-tuning, attained an error rate below 2% in ATS algorithm\\n•Engineered and deployed interview scoring systems, tuning BERT Transformers on AWS SageMaker, achieving a\\nMean Absolute Error (MAE) below 4.\\n•Designed resume parser incorporating YOLOv8, PyTesseract and open-source LLM for extraction and formatting,\\nachieving a parse rate of 90%.\\nMedical Imaging and Signal Analysis Hub Sep 2023 - Nov 2023\\nDeep Learning Research Intern\\n•Researched and integrated object detection and semantic segmentation models, achieving a remarkable 97% accuracy'),\n",
       " 'a855a40f-455b-423a-ae11-ee93c6a1ed82': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_Resume__july2024.pdf', 'page': 0}, page_content='rate on Video Capusle Endoscopy (VCE) frames dataset.\\n•Quantized deep learning models and reduced size by 50% for deployment on mobile devices.\\n•Annotated and benchmarked the VCE dataset containing 3500 images for multi-region bleeding detection.\\nIntel oneAPI Student’s Club (IoSC GGSIPU-EDC) Jul 2023 - Aug 2023\\nWeb Development Intern\\n•Engineered HealthCare Companion AI for Intel oneAPI Ambassador’s Hackathon, performing above 90% mark for\\ndisease prediction from symptom descriptions.\\n•Enhanced model performance by integrating Intel’s AI Reference kits, showcasing a notable 20% increase in training\\nspeeds.\\n•Mentored a team of 10 students in the collaborative development of the official club website for IoSC USAR 2023-24.\\nProjects\\nLegal Buddy Aug 2023 - Sep 2023\\n•Implemented a Django-based law prediction system using NLP, achieving 85% accuracy in predicting applicable laws\\nfrom case descriptions.'),\n",
       " '5d2fc30d-5581-4bb7-9769-faa725c56736': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_Resume__july2024.pdf', 'page': 0}, page_content='•Compiled a dataset from 200 case studies sourced from news articles and Supreme Court records, enhanced using\\ntext augmentation techniques.\\nAdvertisement Optimizer Apr 2023 - May 2023\\n•Utilized Reinforcement Learning to predict the Click-Through Rates of advertisements and give personalised recom-\\nmendation within 100 interactions with the RL system.\\n•Resolved exploration-exploitation tradeoffs using the Upper Confidence Bound and Thompson Sampling algorithms .\\nSkills\\nLanguages: Python, Java, C/C++, SQL\\nWeb Development: Django, Django-REST Framework, FastAPI, PostgreSQL, SQLalchemy, React\\nData Science: Machine Learning, Data analysis, Deep Learning, Computer Vision , NLP , Generative AI, MLops\\nTools and Frameworks: TensorFlow, PyTorch, Scikit-Learn,Tableau, MATLAB, NLTK, Hugging Face, LangChain\\nDeployment: AWS, AWS SageMaker, TensorFlow Extended , Docker, TFlite, Kubeflow, Microsoft Azure AI services, Git\\nPositions of Responsibility\\nEvaluation Team Member -WCEBleedGen Challenge'),\n",
       " '3f97fdb7-1c7e-47e6-b1ea-1f1ae207b028': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_Resume__july2024.pdf', 'page': 0}, page_content='•Spearheaded the Evaluation Team for the WCEBleedGen Challenge, leading the development of an annotated dataset\\nof 2500 images.\\n•Pioneered pipelines that automated submission scoring, resulting in a significant reduction of evaluation time by 30%.\\nTech-Lead, Intel oneAPI Student’s Club, GGSIPU\\n•Conducted seminars on Intel oneAPI toolkits, oneDNN, and Intel DevCloud, engaging over 100 attendees.\\n•Orchestrated a tech-fest with 600 participants, coordinating coding competitions and hackathons in collaboration with\\nCoding Ninjas and Devfolio.\\nCertifications\\nMicrosoft Certified: Azure AI Fundamentals Issued on: June 2024\\nDeepLearning.AI - Machine Learning Engineering for Production (MLOps) Specialization Issued on: March 2024'),\n",
       " '7e3fca7a-04ed-454e-8656-2410d8f5eedf': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_Resume__july2024.pdf', 'page': 0}, page_content='DIVYANSH NAUTIYAL\\nćndivyansh29@gmail.com |\\x99New Delhi, India |Ħ+91 9599549382\\naGitHub |ĲLeetCode |ǑKaggle |]linkedin\\nEducation\\nGuru Gobind Singh Indraprastha University Expected graduation: 2025\\nUniversity School of Automation and Robotics CGPA: 9.32/10\\nB. Tech in Industrial IoT\\nExperience\\nAcencore Technologies Feb 2024 - present\\nAI-ML Intern\\n•Developed LLM applications using prompt engineering and fine-tuning, attained an error rate below 2% in ATS algorithm\\n•Engineered and deployed interview scoring systems, tuning BERT Transformers on AWS SageMaker, achieving a\\nMean Absolute Error (MAE) below 4.\\n•Designed resume parser incorporating YOLOv8, PyTesseract and open-source LLM for extraction and formatting,\\nachieving a parse rate of 90%.\\nMedical Imaging and Signal Analysis Hub Sep 2023 - Nov 2023\\nDeep Learning Research Intern\\n•Researched and integrated object detection and semantic segmentation models, achieving a remarkable 97% accuracy'),\n",
       " '2be3d655-1610-4f67-9e05-0d599d68ce12': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_Resume__july2024.pdf', 'page': 0}, page_content='rate on Video Capusle Endoscopy (VCE) frames dataset.\\n•Quantized deep learning models and reduced size by 50% for deployment on mobile devices.\\n•Annotated and benchmarked the VCE dataset containing 3500 images for multi-region bleeding detection.\\nIntel oneAPI Student’s Club (IoSC GGSIPU-EDC) Jul 2023 - Aug 2023\\nWeb Development Intern\\n•Engineered HealthCare Companion AI for Intel oneAPI Ambassador’s Hackathon, performing above 90% mark for\\ndisease prediction from symptom descriptions.\\n•Enhanced model performance by integrating Intel’s AI Reference kits, showcasing a notable 20% increase in training\\nspeeds.\\n•Mentored a team of 10 students in the collaborative development of the official club website for IoSC USAR 2023-24.\\nProjects\\nLegal Buddy Aug 2023 - Sep 2023\\n•Implemented a Django-based law prediction system using NLP, achieving 85% accuracy in predicting applicable laws\\nfrom case descriptions.'),\n",
       " 'a54ec5c5-285d-4df7-96a0-29f469108e7f': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_Resume__july2024.pdf', 'page': 0}, page_content='•Compiled a dataset from 200 case studies sourced from news articles and Supreme Court records, enhanced using\\ntext augmentation techniques.\\nAdvertisement Optimizer Apr 2023 - May 2023\\n•Utilized Reinforcement Learning to predict the Click-Through Rates of advertisements and give personalised recom-\\nmendation within 100 interactions with the RL system.\\n•Resolved exploration-exploitation tradeoffs using the Upper Confidence Bound and Thompson Sampling algorithms .\\nSkills\\nLanguages: Python, Java, C/C++, SQL\\nWeb Development: Django, Django-REST Framework, FastAPI, PostgreSQL, SQLalchemy, React\\nData Science: Machine Learning, Data analysis, Deep Learning, Computer Vision , NLP , Generative AI, MLops\\nTools and Frameworks: TensorFlow, PyTorch, Scikit-Learn,Tableau, MATLAB, NLTK, Hugging Face, LangChain\\nDeployment: AWS, AWS SageMaker, TensorFlow Extended , Docker, TFlite, Kubeflow, Microsoft Azure AI services, Git\\nPositions of Responsibility\\nEvaluation Team Member -WCEBleedGen Challenge'),\n",
       " '6feaf934-316a-4fad-ab2d-590cf9fef8d6': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_Resume__july2024.pdf', 'page': 0}, page_content='•Spearheaded the Evaluation Team for the WCEBleedGen Challenge, leading the development of an annotated dataset\\nof 2500 images.\\n•Pioneered pipelines that automated submission scoring, resulting in a significant reduction of evaluation time by 30%.\\nTech-Lead, Intel oneAPI Student’s Club, GGSIPU\\n•Conducted seminars on Intel oneAPI toolkits, oneDNN, and Intel DevCloud, engaging over 100 attendees.\\n•Orchestrated a tech-fest with 600 participants, coordinating coding competitions and hackathons in collaboration with\\nCoding Ninjas and Devfolio.\\nCertifications\\nMicrosoft Certified: Azure AI Fundamentals Issued on: June 2024\\nDeepLearning.AI - Machine Learning Engineering for Production (MLOps) Specialization Issued on: March 2024'),\n",
       " 'd0160c3a-6c64-4d42-87c1-2a42e86ca50b': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_Resume__july2024.pdf', 'page': 0}, page_content='DIVYANSH NAUTIYAL\\nćndivyansh29@gmail.com |\\x99New Delhi, India |Ħ+91 9599549382\\naGitHub |ĲLeetCode |ǑKaggle |]linkedin\\nEducation\\nGuru Gobind Singh Indraprastha University Expected graduation: 2025\\nUniversity School of Automation and Robotics CGPA: 9.32/10\\nB. Tech in Industrial IoT\\nExperience\\nAcencore Technologies Feb 2024 - present\\nAI-ML Intern\\n•Developed LLM applications using prompt engineering and fine-tuning, attained an error rate below 2% in ATS algorithm\\n•Engineered and deployed interview scoring systems, tuning BERT Transformers on AWS SageMaker, achieving a\\nMean Absolute Error (MAE) below 4.\\n•Designed resume parser incorporating YOLOv8, PyTesseract and open-source LLM for extraction and formatting,\\nachieving a parse rate of 90%.\\nMedical Imaging and Signal Analysis Hub Sep 2023 - Nov 2023\\nDeep Learning Research Intern\\n•Researched and integrated object detection and semantic segmentation models, achieving a remarkable 97% accuracy'),\n",
       " '34513824-724a-4cc9-9fba-8821a4fd9e1b': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_Resume__july2024.pdf', 'page': 0}, page_content='rate on Video Capusle Endoscopy (VCE) frames dataset.\\n•Quantized deep learning models and reduced size by 50% for deployment on mobile devices.\\n•Annotated and benchmarked the VCE dataset containing 3500 images for multi-region bleeding detection.\\nIntel oneAPI Student’s Club (IoSC GGSIPU-EDC) Jul 2023 - Aug 2023\\nWeb Development Intern\\n•Engineered HealthCare Companion AI for Intel oneAPI Ambassador’s Hackathon, performing above 90% mark for\\ndisease prediction from symptom descriptions.\\n•Enhanced model performance by integrating Intel’s AI Reference kits, showcasing a notable 20% increase in training\\nspeeds.\\n•Mentored a team of 10 students in the collaborative development of the official club website for IoSC USAR 2023-24.\\nProjects\\nLegal Buddy Aug 2023 - Sep 2023\\n•Implemented a Django-based law prediction system using NLP, achieving 85% accuracy in predicting applicable laws\\nfrom case descriptions.'),\n",
       " '88dbe46f-ddee-4661-aeb6-a8801138118d': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_Resume__july2024.pdf', 'page': 0}, page_content='•Compiled a dataset from 200 case studies sourced from news articles and Supreme Court records, enhanced using\\ntext augmentation techniques.\\nAdvertisement Optimizer Apr 2023 - May 2023\\n•Utilized Reinforcement Learning to predict the Click-Through Rates of advertisements and give personalised recom-\\nmendation within 100 interactions with the RL system.\\n•Resolved exploration-exploitation tradeoffs using the Upper Confidence Bound and Thompson Sampling algorithms .\\nSkills\\nLanguages: Python, Java, C/C++, SQL\\nWeb Development: Django, Django-REST Framework, FastAPI, PostgreSQL, SQLalchemy, React\\nData Science: Machine Learning, Data analysis, Deep Learning, Computer Vision , NLP , Generative AI, MLops\\nTools and Frameworks: TensorFlow, PyTorch, Scikit-Learn,Tableau, MATLAB, NLTK, Hugging Face, LangChain\\nDeployment: AWS, AWS SageMaker, TensorFlow Extended , Docker, TFlite, Kubeflow, Microsoft Azure AI services, Git\\nPositions of Responsibility\\nEvaluation Team Member -WCEBleedGen Challenge'),\n",
       " '919a112c-3b9a-4383-a846-629258be3821': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_Resume__july2024.pdf', 'page': 0}, page_content='•Spearheaded the Evaluation Team for the WCEBleedGen Challenge, leading the development of an annotated dataset\\nof 2500 images.\\n•Pioneered pipelines that automated submission scoring, resulting in a significant reduction of evaluation time by 30%.\\nTech-Lead, Intel oneAPI Student’s Club, GGSIPU\\n•Conducted seminars on Intel oneAPI toolkits, oneDNN, and Intel DevCloud, engaging over 100 attendees.\\n•Orchestrated a tech-fest with 600 participants, coordinating coding competitions and hackathons in collaboration with\\nCoding Ninjas and Devfolio.\\nCertifications\\nMicrosoft Certified: Azure AI Fundamentals Issued on: June 2024\\nDeepLearning.AI - Machine Learning Engineering for Production (MLOps) Specialization Issued on: March 2024'),\n",
       " '2519259f-278d-4ec5-a3b3-081936547e78': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_USAR.pdf', 'page': 0}, page_content='Divyansh Nautiyal +91-+91 9599549382\\nBachelor of Technology ndivyansh29@gmail.com\\nIndustrial Internet of Things GitHub\\nGuru Gobind Singh Indraprastha University EDC Delhi LeetCode\\nEnrollment No: 04419011721 LinkedIn\\nKaggle\\nEducation\\n•Bachelor of Technology in Industrial Internet of Things 2021-25\\nUNIVERSITY SCHOOL OF AUTOMATION AND ROBOTICS Surajmal Vihar New Delhi CGPA: 9.32\\n•Intermediate Year: 2021\\nShiv Vani Model Sr. Secondary School, New Delhi Percentage: 94.4%\\nPersonal Projects\\n•Legal Buddy Aug 2023 - Sept 2023\\nGitHub Link\\n–Implemented a NLP-powered legal application that predicts applicable laws from case descriptions, obtained a\\ntesting accuracy of 80%.\\n–Achieved notable performance improvement by applying data mining techniques to a dataset of 200 case studies\\nsourced from news articles and Supreme Court records.\\n•Advertisement Optimizer Apr 2023 - May 2023\\nGitHub Link\\n–Utilized Reinforcement Learning to predict the Click-Through Rates of advertisements and give personalised'),\n",
       " 'cbb9cff5-4e33-444c-891c-4e9c9922f1d8': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_USAR.pdf', 'page': 0}, page_content='recommendation within 100 interactions with the RL system.\\n–Resolved exploration-exploitation trade-offs using the Upper Confidence Bound and Thompson Sampling algo-\\nrithms .Experience\\n•AI-ML Intern Feb 2024 - Present\\nAcencore Technologies Online\\n–Engineered LLM applications using prompt engineering and fine-tuning, Attained an error rate below 2% in ATS\\nalgorithm, optimizing performance and effectiveness.\\n–Developed and deployed interview scoring systems utilizing BERT Transformers on AWS SageMaker, achieving a\\nMean Absolute Error (MAE) below 4.\\n–Designed a resume parser incorporating YOLOv9 for section extraction and classification, and OCR for text\\nextraction, achieving a mAP@50 of 75%.\\n•Deep Learning Research Intern Sep 2023 - Nov 2023\\nMedical Imaging and Signal Analysis Hub , IGDTUW Hybrid\\n–Researched on integration of object detection and semantic segmentation models, achieving a remarkable 97%\\naccuracy rate on Video Capusle Endoscopy (VCE) frames.'),\n",
       " '2b807a36-b265-4e3e-ac38-f632373519fa': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_USAR.pdf', 'page': 0}, page_content='–Quantized deep learning models and reduced size by 50% for deployment on mobile devices.\\n–Annotated and benchmarked the VCE dataset containing 3500 images for multi-region bleeding detection.\\nTechnical Skills\\nLanguages : C/C++, Java, Python, SQL, HTML-CSS, Javascript\\nLibraries and Tools : Git, PostgreSQL, SQLAlchemy, VScode, openCV, NLTK, HuggingFace, Tableau, Docker,\\nTensorboard, AWS SageMaker, Azure AI services\\nFrameworks : Django, Django Rest Framework, FastAPI, React, Sci-kit learn, Tensorflow, Pytorch, Langchain,\\nTensorflow Extended\\nCloud Platforms : Google Cloud Platform (GCP), Amazon Web Services(AWS), Microsoft Azure\\nOther Skills : Web Development, IOT, Embedded Systems, Data Analysis, Machine Learning, Deep Learning, NLP,\\nComputer Vision, Generative AI, ML operations\\nPositions of Responsibility\\n•Tech Lead Intel oneAPI Student Community June 2023 - Present\\n–Orchestrated a tech-fest including hackathon and coding challenges, drawing an impressive footfall of 600 partici-'),\n",
       " 'be63a348-a58c-4a84-b3e7-dd3cc49180e6': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_USAR.pdf', 'page': 0}, page_content='pants.\\n–Conducted workshops on Intel’s oneAPI toolkits, engaging with over 300 attendees.\\n•Evaluation Team Member Auto WCEBleedGen Challenge Sep 2021 - Oct 2023\\n–Spearheaded the Evaluation Team for the Auto WCEBleedGen Challenge 2023, developing automated evaluation\\npipelines and contributing to result assessment and winner declaration.\\nAchievements\\n•Recognized as a Kaggle Notebook Expert for advanced data analysis and machine learning.\\n•Conquered over 500 LeetCode problems , among the top 34% in contests.\\n•Earned 2 stars on CodeChef, demonstrating proficiency in competitive programming'),\n",
       " 'd735335d-768f-405a-9da2-1cc1eeb430e5': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_USAR.pdf', 'page': 0}, page_content='Divyansh Nautiyal +91-+91 9599549382\\nBachelor of Technology ndivyansh29@gmail.com\\nIndustrial Internet of Things GitHub\\nGuru Gobind Singh Indraprastha University EDC Delhi LeetCode\\nEnrollment No: 04419011721 LinkedIn\\nKaggle\\nEducation\\n•Bachelor of Technology in Industrial Internet of Things 2021-25\\nUNIVERSITY SCHOOL OF AUTOMATION AND ROBOTICS Surajmal Vihar New Delhi CGPA: 9.32\\n•Intermediate Year: 2021\\nShiv Vani Model Sr. Secondary School, New Delhi Percentage: 94.4%\\nPersonal Projects\\n•Legal Buddy Aug 2023 - Sept 2023\\nGitHub Link\\n–Implemented a NLP-powered legal application that predicts applicable laws from case descriptions, obtained a\\ntesting accuracy of 80%.\\n–Achieved notable performance improvement by applying data mining techniques to a dataset of 200 case studies\\nsourced from news articles and Supreme Court records.\\n•Advertisement Optimizer Apr 2023 - May 2023\\nGitHub Link\\n–Utilized Reinforcement Learning to predict the Click-Through Rates of advertisements and give personalised'),\n",
       " '2b202469-7851-499d-9af3-0f1fd9f561c8': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_USAR.pdf', 'page': 0}, page_content='recommendation within 100 interactions with the RL system.\\n–Resolved exploration-exploitation trade-offs using the Upper Confidence Bound and Thompson Sampling algo-\\nrithms .Experience\\n•AI-ML Intern Feb 2024 - Present\\nAcencore Technologies Online\\n–Engineered LLM applications using prompt engineering and fine-tuning, Attained an error rate below 2% in ATS\\nalgorithm, optimizing performance and effectiveness.\\n–Developed and deployed interview scoring systems utilizing BERT Transformers on AWS SageMaker, achieving a\\nMean Absolute Error (MAE) below 4.\\n–Designed a resume parser incorporating YOLOv9 for section extraction and classification, and OCR for text\\nextraction, achieving a mAP@50 of 75%.\\n•Deep Learning Research Intern Sep 2023 - Nov 2023\\nMedical Imaging and Signal Analysis Hub , IGDTUW Hybrid\\n–Researched on integration of object detection and semantic segmentation models, achieving a remarkable 97%\\naccuracy rate on Video Capusle Endoscopy (VCE) frames.'),\n",
       " 'bbd0efe6-6b58-4c4e-bab7-808c1665cd0b': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_USAR.pdf', 'page': 0}, page_content='–Quantized deep learning models and reduced size by 50% for deployment on mobile devices.\\n–Annotated and benchmarked the VCE dataset containing 3500 images for multi-region bleeding detection.\\nTechnical Skills\\nLanguages : C/C++, Java, Python, SQL, HTML-CSS, Javascript\\nLibraries and Tools : Git, PostgreSQL, SQLAlchemy, VScode, openCV, NLTK, HuggingFace, Tableau, Docker,\\nTensorboard, AWS SageMaker, Azure AI services\\nFrameworks : Django, Django Rest Framework, FastAPI, React, Sci-kit learn, Tensorflow, Pytorch, Langchain,\\nTensorflow Extended\\nCloud Platforms : Google Cloud Platform (GCP), Amazon Web Services(AWS), Microsoft Azure\\nOther Skills : Web Development, IOT, Embedded Systems, Data Analysis, Machine Learning, Deep Learning, NLP,\\nComputer Vision, Generative AI, ML operations\\nPositions of Responsibility\\n•Tech Lead Intel oneAPI Student Community June 2023 - Present\\n–Orchestrated a tech-fest including hackathon and coding challenges, drawing an impressive footfall of 600 partici-'),\n",
       " 'a2fcbba3-0e16-4236-9337-2157ef825bb4': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_USAR.pdf', 'page': 0}, page_content='pants.\\n–Conducted workshops on Intel’s oneAPI toolkits, engaging with over 300 attendees.\\n•Evaluation Team Member Auto WCEBleedGen Challenge Sep 2021 - Oct 2023\\n–Spearheaded the Evaluation Team for the Auto WCEBleedGen Challenge 2023, developing automated evaluation\\npipelines and contributing to result assessment and winner declaration.\\nAchievements\\n•Recognized as a Kaggle Notebook Expert for advanced data analysis and machine learning.\\n•Conquered over 500 LeetCode problems , among the top 34% in contests.\\n•Earned 2 stars on CodeChef, demonstrating proficiency in competitive programming'),\n",
       " 'dc3710e9-56e2-47a4-a2a9-f463a01553c8': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_USAR.pdf', 'page': 0}, page_content='Divyansh Nautiyal +91-+91 9599549382\\nBachelor of Technology ndivyansh29@gmail.com\\nIndustrial Internet of Things GitHub\\nGuru Gobind Singh Indraprastha University EDC Delhi LeetCode\\nEnrollment No: 04419011721 LinkedIn\\nKaggle\\nEducation\\n•Bachelor of Technology in Industrial Internet of Things 2021-25\\nUNIVERSITY SCHOOL OF AUTOMATION AND ROBOTICS Surajmal Vihar New Delhi CGPA: 9.32\\n•Intermediate Year: 2021\\nShiv Vani Model Sr. Secondary School, New Delhi Percentage: 94.4%\\nPersonal Projects\\n•Legal Buddy Aug 2023 - Sept 2023\\nGitHub Link\\n–Implemented a NLP-powered legal application that predicts applicable laws from case descriptions, obtained a\\ntesting accuracy of 80%.\\n–Achieved notable performance improvement by applying data mining techniques to a dataset of 200 case studies\\nsourced from news articles and Supreme Court records.\\n•Advertisement Optimizer Apr 2023 - May 2023\\nGitHub Link\\n–Utilized Reinforcement Learning to predict the Click-Through Rates of advertisements and give personalised'),\n",
       " 'b12daaea-9cde-4001-a51c-0f3b3afacc63': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_USAR.pdf', 'page': 0}, page_content='recommendation within 100 interactions with the RL system.\\n–Resolved exploration-exploitation trade-offs using the Upper Confidence Bound and Thompson Sampling algo-\\nrithms .Experience\\n•AI-ML Intern Feb 2024 - Present\\nAcencore Technologies Online\\n–Engineered LLM applications using prompt engineering and fine-tuning, Attained an error rate below 2% in ATS\\nalgorithm, optimizing performance and effectiveness.\\n–Developed and deployed interview scoring systems utilizing BERT Transformers on AWS SageMaker, achieving a\\nMean Absolute Error (MAE) below 4.\\n–Designed a resume parser incorporating YOLOv9 for section extraction and classification, and OCR for text\\nextraction, achieving a mAP@50 of 75%.\\n•Deep Learning Research Intern Sep 2023 - Nov 2023\\nMedical Imaging and Signal Analysis Hub , IGDTUW Hybrid\\n–Researched on integration of object detection and semantic segmentation models, achieving a remarkable 97%\\naccuracy rate on Video Capusle Endoscopy (VCE) frames.'),\n",
       " '6dea7658-afa6-438f-b13b-a7de3100ea75': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_USAR.pdf', 'page': 0}, page_content='–Quantized deep learning models and reduced size by 50% for deployment on mobile devices.\\n–Annotated and benchmarked the VCE dataset containing 3500 images for multi-region bleeding detection.\\nTechnical Skills\\nLanguages : C/C++, Java, Python, SQL, HTML-CSS, Javascript\\nLibraries and Tools : Git, PostgreSQL, SQLAlchemy, VScode, openCV, NLTK, HuggingFace, Tableau, Docker,\\nTensorboard, AWS SageMaker, Azure AI services\\nFrameworks : Django, Django Rest Framework, FastAPI, React, Sci-kit learn, Tensorflow, Pytorch, Langchain,\\nTensorflow Extended\\nCloud Platforms : Google Cloud Platform (GCP), Amazon Web Services(AWS), Microsoft Azure\\nOther Skills : Web Development, IOT, Embedded Systems, Data Analysis, Machine Learning, Deep Learning, NLP,\\nComputer Vision, Generative AI, ML operations\\nPositions of Responsibility\\n•Tech Lead Intel oneAPI Student Community June 2023 - Present\\n–Orchestrated a tech-fest including hackathon and coding challenges, drawing an impressive footfall of 600 partici-'),\n",
       " 'af2565e0-2deb-4ea4-8710-c66bfaa5933e': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Divyansh_USAR.pdf', 'page': 0}, page_content='pants.\\n–Conducted workshops on Intel’s oneAPI toolkits, engaging with over 300 attendees.\\n•Evaluation Team Member Auto WCEBleedGen Challenge Sep 2021 - Oct 2023\\n–Spearheaded the Evaluation Team for the Auto WCEBleedGen Challenge 2023, developing automated evaluation\\npipelines and contributing to result assessment and winner declaration.\\nAchievements\\n•Recognized as a Kaggle Notebook Expert for advanced data analysis and machine learning.\\n•Conquered over 500 LeetCode problems , among the top 34% in contests.\\n•Earned 2 stars on CodeChef, demonstrating proficiency in competitive programming'),\n",
       " '221d1ae2-8c02-44c0-a513-5e74b609f560': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 0}, page_content='Towards Detecting and Classifying Malicious URLs\\nUsing Deep Learning\\nClayton Johnson1, Bishal Khadka1, Ram B. Basnet1*, and Tenzin Doleck2\\n1Colorado Mesa University, Grand Junction, CO 81501, USA\\n{cpjohnson, bkhadka }@mavs.coloradomesa.edu, rbasnet@coloradomesa.edu\\n2University of Southern California, Los Angeles, CA 90007, USA\\ndoleck@usc.edu\\nReceived: September 30, 2020; Accepted: December 11, 2020; Published: December 31, 2020\\nAbstract\\nEmails containing Uniform Resource Locators (URLs) pose substantial risks to organizations by po-\\ntentially compromising both credentials and network security through general and spear-phishing\\ncampaigns to their employees. The detection and classiﬁcation of malicious URLs is an important\\nresearch problem with practical applications. With an appropriate machine learning model, an orga-\\nnization may protect itself by ﬁltering incoming emails and the websites its employees are visiting'),\n",
       " '242bf782-36bf-4c39-9581-7e84c25087f6': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 0}, page_content='based on the maliciousness of URLs contained in emails and web pages. In this work, we com-\\npare the performance of traditional machine learning algorithms, such as Random Forest, CART,\\nand kNN against popular deep learning framework models, such as Fast.ai and Keras-TensorFlow\\nacross CPU, GPU, and TPU architectures. Using the publicly available ISCX-URL-2016 dataset,\\nwe present the models’ performances across binary and multiclass classiﬁcation experiments. By\\ncollecting accuracy and timing metrics, we ﬁnd that Random Forest, Keras-TensorFlow, and Fast.ai\\nmodels performed comparably and with the highest accuracies >96% in both the detection and\\nclassiﬁcation of malicious URLs, with Random Forest as the preferable model based on time, perfor-\\nmance, and complexity constraints. Additionally, by ranking and using feature selection techniques,\\nwe determine that the top 5-10 features provide the best performances compared to using all the fea-\\ntures provided in the dataset.'),\n",
       " '86942e98-62ec-4299-90f4-9d2e5d62d70d': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 0}, page_content='Keywords : Malicious URLs, Phishing URLs, Deep Learning, Web Security, Machine Learning\\n1 Introduction\\nPhishing—along with its more targeted version, spear phishing—is a social engineering attack in which\\nthe attacker attempts to compromise a user’s credentials or a system by presenting itself as a legitimate\\nbusiness communication [38]. These communications, most commonly emails, will often contain links\\nto websites controlled by the attacker that attempt to: 1) mimic a popular website to scrape the user’s cre-\\ndentials, 2) install malware onto the user’s system, or 3) spam the user. These links with malicious intents\\nare called malicious URLs. According to Verizon’s 2020 Data Breach Investigations Report [39], phish-\\ning attacks have been in the top three types of data breaches for the past 6 years and have been number\\none for the past two years, with around 96% of the social engineering attacks conducted through email.'),\n",
       " '2096d01c-f33e-49e1-a5f0-fe3408467db6': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 0}, page_content='There is a clear increase in popularity with these types of attacks, highlighted by the Internet Crime Com-\\nplaint Center (IC3) Public Service Announcement on Business Email Compromises (BECs) [28]. The\\nJournal of Wireless Mobile Networks, Ubiquitous Computing, and Dependable Applications (JoWUA) , 11(4):31-48, Dec. 2020\\nDOI:10.22667/JOWUA.2020.12.31.031\\n*Corresponding author: Department of Computer Science and Engineering, Colorado Mesa University, Grand Junction, CO\\n81501, USA, Tel: +1-970-248-1400\\n31'),\n",
       " '594965dc-46cd-4e1a-9c94-635171b8ff84': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 1}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nIC3’s 2019 Internet Crime Report [9] reported that losses from BECs and Email Account Compromises\\n(EACs) were in excess of $1.7 billion. The threat posed to an organization by these types of attacks is\\nclearly substantial, with trends indicating increase in popularity and severity over time. To address the\\nthreat phishing and malicious URLs pose to businesses, many popular websites and educational insti-\\ntutions have added “Phishing Awareness” programs, as exempliﬁed by the following examples: SANS\\nInstitute [16], InfoSec Institute [15], and University of Connecticut [27]. While providing training, edu-\\ncation, and awareness on phishing attacks have become trivial and common, such efforts are not adequate\\nto protect users. This warrants the need to apply machine learning algorithms to detect malicious URLs\\nbefore they arrive at intended targets within an organization, thus decreasing the efforts required from'),\n",
       " '7f8a0291-4131-474b-b781-c83de7540709': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 1}, page_content='the user. The problem of detection and classiﬁcation of URLs is illustrated in Figure 1.\\nFigure 1: Detection and Classiﬁcation of URLs Problem Overview\\nThe contributions of this work are twofold: 1) we offer a direct comparison of popular deep learn-\\ning frameworks (Keras and Fast.ai) against traditional machine learning algorithms (Random Forest,\\nClassiﬁcation and Regression Tree, k-Nearest Neighbor, Support Vector Machine, Logistic Regression,\\nLinear Discriminant Analysis, AdaBoost, and Naive Bayes) in the detection and classiﬁcation of ma-\\nlicious URLs; and 2) we expand on the literature utilizing the public ISCX-URL-2016 dataset, which\\ncontains lexical features of malicious URLs. The remainder of this research is structured as follows. The\\nRelated Works section will explore recent advancements and contributions towards the detection and\\nclassiﬁcation of malicious URLs, as well as highlight any research gaps within the ﬁeld. Following this,'),\n",
       " '107a413a-19f8-497b-9786-8254eecb7375': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 1}, page_content='the machine learning and deep learning frameworks utilized in the work are discussed in the Frameworks\\nsection. The ISCX-URL-2016 dataset will be discussed in depth before the presentation and explana-\\ntion of the experiments and results. Finally, the paper will conclude and suggest trajectories for further\\nresearch.\\n2 Related Works\\nWe summarize indicative studies examining malicious URLs. We follow this with a discussion of some\\nof the patterns seen across the literature and the research gaps.\\nMamun et al. [24] presented a new, public malicious URLs dataset called ISCX-URL-2016. Ad-\\nditionally, Mamun et al. experimented with the usage of RF, C4.5, and kNN algorithms to detect and\\nclassify malicious URLs on this new dataset by generating features directly from the URL, such as URL\\nlength, domain entropy, arguments, etc. In detecting malicious URLs, RF performed the highest with\\n32'),\n",
       " '30582d96-cb95-493d-a315-a2a44175585f': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 2}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\n>0.99 precision and recall; however, both C4.5 and kNN models performed with >0.97 precision and\\nrecall. When classifying malicious trafﬁc, RF achieved the highest performance with recall and preci-\\nsion of 0.97. In this second classiﬁcation phase, all models perform with precision and recall in the range\\n0.92-0.97. Finally, the authors demonstrated that the obfuscation of malicious URLs does decrease the\\ndetection and classiﬁcation accuracies of the ML models used.\\nCui et al. [8] proposed a system to detect malicious URLs by using NB, DT, and SVM classiﬁers. The\\nreported accuracies >98.7% for all classiﬁers. Due to the high-performance of the models, the authors\\nclaim to have deployed the system, analyzing up to 2 TB worth of data per day. This work was extended\\nby Zhao et al. [41], who compared the performance of RF and a Gated Recurrent Neural Network (GRU)'),\n",
       " '78d6b30e-b78d-4dd3-929f-1df0729ae853': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 2}, page_content='over the same dataset used in [8] in detecting malicious URLs. Zhao et al. found that the GRU model\\nwith 98.5% accuracy outperformed the RF model with 96.4% accuracy. This trend was seen across\\nvarying sizes of training datasets (from 600 to 240,000 samples). They also presented graphs showing\\nthe distributions of each classiﬁcation (Legitimate, SQL Injection, XSS Attack, Sensitive File Attack,\\nand Directory Traversal) based on the “Number of Characters” feature. Additional statistical analysis of\\neach feature may shed more light into understanding the malicious URLs detection problem—one of the\\ncontributions of this research study.\\nChoi et al. [6] experimented with the detection and classiﬁcation of malicious URLs using SVM\\nfor binary classiﬁcation and C4.5, Label Powerset, and kNN classiﬁers for multiclass classiﬁcation. The\\nfeatures used for this system contain lexical characteristics, link structures, DNS, network trafﬁc, and'),\n",
       " '181f165a-d42b-4a49-b4ee-290723ecd77b': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 2}, page_content='content composition information. The detection of malicious trafﬁc demonstrated high performance\\nwith SVM’s accuracy exceeding 98%. The work is claimed to be the ﬁrst report of malicious URLs\\nclassiﬁcation across phishing, spamming, and malware categories. Their models for multi-class classiﬁ-\\ncation performed with accuracies >93%. Additionally, the article states that due to the details the model\\ntook into account, obfuscation techniques such as redirection, link manipulation, and fast-ﬂux hosting\\nwould hinder the effectiveness in detecting malicious URLs.\\nMa et al. [23] presented a novel technique for detecting malicious URLs by utilizing continuous,\\nonline machine learning techniques. The work experimented with Logistic Regression, SVM, NB, Con-\\nﬁdence Weighted (CW), and Perceptron models. By combining features such as DNS information, lex-\\nical characteristics, web registration dates, etc. with blacklist information, the team was able to compile'),\n",
       " 'd37cada2-e061-4357-8f0a-19e847440632': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 2}, page_content='a total of 2.9 million features over the course of 100 days. The models were trained using a sliding\\nwindow of two weeks’ worth of training data. While all models showed decreasing error rates over the\\n100 day period, CW consistently maintained the lowest error rates with a minimum of 1%. Due to the\\ndynamic landscape of the malicious URLs problem, Ma et al. addressed the need for large, fresh datasets\\nto emphasize and encourage the continuous training of models.\\nUc ¸ar et al. [37] applied two deep learning models, Long Short-Term Memory (LSTM) and Con-\\nvolutional Neural Network (CNN), in the detection and classiﬁcation of malicious URLs. Detection of\\nmalicious URLs demonstrated high performance, with accuracies of 97.25% and 98.86% for the LSTM\\nand CNN models, respectively. The CNN model showed an accuracy of 95.37% on the classiﬁcation of\\nmalicious URLs, higher than 91.13% for LSTM. The work is signiﬁcant for two reasons. First, there'),\n",
       " 'd6991452-cbec-42d8-ad32-13d1905a77c4': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 2}, page_content='are few applications of deep learning in this research problem and secondly, few works have used the\\nISCX-URL-2016 dataset.\\nVinayakumar et al. [21] evaluated the performance of multiple deep learning architectures (LSTM,\\nCNN, Bidirectional Recurrent Structures, LSTM-CNN hybrid, and Stacked CNNs) against their pro-\\nposed deep learning architecture in the detection of malicious URLs. The architectures explored were\\nimplemented through Keras, described in the Frameworks section. The two-part dataset includes data\\nfrom Alexa.com, DMOZ Directory, Sophos, and other related sources. The models perform in the range\\nof 95 - 99% accuracy over the ﬁrst dataset, with LSTM as the highest performing model. The authors\\nplace an emphasis on the difference between the random-split and time-split versions of the dataset,\\n33'),\n",
       " '2d8806cb-2add-4568-b727-af238239b5fd': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 3}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nshowing that there is a higher variance in the accuracy results from the time-split than the random-split\\nversions of the second dataset. The models’ accuracy ranges from 95-96.6% in the random-split section\\nof the second dataset, while the time-split version has models ranging from 93-97.1% accuracy.\\nSahoo et al. [33] conducted a survey of the work done within the realm of detecting and classifying\\nmalicious URLs. The work discussed in depth the multitude of issues with existing, commercial blacklist\\nand heuristic solutions, which lead to ungeneralized, nonﬂexible models that are susceptible to novel or\\nobfuscated attacks. For simplicity and security, most of the recent efforts have been focused on static\\nanalysis of URLs using various machine learning techniques. Almost half of the works analyzed utilized'),\n",
       " '797062e5-af34-4d01-a473-197bb2b3bda3': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 3}, page_content='either lexical or host-based features [33]. There is a clear preference for these types of features within\\nthe ﬁeld, while other works are spread out among HTML, JavaScript, and Context-based features. While\\nthere have been some efforts to apply deep learning architectures, such as LSTMs and CNNs to the\\nmalicious URLs problem, computational complexity is the largest constraint [33]. However, there are\\nfew works exploring and comparing the results of deep neural network frameworks against traditional\\nmachine learning algorithms such as RF, SVM, kNN, etc, in the context of the malicious URLs problem.\\nThis is the primary contribution of our work.\\nHung et al. [22] performed a series of deep learning experiments and neural networks like Convolu-\\ntional Neural Networks (CNN) so that the model would learn URL embedding. They use the large dataset\\nfrom VirusTotal anti-virus group. The team attempted to ﬁnd the limitations in identifying the malicious'),\n",
       " 'da093770-eba4-48e2-8905-6ffb6092e278': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 3}, page_content='URLs using lexical properties like inability to identify the proper semantic meaning and patterns in URL\\nand they use end-to-end deep learning framework to address this kind of problem. The team addressed\\nthe problem of identifying whether a URL is malicious or not by formulating a problem as a binary\\nclassiﬁcation task [22]. They use the technique called character-level CNN which learns the frequency\\nand sequence of the characters in the URL. Moreover, word-level CNN is also applied for identifying the\\nunique words in the URL. The main goal of the paper was to use character-level and word-level CNN to\\naddress the limitations produced by previous methods and precisely identify the malicious URLs.\\nBirhanu et al. [10] employed a technique called BINSPECT, which is a combination of static analysis\\nand minimalistic emulation and uses supervised learning techniques, for detecting malicious web pages.'),\n",
       " 'bfa90beb-9a60-457a-b0d9-2bac31a776ee': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 3}, page_content='The team was able to generate accuracy of more than 97% when using this technique with low false\\nsignals [10]. Furthermore, the team also mentioned that, with this approach, it took them only 3-5\\nseconds to analyze a single web page.\\n3 Motivation\\nWhen addressing the problem, most of the works available attempt to only detect malicious URLs, while\\nfew works experiment with classifying malicious URLs. While detection is a substantial issue, the\\nclassiﬁcation of these malicious URLs indicates where the direction and priority of actions within an\\norganization should be to best protect its network. For instance, defaced URLs may require more urgent\\nreaction due to potential credential compromises than spam URLs, which don’t need immediate action.\\nThere is a clear consensus that blacklisting malicious URLs is a poor solution since it fails to address\\nissues such as obfuscation or novel attacks. As a consequence of this, most of the recent work has'),\n",
       " '10fb3864-078e-4eca-bff1-56b027f9c72b': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 3}, page_content='focused on applications of machine learning algorithms. Of the popular algorithms, Random Forest (RF)\\nappears to be one of the most effective. The survey of relevant work revealed few studies that address\\napplications of deep learning within the ﬁeld and even fewer that directly compare the performance of\\ndeep learning models to traditional machine learning models. This is a key gap in the literature that the\\npresent study aims to address. Additionally, this work compares various metrics, such as training and\\nprediction times, across multiple architectures (CPU, GPU, and TPU) to determine which model would\\nbe most practical to use in an industry setting.\\n34'),\n",
       " '1b2c30a3-3038-4037-a549-28bd3d29e2bf': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 4}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nThe features used in the ﬁeld are varied and derived from many different sources. Despite this,\\nfeatures may be derived from both static analysis of the URL and dynamic analysis from visiting the\\nwebsite. Most of the research works analyze static features for security and cost reasons. It is safer and\\ncomputationally cheaper to statically analyze a suspicious URL than visit the website and execute the\\nattacker’s code, which may take an arbitrary amount of time to complete. The majority of the related\\nworks presented create custom datasets with different types of features (statistical and lexical, host-\\nbased, WHOIS, etc.), demonstrating a lack of standardization within the datasets used for the research\\nproblem. One of the reasons for our usage of the ISCX-URL-2016 dataset is to show support for usage\\nof a standardized, lexicon-based dataset for static analysis of suspicious URLs. This work explores the'),\n",
       " '5a397d29-febe-467a-9671-998cf5f8330f': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 4}, page_content='effectiveness of the provided URL attributes and demonstrates the effectiveness lexical analysis may\\nhave in detecting malicious URLs, as well as the limitations.\\n4 Machine and Deep Learning Frameworks\\n4.1 fast.ai\\nfast.ai is one of the famous python libraries which includes various machine learning and deep learning\\nlibraries. fast.ai is a Python module that implements a high-level API to a PyTorch backend. The goal of\\nfast.ai is to easily allow experts in other ﬁelds, such as virologists or astronomers, to implement popular\\ndeep learning techniques within their respective settings. This framework has seen multiple popular\\nsuccesses in research and industry [11].\\n4.2 Keras\\nKeras is a deep learning framework for Python that allows for the implementation of both TensorFlow\\nand Theano in the backend [20]. Keras runs Tensorﬂow 2.0 in the backend for easy machine learning\\nworkﬂow and for proper data management. Keras allows for easy and fast prototyping through user'),\n",
       " '23cfc1cb-d828-4840-848c-e8e9c38cef04': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 4}, page_content='friendliness, modularity, and extensibility. It usually runs seamlessly on CPU and GPU.\\n4.3 TensorFlow\\nTensorﬂow was originally developed by Google and it is free to use. Tensorﬂow uses static computa-\\ntional graphs also known as the deﬁne-and-run approach. The libraries and large community resources\\navailable in the Tensorﬂow allows its user to build powerful and advanced machine learning models and\\napplications. It builds and trains ML models easily using intuitive high-level APIs like Keras with ea-\\nger execution, which makes for immediate model iteration and easy debugging [36]. One of the key\\naffordances of Tensorﬂow is that it not only uses CPU but also GPU which helps in gaining much more\\ncomputing power. Tensorﬂow 2.0 further uses TPU, also known as Tensor Processing Unit, which adds\\ncomputational power and improves performance. Using this deep learning module, even fairly compli-\\ncated models can be created with very little coding effort.\\n4.4 PyTorch'),\n",
       " 'be76a3e3-9e2a-4795-a308-5855c922643b': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 4}, page_content='4.4 PyTorch\\nPyTorch is an open source machine learning framework that accelerates the path from research prototyp-\\ning to production deployment [30]. This machine learning framework was ﬁrst developed by Facebook.\\nPyTorch uses dynamic computational graphs which lets the user process variable length input and out-\\nput. PyTorch has many libraries like captum, pytorch geometric, and skorch which are all open source\\nand help in the tasks like model interpretability, performing deep learning on irregular input data, and\\nproviding scikit-learn compatibility.\\n35'),\n",
       " 'b951ffd5-d987-46df-a1b0-e48adfab3b66': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 5}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\n4.5 Scikit-Learn\\nScikit-learn (sklearn)—a python library built upon Scipy, Numpy, and matplotlib—is a handy tool for\\npredictive data analysis [29]. This cutting edge software is free to use for the public. Common machine\\nlearning tasks, such as classiﬁcation, regression, and clustering, can be easily tackled using scikit-learn.\\nFor some machine learning classiﬁers like Support Vector Classiﬁcation and Lasso, scikit-learn outper-\\nforms other python machine learning libraries [29].\\nThe models used from the Scikit-learn module are the RandomForestClassiﬁer, Decision Tree (op-\\ntimized CART algorithm), KNeighborsClassiﬁer, SVC (Support Vector Machine), LogisticRegression,\\nLinearDiscriminantAnalysis, AdaBoostClassiﬁer, and GaussianNB (Naive Bayes).\\n5 Dataset\\nThis experiment presents ISCX-URL-2016 URL [24] dataset. Around 78 lexical features were extracted'),\n",
       " '9a98b2d9-e18c-413b-90bb-8ac7e4a09eb8': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 5}, page_content='from URLs, and 5 URL classes such as benign, defacement, malware, phishing, and spam were labeled\\nin the dataset. The different classes of URLs are brieﬂy introduced below.\\nBenign URLs: Benign URLs are legitimate URLs that do not lead to any infectious websites and do\\nnot try to inject the user’s computer with any kind of harmful malware. Benign websites may contain\\nadvertisements and adware which are typically harmless to a computer.\\nDefacement URLs: Website defacement usually means changing a certain aspect of the website\\nsuch as it’s visual appearances and some contents on it. Hacktivists try to deface a website for numerous\\nreasons [32]. This kind of act is done when some content in the web page needs to be changed without\\nthe permission of the original owner of the website which technically means penetrating a website.\\nMalware URLs: Malware URLs take a user to the malicious website that typically installs some'),\n",
       " '224368f3-9763-490a-9c1e-4054422fefe6': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 5}, page_content='malware on that user’s device which can be used for identity theft, corrupting ﬁles, and even logging\\nkeystrokes. Malware can indirectly be a dangerous software that can harm a computer and steal some-\\none’s private information [25]. Some threats such as harmful biological agents, a terrorist cell intent on\\ndisrupting operations, etc. can be considered as malware [26]. Some of the examples of malware are\\nransomware, spyware, scareware, among others.\\nPhishing URLs: Phishing URLs conventionally entice a user to visit a fake website and will try to\\nsteal as much information they can get from the user. Sometimes a user can easily be led to phishing\\nwebsites just by having a typo in a URL. Phishing can be deﬁned as the intent of the hacker to steal some\\nprivate information like credit card number and other digital identity by employing social engineering\\ntechniques [40].\\nSpam URLs: Spam is a way of sending unsolicited emails to the user with the intent of advertise-'),\n",
       " '23cd0420-1ee9-474a-bfa2-fad266b10aa9': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 5}, page_content='ments or for serious harm to the computer [19]. Spam URLs are usually seen in the spam email. Some\\nspam URLs can be harmful and can infect the computer of the user with spyware and adware.\\n5.1 Lexical Analysis\\nMalicious URLs may contain some pattern in their URL text which gives us a hint that the URL is not\\nlegitimate. Various lexical features such as query length, domain token count, path token count, URL\\nlength, domain length, path length, and many more were used in the experiment for better performance\\nand results. Let’s consider the following URL to extract lexical features, e.g.:\\nhttp://www.example.site.com/path dir1/path dir2/ﬁle.php\\nThe URL is 56 characters long, thus the ‘URLLen’ feature is 56. The length of the domain name\\n“domainlength” example.site.com is 16, “domainUrlRatio” is 0.2857, and “NumberofDotsinURL” is 4.\\nThis process of extracting all 78 lexical features is thoroughly discussed in [24].\\n36'),\n",
       " '1d6f8f5b-0a8f-4518-a02e-606505cd122f': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 6}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nAs discussed previously in the Related Works section, traditional blacklisting techniques have clear\\ndisadvantages in the detection and classiﬁcation problems. Thus, taking a closer look at lexical analysis\\nwould be a better choice for identifying those URLs.\\n5.2 Data Preprocessing\\nOriginal dataset had a total sample of 36,707. From this, many entries were NaN, inﬁnity, or empty.\\nAll the entries and attributes with NaN, Inﬁnity, and missing values were eliminated from the dataset\\nwith the aim of getting more precise results. During the data cleansing process, around 17K rows with\\nNaN and redundant values were dropped from the dataset. In addition, 7 attributes of columns with NaN\\nvalues were eliminated leaving only 72 attributes in the dataset. Table 1 illustrates the number of samples\\nbefore and after data cleanup.\\nURL Type Raw Samples Filtered Samples\\nBenign 7,781 2,709\\nDefacement 7,930 2,477'),\n",
       " 'f0392568-25a0-4768-8806-e0ba5df1755e': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 6}, page_content='Malware 6,712 4,440\\nPhishing 7,586 4,014\\nSpam 6,698 5,342\\nTable 1: Number of samples before and after data cleanup\\n6 Experiment and Results\\nThe experiments demonstrate a two-layered approach to the malicious URLs research problem: (1)\\nExperiment 1 involves the detection of malicious URLs and (2) Experiment 2 involves the classiﬁcation\\nof malicious URLs. Experiment 1 is a binary classiﬁcation problem with two classes: Benign and\\nMalicious. All non-benign categories of URLs are labelled as malicious. Experiment 2 is a multi-class\\nclassiﬁcation problem where malicious URLs are categorized into speciﬁc classes such as: Defacement,\\nMalware, Phishing, and Spam.\\nBoth the experiments utilize the same set of machine learning models: Random Forest [3] (RF),\\nDecision Tree/CART [4] (DT), k-Nearest Neighbors [7] (kNN), Support Vector Machine (SVM), Lo-\\ngistic Regression (LR), Linear Discriminant Analysis (LDA), AdaBoost [13] (AB), Naive Bayes (NB).'),\n",
       " '04caf0fb-26c4-44b1-8b3f-98feaa583de9': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 6}, page_content='In addition, two deep learning models, Fast.ai [14] and Keras-TensorFlow [20] [36] [1], are also used.\\nThe models are trained and evaluated on the ISCX-URL-2016 dataset using stratiﬁed 10-fold cross val-\\nidation. All experiments were conducted on the free tier of Google Colaboratory system using Jupyter\\nNotebooks. The Jupyter Notebooks and scripts used for the experiments can be found on GitHub.com\\n[2].\\n6.1 Performance Metrics\\nVarious metrics are collected to compare and determine the most appropriate model meeting real-world\\ndeployment constraints. Initially, all models are trained and evaluated on both Experiments 1 and 2, gen-\\nerating accuracy using 10-fold cross validation. Confusion matrices are presented to clearly demonstrate\\nthe performance of the DNN models. An additional experiment is performed on the highest-performing\\ntraditional machine learning and the DNN algorithms in which the benign samples are removed from the'),\n",
       " '03196b3f-a169-48fb-a1d1-4abd5063ab6c': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 6}, page_content='multi-class classiﬁcation problem. The goal of the experiment is to see how the best classiﬁers would\\n37'),\n",
       " '84e2c0c5-7b9e-43d8-a719-b1d6107117a6': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 7}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nperform on multi-class classiﬁcation of just the malicious URLs. Then, time metrics are calculated on\\nthese three models. These time metrics describe two different characteristics of the models. These time\\nvalues are collected across CPU, GPU, and TPU architectures to further assist an organization’s invest-\\nment decisions in projects in which architecture is a consideration.\\nThe ﬁrst time metrics used to evaluate the models is the total time required to train each model. This\\ntraining time metric is calculated by summing the total training time elapsed for each fold. Training time\\nexcludes object instantiation and testing since the focus is on training speciﬁcally. Even though training\\nis often described as a single event, training time is included as a metric because retraining with new data\\nmay occur multiple times for a model implemented by a system. Thus, the total amount of time required'),\n",
       " 'a351b18c-5575-4aa9-96c0-6387cdb2a0db': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 7}, page_content='to train a model has a direct impact on the organization.\\nThe second time metric is the average time required for a model to predict the classiﬁcation of a pro-\\nvided sample. If a model was deployed and used to predict the malevolence of a URL, the time required\\nto generate this prediction has a clear impact on the ability of the organization to ﬁlter URLs. This effect\\nis exponential for larger organizations with growing attack surfaces. For simplicity, we assume that each\\ndeployed model would predict on a single sample at a time, as opposed to batches of samples. Addition-\\nally, since this metric is calculated by timing the ‘predict’ method of each object on samples from the\\nISCX-URL-2016 dataset, we do not report the time required to generate the features used in the dataset.\\nHowever, time required to generate features may be minimized by balancing the trade-off between the\\nnumber of features used and reported accuracy, as discussed in the feature analysis section.\\n6.2 Results'),\n",
       " 'c6d4bc8f-57ad-4e8b-850a-09be727c1750': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 7}, page_content='6.2 Results\\nAs seen in Table 2, Random Forest demonstrates superior accuracy among all machine learning algo-\\nrithms used for both the experiments. This result is well documented in the literature [21, 40] and further\\nsupported by our experiments. Both DNN models show high accuracy in Experiment 1, performing sim-\\nilar to DT, kNN, and AB, while still being eclipsed by RF. Experiment 2 sees performance decreases for\\nall models except Fast.ai, with both Fast.ai and RF performing comparably. Keras-TensorFlow’s accu-\\nracy decreased substantially to 91.5%, around ﬁve percentage points lower than that from RF and Fast.ai.\\nAn additional observation is the higher standard deviation for Keras-TensorFlow.\\nClassiﬁer Binary-Class Accuracy (%) Multi-Class Accuracy (%)\\nRF 98.68±0.22 96.26±0.53\\nDT 97.63±0.24 92.81±0.62\\nkNN 97.47±0.39 92.52±0.79\\nSVM 93.96±0.60 80.77±0.63\\nLR 90.50±0.30 69.54±0.97\\nLDA 94.34±0.33 82.31±0.78\\nAB 96.21±0.39 76.58±1.60\\nNB 68.73±0.83 59.55±0.91\\nfast.ai 96.88±0.33 96.77±0.51'),\n",
       " '148ff467-27dc-4ad8-b87e-66916ee32b74': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 7}, page_content='Keras-TensorFlow 97.13±1.93 91.45±2.94\\nTable 2: Accuracy Results for Binary and Multi-Class Classiﬁcation\\nRemoving the benign samples contained in the multi-class dataset, as seen in Table 3, appears to\\nhave little effect on the performance of the classiﬁers. While all three classiﬁers appear to perform better\\nwithout the benign samples, the performance is within one standard deviation of the previous results.\\nThus, this improvement appears to be insigniﬁcant. Nevertheless, RF, Fast-AI, and Keras-TensorFlow\\nclassiﬁers clearly demonstrate high accuracy metrics when classifying malicious URLs.\\n38'),\n",
       " '35a61ccf-1d89-4105-9849-6c4719de647e': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 8}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nClassiﬁer Accuracy (%)\\nRF 96.99±0.50\\nfast.ai 97.55±0.37\\nKeras-TensorFlow 93.81±2.34\\nTable 3: Top Classiﬁer’s Accuracies Results for Multi-class Malicious URL Classiﬁcation (without Be-\\nnign Samples)\\nFigure 2: Confusion Matrices for Keras-TensorFlow and Fast.ai DNN Models for Experiment 1\\nThe confusion matrices for the deep learning models are presented in Figure 2 and Figure 3. While\\nan overwhelming majority of the samples are classiﬁed correctly—seen through the main diagonal of\\nthe matrix— the Phishing URL classiﬁcation has the clearest rate of failure. Both deep learning models\\ndemonstrate a pattern of both over- and under-predicting Phishing URLs.\\nClassiﬁer Experiment CPU GPU TPU\\nRF Binary 75.83 61.58 95.19\\nMulti 87.26 71.02 106.83\\nfast.ai Binary 323.36 221.52 360.47\\nMulti 320.91 220.00 362.29\\nKeras-TensorFlow Binary 445.78 135.63 123.17\\nMulti 451.37 135.65 124.09'),\n",
       " '3fc1d898-113c-4f5c-a203-276bcafe4fde': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 8}, page_content='Table 4: Training Time across Runtime Environments in Seconds\\nTable 4 presents the total time taken for Random Forest and the DNN models to train across the 10\\nfolds on a CPU, GPU, and TPU run-time environments. Keras-TensorFlow demonstrates its lowest train\\ntimes on the TPU, as expected since Google develops both TensorFlow and the TPU architecture [18].\\nHowever, the best metric for Keras-TensorFlow is still twice the fastest time recorded for RF. Both RF and\\nFast.ai have their best training times on the GPU, which is likely because there is currently no support\\nor documentation for the TPU architecture through Scikit-Learn or Fast.ai’s PyTorch version (Fast.ai\\n39'),\n",
       " 'f09be61b-9574-4967-a423-058e4cb34196': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 9}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nFigure 3: Confusion Matrix for Keras-Tensorﬂow (left) and Fast.ai (right) for Multi-Class Classiﬁcation\\nExperiments\\nClassiﬁer Experiment CPU GPU TPU\\nRF Binary 8.12±0.77 6.61±0.63 7.99±0.74\\nMulti 8.30±1.09 6.66±0.73 8.02±0.66\\nfast.ai Binary 53.17±9.95 44.29±6.65 54.59±8.888\\nMulti 54.62±17.31 44.34±4.98 54.53±11.05\\nKeras-TensorFlow Binary 38.53±4.71 28.81±5.51 41.52±4.30\\nMulti 40.20±5.56 28.54±4.48 41.49±4.32\\nTable 5: Average Time to Predict Classiﬁcation of a Sample in Milliseconds\\ncurrently implements PyTorch v1, however Fast.ai v2 claims it will support TPU through PyTorch v1.3+)\\n[31] [12] [14]. While Keras-TensorFlow trains faster than Fast.ai, there appears to be a large performance\\nhit, especially for Experiment 2. There is little change in performance times based on the experiment\\nconducted.\\nThe times presented in Table 5 show the average time for the RF and DNN models to predict a'),\n",
       " 'b7c0e3fd-1a95-45e9-bf6b-de3d2e0d0964': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 9}, page_content='classiﬁcation for a given sample. As expected, there is a uniform drop in average time from CPU to\\nGPU. The TPU architecture, on the other hand, appears to have mixed results on the prediction times,\\nleading to all three models showing the best prediction times on the GPU. Finally, Random Forest appears\\nto be the most consistent with standard deviations less than 1.09 ms. This result is in extreme contrast\\nto the DNN models, which experience standard deviations ranging from 4.30 ms to 17.31 ms. These\\nmetrics appear to have mixed results depending on the experiment conducted.\\n6.3 Feature Analysis\\nThe features in the ISCX-URL-2016 dataset are analyzed to shine further light onto the subject of de-\\nveloping efﬁcient mechanisms to detect and classify malicious URLs. There are a total of 78 features\\navailable from the dataset. These features naturally vary in their predictive capabilities and thus their\\n40'),\n",
       " '744fe556-0abc-4d9d-9cf3-78ce20e10786': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 10}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nusefulness for a machine learning algorithm. By applying the chi-squared test [29] [35] [34], we rank\\nthe features with highest correlation to the classiﬁcation of a malicious URL sample. The chi-squared\\nfeature ranking technique is chosen for its simplicity and availability through sklearn [35]. Additionally,\\nthe p-value calculated by the test for each feature clearly indicates its importance in classiﬁcation prob-\\nlems, where high p-values indicate independence from the target classiﬁcation and are thus poor-quality\\nfeatures and vice-versa. The effect of this process is two-fold. Using this feature selection technique,\\nwe decrease noise within a dataset and decrease the resources required to generate a prediction on a\\nsample in a real-world setting. If a model handles noisy data poorly and thus performs poorly, this will'),\n",
       " '41188536-06bc-4aff-bf41-e1f734da4fc9': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 10}, page_content='help remedy its sensitivity to the data. Additionally, when trying to determine if a URL in an email or a\\nwebsite is malicious, less time and computation is spent generating characteristics that are unused or not\\nas useful.\\nRank Binary-Class Feature Binary-Class p-value Multi-Class Feature Multi-Class p-value\\n1 ﬁleNameLen 1.406341e-70 Entropy Afterpath 0\\n2 domain token count 1.636373e-41 argPathRatio 1.397820e-290\\n3 tld 1.636373e-41 NumberRate AfterPath 1.983143e-290\\n4 SymbolCount Domain 1.678641e-41 NumberRate Domain 5.645027e-279\\n5 Entropy Afterpath 3.584071e-41 ArgUrlRatio 3.924478e-272\\n6 delimeter path 9.681864e-40 Extension DigitCount 6.874504e-171\\n7 argPathRatio 9.901046e-38 dldgetArg 1.534782e-147\\n8 Entropy Filename 2.679810e-37 ldlgetArg 1.714726e-147\\n9 Entropy DirectoryName 7.487584e-36 Query DigitCount 1.118545e-135\\n10 Filename LetterCount 3.891527e-33 LongestVariableValue 2.788998e-135\\n...\\n69 Entropy URL 0.161457 ldldomain 7.808014e-11'),\n",
       " 'a905279b-0f71-4478-85c6-0727debb03b8': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 10}, page_content='70 isPortEighty 0.248489 domainlength 8.353102e-10\\n71 Directory LetterCount 0.265838 Entropy Domain 6.009946e-09\\n72 pathDomainRatio 0.305007 host letter count 1.007751e-08\\n73 dlddomain 0.339682 avgpathtokenlen 1.192005e-07\\n74 NumberRate URL 0.442371 isPortEighty 2.881362e-05\\n75sub-Directory\\nLongestWordLength0.587184sub-Directory\\nLongestWordLength3.440676e-04\\n76 Path LongestWordLength 0.868772 dlddomain 4.124519e-04\\n77 charcompvowels 0.911420 Path LongestWordLength 1.380409e-02\\n78 avgdomaintokenlen 0.993084 Entropy URL 1.272731e-01\\nTable 6: Feature Rankings across Binary and Multi-Classiﬁcation Experiments using Chi-Squared Test\\nTable 6 presents the results of applying the Chi-Squared test on the feature sets for the binary and\\nmulti-class datasets. It is interesting to note that out of the top ten highest rated features for both datasets,\\nonly two are present in both (20%). Conversely, 50% of the features in the lowest-rated—the bottom 10'),\n",
       " '74a8f8ef-671e-42a1-be05-09ce61d81982': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 10}, page_content='features—are shared. While the best features chosen for a model are highly dependent on the problem\\n(detection vs classiﬁcation), it appears that there is a consensus on which features to not use. The\\n“Entropy AfterPath” feature’s p-value is rounded off since the smallest ﬂoat available for the distribution\\nof Python used (3.8.3) is 1.0e-308. The ‘ISIpAddressInDomainName’ feature was removed from both\\ndatasets since there was no variation in the data.\\nThree of the top ten features for the binary classiﬁcation problem describe the entropy of various\\nsegments of a URL. The high value of these features is also reported in [17], which reported a 20%\\nincrease in malicious URL detection combined with a decrease in false negative rates after deploying the\\nmodel.\\nFigures 4 and 5 present the distribution graphs of the top four rated features from Table 6 for the\\n41'),\n",
       " 'bf37968b-f58e-4336-9b7b-67800d5ea290': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 11}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nmulti-class and binary-class datasets, respectively. One interesting note for the ‘argPathRatio’ graph in\\nFigure 4 is that the Defacement and Spam classiﬁcations tend towards larger values and larger variation,\\nwhile Phishing links tend towards much smaller values with the smallest variation out of all the classi-\\nﬁcations. This trend may be due to the fact that many malicious URLs may attempt to obfuscate their\\nintent through long arguments, while seemingly benign URLs may have shorter arguments with longer\\npaths. The other features shown present seemingly more complex relationships across the data. This is\\nexpected given the increased number of classiﬁcations.\\nFigure 4: Distribution Plots of Top Four Multi-class Features\\nThe binary-class features presented in Figure 5 show intriguing trends. Starting with ‘ﬁleName-'),\n",
       " '51c1bb3b-60ed-43da-8466-7cfe8e404cfb': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 11}, page_content='Len’, one may expect to observe that malicious URLs have longer ﬁlenames for obfuscation reasons;\\nhowever, the distribution plot indicates otherwise. Further, there is wider variation in the benign URL\\nﬁlename lengths than malicious URLs. There are clear trends in ‘domain token count’, ‘tld’, and ‘Sym-\\nbolCount Domain’ features, all showing malicious URLs tend to have more items than benign URLs.\\nThis result is supported by [24] [17] [5], which report higher presence of special characters and usage of\\nmultiple top-level domains (tld) in malicious URLs.\\nFigures 6 and 7 present the accuracies of RF and DNN models with varying numbers of features,\\nusing the same stratiﬁed 10-fold cross validation methodology as before. The features included are in\\norder of rank as presented in Table 6; as such, the models are trained on the rank 1 feature, then the rank\\n1 and rank 2 features, etc until all 78 features are included (note that the ‘ISIpAddressInDomainName’'),\n",
       " 'a513fe59-5434-444a-bbd9-3537f8381141': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 11}, page_content='feature is omitted, for no variance occurs). It is expected that the models perform with higher accuracy\\nas we increase the number of features available; however, the goal is to determine the optimal number of\\nfeatures such that the accuracy of the model is not penalized substantially. Figure 6 presents data from\\none feature up to 20 features for both binary and multi-class problems to increase the data’s resolution,\\nwhile Figure 7 illustrates the model accuracies as we increase the feature count up to 78. We brieﬂy\\n42'),\n",
       " '1eae0ac3-b370-4ff8-acfd-4448b5ac6d76': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 12}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nFigure 5: Distribution Plots of Top Four Binary-class Features\\nanalyze the results in each ﬁgure below.\\nThe multi-class problem analyzed in Figure 6 shows the expected upward trend for all models across\\nthe ﬁrst 20 features; however, most of this upward growth occurs in the ﬁrst ﬁve features. Random Forest\\ndemonstrates the signiﬁcance of these ﬁrst ﬁve features and slowly increases as we add more features.\\nFor the RF model, this is a clear indication of diminishing returns as the feature count increases. Both\\nKeras and Fast.ai severely underperform compared to RF for the ﬁrst 20 features. The Keras-TensorFlow\\nmodel has the largest growth in accuracy from 1-5 and 15-20 features. Fast.ai increases performance by\\n30 percentage points within the ﬁrst 10 features, however this accuracy stays constant with the addition'),\n",
       " '45e93104-0cf2-4d59-86f0-414a6ebba23f': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 12}, page_content='of another 10 features. Finally, the variation of the accuracies across the 10 folds is shown, with the\\nKeras-TensorFlow model demonstrating extremely high variation compared to both Fast.ai and RF.\\nIn contrast to Figure 6, the accuracies of the models on the binary classiﬁcation problem indicate that\\nthe models perform similarly and require fewer features. While RF still demonstrates superior accuracy,\\nKeras is consistently within 5 percentage points. Surprisingly, these models were able to perform with\\n85-90% accuracy with only the ﬁrst feature. Fast.ai, conversely, shows very poor performance with the\\nﬁrst feature, but the performance slowly increases as the feature size increases. Additionally, all models\\nshow smaller variation across the folds compared to the data seen in the multiclass problem (Figure 6).\\nFigure 7 show the change in accuracies for each model from one feature up to all 78, with half the data'),\n",
       " '6088a654-099c-4bcf-be82-b48548be8a80': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 12}, page_content='resolution of Figure 6. We see that the accuracies of all the models in Figure 7 increase with the addition\\nof more features, as expected. However, while Fast.ai shows substantial improvement in performance\\nfrom 20 to 50 features, neither Keras-TensorFlow nor RF dramatically increase in performance. The\\nissue of high variation across the folds continues for Keras-TensorFlow, while only being an issue for\\nFast.ai between 10 and 25 features.\\nWhile it is expected that the models will perform with higher accuracy when we increase the features\\navailable to the algorithms, Figure 7 presents slightly different trends. While RF, Keras-TensorFlow,\\n43'),\n",
       " '80daaa6e-5fec-4762-901c-8babb8c4b2d9': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 13}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nFigure 6: Binary Classiﬁcation (left) and Multi Classiﬁcation (right) for Increasing Number of Features\\nup to n=20\\nFigure 7: Binary Classiﬁcation (left) and Multi Classiﬁcation (right) for Increasing Number of Features\\nup to n=78\\nand Fast.ai all show increases in performance from 1 to 40 features, the addition of more features ap-\\npears to have a detrimental effect on RF and Keras-TensorFlow models. While the Keras-TensorFlow\\nmodel shows high variance within the multiclass problem, the binary classiﬁcation problem sees this\\nfor the addition of speciﬁc features. In order of appearance from left to right, the features that appear\\nto have substantial, negative effects on the performance of the Keras-TensorFlow model are ’domain-\\nUrlRatio’, ‘ldl getArg’, ’File name DigitCount’, ’File name DigitCount’, ’LongestVariableValue’, and\\n’subDirLen’.\\n7 Discussion'),\n",
       " '9fc38621-8947-49c9-a6fc-750482252bb2': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 13}, page_content='7 Discussion\\nThe current research sought to explore the effectiveness of the provided URL attributes and demonstrate\\nthe effectiveness lexical analysis may have in detecting and classifying malicious URLs, placing an\\nemphasis on practicality for an industrial setting. This experimental research mainly focused on the\\ndetection (Experiment 1) and classiﬁcation (Experiment 2) of various types of URLs based on lexical\\nanalysis through binary and multiclass classiﬁcation experiments, with an emphasis on the comparison\\nof popular deep learning models with traditional machine learning algorithms. Experiment 1 results\\ndemonstrated higher performance accuracy overall, with an increase of 8-10% on average across all\\nmodels. While Experiment 2 results showed lower performance with the average accuracy around >\\n44'),\n",
       " '1c4dce3c-f317-4755-8c9d-070312083d1a': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 14}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\n85%. Random Forest, Keras-TensorFlow, and Fast.ai consistently performed the best of all the models\\nexperimented, with >96% accuracy in both the experiments.\\nBy collecting the training and prediction times, in conjunction with the feature analysis, we conclude\\nthat, despite their popularity, deep neural networks are inferior to Random Forest due to their higher\\nvariance, count of features required to match RF performance, complexity, and overall amount of time\\nto train and predict when deployed. To minimize the effort required to potentially deploy a RF model,\\nthe feature set can be reduced down to 5-10 features with minimal cost to performance. While both\\nKeras-TensorFlow and Fast.ai are popular DNN frameworks, their deployment over RF requires more\\nresources which may be better spent elsewhere within an organization.'),\n",
       " '6cdbf26c-c8cb-4255-a41f-2f7e39e87942': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 14}, page_content='Overall, it is clear from the results that Random Forest is the most appropriate model for deployment\\nin an organization’s detection system. RF was a model that demonstrated lower complexity, as seen in\\nits lower training (Table 4) and prediction times (Table 5), and higher performance as seen in Tables 2\\nand 3. Further, RF typically performed up to or over 90% accuracy within the top ﬁve features for both\\nbinary and multi-class problems, whereas both DNN models required up to 30-40 features to match RF’s\\nperformance with only ﬁve in the multi-class problem (Figures 6 and 7).\\nThe results acquired from the deep neural network models indicate further work is required to clearly\\ndemonstrate one’s superiority over another. While the accuracy of the Fast-AI model exceeded that of\\nKeras-TensorFlow (Tables 2 and 3), Fast-AI experienced a substantial performance hit for both training\\nand sample prediction times (Tables 4 and 5, respectively). With the current work, a preference of one'),\n",
       " '0b7f3c7e-f7cc-4220-b847-d688936a5fc1': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 14}, page_content='DNN model over the other would indicate the priorities required from the model: accuracy at the cost\\nof time dictates Fast-AI superior while low latency at the cost of accuracy prefers the Keras-TensorFlow\\nmodel.\\nAdditionally, as the ﬁnal contribution of this work, the feature analysis of the lexical-based ISCX-\\nURL-2016 dataset shows the importance of speciﬁc characteristics of these malicious URLs. The main\\nconclusion from this section of the work clearly indicates a higher need for more features in the multi-\\nclassiﬁcation problem than the binary classiﬁcation problem. This is clearly seen from the p-values\\npresented in Table 6 and the slower increase in accuracies in Figures 6 and 7. While all three models\\nshowed large improvements within the ﬁrst 5-10 features, as ranked by their p-values, there was a drastic\\nimprovement in the accuracies when features ranked 15-25 were included.\\n8 Conclusion and Future Work'),\n",
       " 'bb5b0249-b4c1-4969-a2dd-2cf9c8190d9a': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 14}, page_content='This work explored the applications of machine learning and deep learning models in detecting and clas-\\nsifying malicious URLs. By collecting performance accuracy, confusion matrices, and both training and\\npredicting times for the Random Forest, Keras-TensorFlow, and Fast-AI models, this study concludes\\nthat Random Forest is the best model to deploy by organizations seeking to build an URL ﬁlter applica-\\ntion, or those wishing to incorporate machine learning techniques to improve existing ones. Additionally,\\nthis study reports the speciﬁc lexical features contained within URLs may be used to minimize overhead\\ncost of a deployed model.\\nSome limitations attached to the present study could motivate further research; e.g., our team did not\\nexhaustively explore all the network conﬁgurations and hyperparameters available for DNNs which may\\npotentially improve their performances. While these improvements may lead to succeeding RF’s reported'),\n",
       " 'ee896005-40aa-4994-9dff-b855398cd48e': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 14}, page_content='accuracy, there is an impact on training and testing times as well as additional drawback of overﬁtting\\nmodels thus reducing their real-word generalizability. Finally, we did not deploy and investigate the\\nefﬁcacy of the models with further experiments as explored in [17]; we leave this as our future research\\nwork. Importantly, we feel that more research on this front is needed to better bridge the gap between\\nacademic research and industrial applications with the goal of reducing detrimental economic impacts of\\n45'),\n",
       " 'ff70ecf8-8eab-40f9-ba83-accfe4fab507': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 15}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nmalicious URLs on organizations in various industries.\\nAcknowledgments\\nThis research project was supported by the state of Colorado through funds appropriated for cybersecurity\\nlaw dubbed “Cyber Coding Cryptology for State Records.” Any opinions, ﬁndings and conclusions, or\\nrecommendations expressed in this paper are those of the authors and do not necessarily reﬂect the views\\nof the funding sources. The authors would like to thank Robert Dunsky, Nathan Bellew and Karen Angels\\nfor helping with some of the early versions of the experiments.\\nReferences\\n[1] M. Abadi, A. Agarwal, et al. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems.\\nArXiv , abs/1603.04467, March 2016.\\n[2] R. Basnet. Deep learning malicious urls. \"https://github.com/rambasnet/\\nDeepLearningMaliciousURLs\" , [Online; accessed on September 10, 2020], 2019.'),\n",
       " 'cc91abe8-fa4d-499f-9017-440a74875d9d': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 15}, page_content='[3] L. Breiman. Random forests. Machine learning , 45(1):5–32, October 2001.\\n[4] L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen. Classiﬁcation and regression trees . CRC press,\\n1984.\\n[5] Z. Chen and J. Szurdi. Cybersquatting: Attackers mimicking domains of major brands including face-\\nbook, apple, amazon and netﬂix to scam consumers. https://unit42.paloaltonetworks.com/\\ncybersquatting/ [Online; accessed on September 15, 2020], 2020.\\n[6] H. Choi, B. Zhu, and H. Lee. Detecting malicious web links and identifying their attack types. In Proc. of the\\n2nd USENIX Conference on Web Application Development (WebApps’11), Portland, Oregon, USA , page 11.\\nUSENIX, June 2011.\\n[7] T. Cover and P. Hart. Nearest neighbor pattern classiﬁcation. IEEE transactions on information theory ,\\n13(1):21–27, January 1967.\\n[8] B. Cui, S. He, X. Yao, and P. Shi. Malicious url detection with feature extraction based on machine learning.'),\n",
       " '9c8b35b4-fbff-4221-9c5c-260e36a3e90a': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 15}, page_content='International Journal of High Performance Computing and Networking , 12(2):75–84, August 2018.\\n[9] Cybersecurity and I. S. Agency. Fbi releases ic3 2019 internet crime report. \"https://us-cert.cisa.\\ngov/ncas/current-activity/2020/02/12/fbi-releases-ic3-2019-internet-crime-report\"\\n[Online; accessed on May 08, 2020], 2019.\\n[10] B. Eshete, A. Villaﬁorita, and K. Weldemariam. Binspect: Holistic analysis and detection of malicious\\nweb pages. In Proc. of the 8th International ICST Conference (SecureComm’12), Padua, Italy , volume\\n106 of Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications\\nEngineering , pages 149–166. Springer, Berlin, Heidelberg, September 2012.\\n[11] fast.ai. About. \"https://www.fast.ai/about\" [Online; accessed on July 29, 2020].\\n[12] fast.ai. fastai v1 for pytorch: Fast and accurate neural nets using modern best practices. https://www.\\nfast.ai/2018/10/02/fastai-ai/ [Online; accessed on September 10, 2020].'),\n",
       " 'a0b46c44-a2f8-4b6c-a72d-96a7a294b22f': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 15}, page_content='[13] Y . Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to\\nboosting. Journal of computer and system sciences , 55(1):119–139, August 1997.\\n[14] J. Howard and S. Gugger. Fastai: A layered api for deep learning. Information , 11(2):108, February 2020.\\n[15] Infosecinstitute. Infosec iq, power to your people. \"https://www.infosecinstitute.com/iq/\" , [On-\\nline; accessed on August 18, 2020].\\n[16] S. Institute. Robust phishing awareness simulation training that changes behavior. \"https://www.sans.\\norg/security-awareness-training/products/phishing\" , [Online; accessed on August 18, 2020].\\n[17] A. Joshi, L. Lloyd, P. Westin, and S. Seethapathy. Using lexical features for malicious url detection–a\\nmachine learning approach. arXiv:1910.06277, October 2019. https://arxiv.org/abs/1910.06277\\n][Online; accessed on September 15, 2020].\\n46'),\n",
       " 'a06088dc-699d-413b-b3a7-c90a1ef57ba1': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 16}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\n[18] N. P. Jouppi, C. Young, et al. In-datacenter performance analysis of a tensor processing unit. In Proc. of the\\n44th Annual International Symposium on Computer Architecture (ISCA’17), Toronto, Canada , pages 1–12.\\nACM, June 2017.\\n[19] A. Karim, S. Azam, B. Shanmugam, K. Kannoorpatti, and M. Alazab. A comprehensive survey for intelligent\\nspam email detection. IEEE Access , 7:168261–168295, November 2019.\\n[20] Keras.io. Keras: The python deep learning api. https://keras.io [Online; accessed on August 09, 2020].\\n[21] S. KP, M. Alazab, et al. Malicious url detection using deep learning. https://www.techrxiv.org/\\narticles/preprint/Malicious_URL_Detection_using_Deep_Learning/11492622 [Online; ac-\\ncessed on September 15, 2020], January 2020.\\n[22] H. Le, Q. Pham, D. Sahoo, and S. Hoi. Urlnet: Learning a url representation with deep learning for malicious'),\n",
       " '0c45fa8d-21c1-47bc-87b8-324dee590110': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 16}, page_content='url detection. arXiv:1802.03162, March 2018. https://arxiv.org/abs/1802.03162 [Online; accessed\\non September 15, 2020]].\\n[23] J. Ma, L. Saul, S. Savage, and G. V oelker. Identifying suspicious urls: An application of large-scale online\\nlearning. In Proceedings of the 26th International Conference on Machine Learning (ICML’09), Montreal\\nQuebec, Canada , pages 681–688. ACM, June 2009.\\n[24] M. Mamun, M. Rathore, A. Lashkari, N. Stakhanova, and A. Ghorbani. Detecting malicious urls using lexical\\nanalysis. In Proc. of the 10th International Conference on Network and System Security (NSS’16), Taipei,\\nTaiwan , volume 9955 of Lecture Notes in Computer Science , pages 467–482. Springer, September 2016.\\n[25] Microsoft. Deﬁning malware: Faq. \"https://docs.microsoft.com/en-us/previous-versions/\\ntn-archive/dd632948(v=technet.10)\" [Online; accessed on August 13, 2020].\\n[26] T. Nash. An undirected attack against critical infrastructure: A case study for improving your control'),\n",
       " '19b37e94-65fc-4dc5-a895-b2fe7fba646d': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 16}, page_content='system security. \"https://us-cert.cisa.gov/sites/default/files/recommended_practices/\\nCaseStudy-002.pdf\" [Online; accessed on August 13, 2020], 2005.\\n[27] U. of Connecticut. Protect yourself from future phishing scams. \"https://phishingeducation.uconn.\\nedu/\" , [Online; accessed on August 18, 2020], 2020.\\n[28] F. B. of Investigation. Business email compromise the $26 billion scam. \"https://www.ic3.gov/media/\\n2019/190910.aspx\" , [Online; accessed on August 18, 2020], 2019.\\n[29] F. Pedregosa, G. Varoquaux, et al. Scikit-learn: Machine learning in python. Journal of Machine Learning\\nResearch , 12(85):2825–2830, 2011.\\n[30] PyTorch. Pytorch. https://pytorch.org [Online; accessed on August 09, 2020].\\n[31] PyTorch. Pytorch 1.3 adds mobile privacy quantization and named tensors. \"https://pytorch.\\norg/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors/\\n#speech-extensions-to-fairseq\" , [Online; accessed on October 09, 2020].'),\n",
       " '41fe0a38-560b-40fe-a2d0-a22724be5726': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 16}, page_content='[32] M. Romagna and N. van den Hout. Hacktivism and website defacement: Motivation, capabilities and poten-\\ntial threats. In Proc. of the 27th Virus Bulletin International Conference, Madrid, Spain , October 2017.\\n[33] D. Sahoo, C. Liu, and S. Hoi. Malicious url detection using machine learning: A survey. arXiv:1701.07179,\\nAugust 2019. https://arxiv.org/abs/1701.07179 [Online; accessed on August 13, 2020].\\n[34] Scikit-Learn. 1.13. feature selection - univariate feature selection. \"https://scikit-learn.org/\\nstable/modules/feature_selection.html#univariate-feature-selection\" [Online; accessed\\non September 15, 2020], 2007.\\n[35] Scikit-Learn. sklearn.feature selection.chi2. \"https://scikit-learn.org/stable/modules/\\ngenerated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2\" [On-\\nline; accessed on September 15, 2020], 2007.\\n[36] TensorFlow. Tensorﬂow. https://www.tensorflow.org [Online; accessed on August 09, 2020].'),\n",
       " '62190b56-4cbd-465f-98af-c5a31bfcc88e': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 16}, page_content='[37] E. Uc ¸ar, M. Incestas ¸, and M. Ucar. A deep learning approach for detection of malicious urls. In Proc. of the\\n6th International Management Information Systems Conference (IMISC’19), Istanbul, Turkey , pages 12–20,\\nOctober 2019.\\n[38] A. Van der Merwe, M. Loock, and M. Dabrowski. Characteristics and responsibilities involved in a phish-\\ning attack. In Proc. of the 4th International Symposium on Information and Communication Technologies\\n(WISICT’05), Cape Town, South Africa , pages 249–254. Trinity College Dublin, January 2005.\\n47'),\n",
       " '9c93d6ac-f9aa-45e0-98f8-5cca2854d3c3': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 17}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\n[39] Verizon. 2020 data breach investigations report. https://enterprise.verizon.com/resources/\\nreports/2020-data-breach-investigations-report.pdf [Online; accessed on May 08, 2020].\\n[40] R. Verma and A. Das. What’s in a url: Fast feature extraction and malicious url detection. In Proc. of the 3rd\\nACM on International Workshop on Security and Privacy Analytics (IWSPA’17), Scottsdale, Arizona, USA ,\\npages 55–63. ACM, March 2017.\\n[41] J. Zhao, N. Wang, Q. Ma, and Z. Cheng. Classifying malicious urls using gated recurrent neural networks.\\nInProc. of the 12th International Conference on Innovative Mobile and Internet Services in Ubiquitous\\nComputing (IMIS’18), Kunibiki Messe, Matsue, Japan , volume 773 of Advances in Intelligent Systems and\\nComputing , pages 385–394. Springer, Cham, June 2018.\\n——————————————————————————\\nAuthor Biography'),\n",
       " 'ce5e719b-3618-4efa-bbe2-2e6588a2d2d5': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 17}, page_content='Author Biography\\nClayton Johnson is a senior undergraduate pursuing his Bachelor’s in Computer Sci-\\nence from Colorado Mesa University (CMU) and earned his Professional Certiﬁcate\\nin Cybersecurity from CMU in 2019. He is the former president of the CMU’s Com-\\nputer Science club and is a research fellow at the Cybersecurity Center at CMU. Clay-\\nton will begin pursing his Master’s in Technology, Cybersecurity, and Policy at the\\nUniversity of Colorado Boulder in 2021.\\nBishal Khadka is a senior undergraduate student pursuing his Bachelor’s in Com-\\nputer Science and Professional Certiﬁcate in Cybersecurity degrees at Colorado Mesa\\nUniversity (CMU). Bishal is currently the president of Cybersecurity club and a re-\\nsearch fellow at the Cybersecurity Center at CMU.\\nRam B. Basnet is an associate professor of Computer Science at Colorado Mesa Uni-\\nversity (CMU). He received his BS in Computer Science from CMU in 2004 and MS'),\n",
       " 'aeede84b-3467-4115-bbba-27efd7e95a84': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 17}, page_content='and PhD in Computer Science from New Mexico Tech in 2008 and 2012, respectively.\\nHis research interests are in the areas of information assurance, machine learning, and\\ncomputer science pedagogy.\\nTenzin Doleck received his PhD from McGill University in 2017. He is currently a\\npost-doctoral fellow at the University of Southern California.\\n48'),\n",
       " '6fbe233c-ad9e-458b-9ca2-e37f1534f73d': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 0}, page_content='Towards Detecting and Classifying Malicious URLs\\nUsing Deep Learning\\nClayton Johnson1, Bishal Khadka1, Ram B. Basnet1*, and Tenzin Doleck2\\n1Colorado Mesa University, Grand Junction, CO 81501, USA\\n{cpjohnson, bkhadka }@mavs.coloradomesa.edu, rbasnet@coloradomesa.edu\\n2University of Southern California, Los Angeles, CA 90007, USA\\ndoleck@usc.edu\\nReceived: September 30, 2020; Accepted: December 11, 2020; Published: December 31, 2020\\nAbstract\\nEmails containing Uniform Resource Locators (URLs) pose substantial risks to organizations by po-\\ntentially compromising both credentials and network security through general and spear-phishing\\ncampaigns to their employees. The detection and classiﬁcation of malicious URLs is an important\\nresearch problem with practical applications. With an appropriate machine learning model, an orga-\\nnization may protect itself by ﬁltering incoming emails and the websites its employees are visiting'),\n",
       " '1e38fb70-0dd2-474f-8593-d674ed7d984b': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 0}, page_content='based on the maliciousness of URLs contained in emails and web pages. In this work, we com-\\npare the performance of traditional machine learning algorithms, such as Random Forest, CART,\\nand kNN against popular deep learning framework models, such as Fast.ai and Keras-TensorFlow\\nacross CPU, GPU, and TPU architectures. Using the publicly available ISCX-URL-2016 dataset,\\nwe present the models’ performances across binary and multiclass classiﬁcation experiments. By\\ncollecting accuracy and timing metrics, we ﬁnd that Random Forest, Keras-TensorFlow, and Fast.ai\\nmodels performed comparably and with the highest accuracies >96% in both the detection and\\nclassiﬁcation of malicious URLs, with Random Forest as the preferable model based on time, perfor-\\nmance, and complexity constraints. Additionally, by ranking and using feature selection techniques,\\nwe determine that the top 5-10 features provide the best performances compared to using all the fea-\\ntures provided in the dataset.'),\n",
       " '8bf83f00-45f3-4142-afc4-1ac1f158bb32': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 0}, page_content='Keywords : Malicious URLs, Phishing URLs, Deep Learning, Web Security, Machine Learning\\n1 Introduction\\nPhishing—along with its more targeted version, spear phishing—is a social engineering attack in which\\nthe attacker attempts to compromise a user’s credentials or a system by presenting itself as a legitimate\\nbusiness communication [38]. These communications, most commonly emails, will often contain links\\nto websites controlled by the attacker that attempt to: 1) mimic a popular website to scrape the user’s cre-\\ndentials, 2) install malware onto the user’s system, or 3) spam the user. These links with malicious intents\\nare called malicious URLs. According to Verizon’s 2020 Data Breach Investigations Report [39], phish-\\ning attacks have been in the top three types of data breaches for the past 6 years and have been number\\none for the past two years, with around 96% of the social engineering attacks conducted through email.'),\n",
       " '4d92bb3d-88d2-44d0-a411-c86c32332663': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 0}, page_content='There is a clear increase in popularity with these types of attacks, highlighted by the Internet Crime Com-\\nplaint Center (IC3) Public Service Announcement on Business Email Compromises (BECs) [28]. The\\nJournal of Wireless Mobile Networks, Ubiquitous Computing, and Dependable Applications (JoWUA) , 11(4):31-48, Dec. 2020\\nDOI:10.22667/JOWUA.2020.12.31.031\\n*Corresponding author: Department of Computer Science and Engineering, Colorado Mesa University, Grand Junction, CO\\n81501, USA, Tel: +1-970-248-1400\\n31'),\n",
       " '7340311d-be8e-43c1-8073-dcd3e1b9856b': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 1}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nIC3’s 2019 Internet Crime Report [9] reported that losses from BECs and Email Account Compromises\\n(EACs) were in excess of $1.7 billion. The threat posed to an organization by these types of attacks is\\nclearly substantial, with trends indicating increase in popularity and severity over time. To address the\\nthreat phishing and malicious URLs pose to businesses, many popular websites and educational insti-\\ntutions have added “Phishing Awareness” programs, as exempliﬁed by the following examples: SANS\\nInstitute [16], InfoSec Institute [15], and University of Connecticut [27]. While providing training, edu-\\ncation, and awareness on phishing attacks have become trivial and common, such efforts are not adequate\\nto protect users. This warrants the need to apply machine learning algorithms to detect malicious URLs\\nbefore they arrive at intended targets within an organization, thus decreasing the efforts required from'),\n",
       " '3ed91760-0797-4fc6-94f2-67bfdfae3c9f': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 1}, page_content='the user. The problem of detection and classiﬁcation of URLs is illustrated in Figure 1.\\nFigure 1: Detection and Classiﬁcation of URLs Problem Overview\\nThe contributions of this work are twofold: 1) we offer a direct comparison of popular deep learn-\\ning frameworks (Keras and Fast.ai) against traditional machine learning algorithms (Random Forest,\\nClassiﬁcation and Regression Tree, k-Nearest Neighbor, Support Vector Machine, Logistic Regression,\\nLinear Discriminant Analysis, AdaBoost, and Naive Bayes) in the detection and classiﬁcation of ma-\\nlicious URLs; and 2) we expand on the literature utilizing the public ISCX-URL-2016 dataset, which\\ncontains lexical features of malicious URLs. The remainder of this research is structured as follows. The\\nRelated Works section will explore recent advancements and contributions towards the detection and\\nclassiﬁcation of malicious URLs, as well as highlight any research gaps within the ﬁeld. Following this,'),\n",
       " '4cbe542f-7c4a-4ba3-a6a2-788cfd459e90': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 1}, page_content='the machine learning and deep learning frameworks utilized in the work are discussed in the Frameworks\\nsection. The ISCX-URL-2016 dataset will be discussed in depth before the presentation and explana-\\ntion of the experiments and results. Finally, the paper will conclude and suggest trajectories for further\\nresearch.\\n2 Related Works\\nWe summarize indicative studies examining malicious URLs. We follow this with a discussion of some\\nof the patterns seen across the literature and the research gaps.\\nMamun et al. [24] presented a new, public malicious URLs dataset called ISCX-URL-2016. Ad-\\nditionally, Mamun et al. experimented with the usage of RF, C4.5, and kNN algorithms to detect and\\nclassify malicious URLs on this new dataset by generating features directly from the URL, such as URL\\nlength, domain entropy, arguments, etc. In detecting malicious URLs, RF performed the highest with\\n32'),\n",
       " '317165da-a4bd-4347-9070-156d690b4145': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 2}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\n>0.99 precision and recall; however, both C4.5 and kNN models performed with >0.97 precision and\\nrecall. When classifying malicious trafﬁc, RF achieved the highest performance with recall and preci-\\nsion of 0.97. In this second classiﬁcation phase, all models perform with precision and recall in the range\\n0.92-0.97. Finally, the authors demonstrated that the obfuscation of malicious URLs does decrease the\\ndetection and classiﬁcation accuracies of the ML models used.\\nCui et al. [8] proposed a system to detect malicious URLs by using NB, DT, and SVM classiﬁers. The\\nreported accuracies >98.7% for all classiﬁers. Due to the high-performance of the models, the authors\\nclaim to have deployed the system, analyzing up to 2 TB worth of data per day. This work was extended\\nby Zhao et al. [41], who compared the performance of RF and a Gated Recurrent Neural Network (GRU)'),\n",
       " '3184b266-5a1d-4a7a-8466-a47516e6b6a4': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 2}, page_content='over the same dataset used in [8] in detecting malicious URLs. Zhao et al. found that the GRU model\\nwith 98.5% accuracy outperformed the RF model with 96.4% accuracy. This trend was seen across\\nvarying sizes of training datasets (from 600 to 240,000 samples). They also presented graphs showing\\nthe distributions of each classiﬁcation (Legitimate, SQL Injection, XSS Attack, Sensitive File Attack,\\nand Directory Traversal) based on the “Number of Characters” feature. Additional statistical analysis of\\neach feature may shed more light into understanding the malicious URLs detection problem—one of the\\ncontributions of this research study.\\nChoi et al. [6] experimented with the detection and classiﬁcation of malicious URLs using SVM\\nfor binary classiﬁcation and C4.5, Label Powerset, and kNN classiﬁers for multiclass classiﬁcation. The\\nfeatures used for this system contain lexical characteristics, link structures, DNS, network trafﬁc, and'),\n",
       " '8783cc42-e523-4fc0-b3c3-9bcdecc42933': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 2}, page_content='content composition information. The detection of malicious trafﬁc demonstrated high performance\\nwith SVM’s accuracy exceeding 98%. The work is claimed to be the ﬁrst report of malicious URLs\\nclassiﬁcation across phishing, spamming, and malware categories. Their models for multi-class classiﬁ-\\ncation performed with accuracies >93%. Additionally, the article states that due to the details the model\\ntook into account, obfuscation techniques such as redirection, link manipulation, and fast-ﬂux hosting\\nwould hinder the effectiveness in detecting malicious URLs.\\nMa et al. [23] presented a novel technique for detecting malicious URLs by utilizing continuous,\\nonline machine learning techniques. The work experimented with Logistic Regression, SVM, NB, Con-\\nﬁdence Weighted (CW), and Perceptron models. By combining features such as DNS information, lex-\\nical characteristics, web registration dates, etc. with blacklist information, the team was able to compile'),\n",
       " '9ed4cc96-1106-454f-a5b1-55fb0cd34ac9': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 2}, page_content='a total of 2.9 million features over the course of 100 days. The models were trained using a sliding\\nwindow of two weeks’ worth of training data. While all models showed decreasing error rates over the\\n100 day period, CW consistently maintained the lowest error rates with a minimum of 1%. Due to the\\ndynamic landscape of the malicious URLs problem, Ma et al. addressed the need for large, fresh datasets\\nto emphasize and encourage the continuous training of models.\\nUc ¸ar et al. [37] applied two deep learning models, Long Short-Term Memory (LSTM) and Con-\\nvolutional Neural Network (CNN), in the detection and classiﬁcation of malicious URLs. Detection of\\nmalicious URLs demonstrated high performance, with accuracies of 97.25% and 98.86% for the LSTM\\nand CNN models, respectively. The CNN model showed an accuracy of 95.37% on the classiﬁcation of\\nmalicious URLs, higher than 91.13% for LSTM. The work is signiﬁcant for two reasons. First, there'),\n",
       " 'fced239a-b1b1-4a93-9511-ca02d99fe690': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 2}, page_content='are few applications of deep learning in this research problem and secondly, few works have used the\\nISCX-URL-2016 dataset.\\nVinayakumar et al. [21] evaluated the performance of multiple deep learning architectures (LSTM,\\nCNN, Bidirectional Recurrent Structures, LSTM-CNN hybrid, and Stacked CNNs) against their pro-\\nposed deep learning architecture in the detection of malicious URLs. The architectures explored were\\nimplemented through Keras, described in the Frameworks section. The two-part dataset includes data\\nfrom Alexa.com, DMOZ Directory, Sophos, and other related sources. The models perform in the range\\nof 95 - 99% accuracy over the ﬁrst dataset, with LSTM as the highest performing model. The authors\\nplace an emphasis on the difference between the random-split and time-split versions of the dataset,\\n33'),\n",
       " '47e77d1c-49fa-4dfc-87b4-27e317f69704': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 3}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nshowing that there is a higher variance in the accuracy results from the time-split than the random-split\\nversions of the second dataset. The models’ accuracy ranges from 95-96.6% in the random-split section\\nof the second dataset, while the time-split version has models ranging from 93-97.1% accuracy.\\nSahoo et al. [33] conducted a survey of the work done within the realm of detecting and classifying\\nmalicious URLs. The work discussed in depth the multitude of issues with existing, commercial blacklist\\nand heuristic solutions, which lead to ungeneralized, nonﬂexible models that are susceptible to novel or\\nobfuscated attacks. For simplicity and security, most of the recent efforts have been focused on static\\nanalysis of URLs using various machine learning techniques. Almost half of the works analyzed utilized'),\n",
       " 'a0e8f719-39e6-44d8-bc17-b7c5407da009': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 3}, page_content='either lexical or host-based features [33]. There is a clear preference for these types of features within\\nthe ﬁeld, while other works are spread out among HTML, JavaScript, and Context-based features. While\\nthere have been some efforts to apply deep learning architectures, such as LSTMs and CNNs to the\\nmalicious URLs problem, computational complexity is the largest constraint [33]. However, there are\\nfew works exploring and comparing the results of deep neural network frameworks against traditional\\nmachine learning algorithms such as RF, SVM, kNN, etc, in the context of the malicious URLs problem.\\nThis is the primary contribution of our work.\\nHung et al. [22] performed a series of deep learning experiments and neural networks like Convolu-\\ntional Neural Networks (CNN) so that the model would learn URL embedding. They use the large dataset\\nfrom VirusTotal anti-virus group. The team attempted to ﬁnd the limitations in identifying the malicious'),\n",
       " '0017c1f0-ef62-4c33-bac8-8c9f61f756a5': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 3}, page_content='URLs using lexical properties like inability to identify the proper semantic meaning and patterns in URL\\nand they use end-to-end deep learning framework to address this kind of problem. The team addressed\\nthe problem of identifying whether a URL is malicious or not by formulating a problem as a binary\\nclassiﬁcation task [22]. They use the technique called character-level CNN which learns the frequency\\nand sequence of the characters in the URL. Moreover, word-level CNN is also applied for identifying the\\nunique words in the URL. The main goal of the paper was to use character-level and word-level CNN to\\naddress the limitations produced by previous methods and precisely identify the malicious URLs.\\nBirhanu et al. [10] employed a technique called BINSPECT, which is a combination of static analysis\\nand minimalistic emulation and uses supervised learning techniques, for detecting malicious web pages.'),\n",
       " '1c320483-cb81-4f0f-a191-03bacb55ff3b': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 3}, page_content='The team was able to generate accuracy of more than 97% when using this technique with low false\\nsignals [10]. Furthermore, the team also mentioned that, with this approach, it took them only 3-5\\nseconds to analyze a single web page.\\n3 Motivation\\nWhen addressing the problem, most of the works available attempt to only detect malicious URLs, while\\nfew works experiment with classifying malicious URLs. While detection is a substantial issue, the\\nclassiﬁcation of these malicious URLs indicates where the direction and priority of actions within an\\norganization should be to best protect its network. For instance, defaced URLs may require more urgent\\nreaction due to potential credential compromises than spam URLs, which don’t need immediate action.\\nThere is a clear consensus that blacklisting malicious URLs is a poor solution since it fails to address\\nissues such as obfuscation or novel attacks. As a consequence of this, most of the recent work has'),\n",
       " 'cbc34b66-0d8a-4671-9036-09a3246f5a3c': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 3}, page_content='focused on applications of machine learning algorithms. Of the popular algorithms, Random Forest (RF)\\nappears to be one of the most effective. The survey of relevant work revealed few studies that address\\napplications of deep learning within the ﬁeld and even fewer that directly compare the performance of\\ndeep learning models to traditional machine learning models. This is a key gap in the literature that the\\npresent study aims to address. Additionally, this work compares various metrics, such as training and\\nprediction times, across multiple architectures (CPU, GPU, and TPU) to determine which model would\\nbe most practical to use in an industry setting.\\n34'),\n",
       " '854e09f4-18f6-4a6b-bd41-ca368f39ac80': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 4}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nThe features used in the ﬁeld are varied and derived from many different sources. Despite this,\\nfeatures may be derived from both static analysis of the URL and dynamic analysis from visiting the\\nwebsite. Most of the research works analyze static features for security and cost reasons. It is safer and\\ncomputationally cheaper to statically analyze a suspicious URL than visit the website and execute the\\nattacker’s code, which may take an arbitrary amount of time to complete. The majority of the related\\nworks presented create custom datasets with different types of features (statistical and lexical, host-\\nbased, WHOIS, etc.), demonstrating a lack of standardization within the datasets used for the research\\nproblem. One of the reasons for our usage of the ISCX-URL-2016 dataset is to show support for usage\\nof a standardized, lexicon-based dataset for static analysis of suspicious URLs. This work explores the'),\n",
       " 'eae0ae67-31fe-405f-93e2-3d1d3818af78': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 4}, page_content='effectiveness of the provided URL attributes and demonstrates the effectiveness lexical analysis may\\nhave in detecting malicious URLs, as well as the limitations.\\n4 Machine and Deep Learning Frameworks\\n4.1 fast.ai\\nfast.ai is one of the famous python libraries which includes various machine learning and deep learning\\nlibraries. fast.ai is a Python module that implements a high-level API to a PyTorch backend. The goal of\\nfast.ai is to easily allow experts in other ﬁelds, such as virologists or astronomers, to implement popular\\ndeep learning techniques within their respective settings. This framework has seen multiple popular\\nsuccesses in research and industry [11].\\n4.2 Keras\\nKeras is a deep learning framework for Python that allows for the implementation of both TensorFlow\\nand Theano in the backend [20]. Keras runs Tensorﬂow 2.0 in the backend for easy machine learning\\nworkﬂow and for proper data management. Keras allows for easy and fast prototyping through user'),\n",
       " '341ab505-2562-4e3c-bb36-ae5ebabb0649': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 4}, page_content='friendliness, modularity, and extensibility. It usually runs seamlessly on CPU and GPU.\\n4.3 TensorFlow\\nTensorﬂow was originally developed by Google and it is free to use. Tensorﬂow uses static computa-\\ntional graphs also known as the deﬁne-and-run approach. The libraries and large community resources\\navailable in the Tensorﬂow allows its user to build powerful and advanced machine learning models and\\napplications. It builds and trains ML models easily using intuitive high-level APIs like Keras with ea-\\nger execution, which makes for immediate model iteration and easy debugging [36]. One of the key\\naffordances of Tensorﬂow is that it not only uses CPU but also GPU which helps in gaining much more\\ncomputing power. Tensorﬂow 2.0 further uses TPU, also known as Tensor Processing Unit, which adds\\ncomputational power and improves performance. Using this deep learning module, even fairly compli-\\ncated models can be created with very little coding effort.\\n4.4 PyTorch'),\n",
       " '532afafc-7c8f-4819-bcfe-2481cb6bb8b3': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 4}, page_content='4.4 PyTorch\\nPyTorch is an open source machine learning framework that accelerates the path from research prototyp-\\ning to production deployment [30]. This machine learning framework was ﬁrst developed by Facebook.\\nPyTorch uses dynamic computational graphs which lets the user process variable length input and out-\\nput. PyTorch has many libraries like captum, pytorch geometric, and skorch which are all open source\\nand help in the tasks like model interpretability, performing deep learning on irregular input data, and\\nproviding scikit-learn compatibility.\\n35'),\n",
       " 'a14a99ef-31a5-4165-8d1e-a9301726c50d': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 5}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\n4.5 Scikit-Learn\\nScikit-learn (sklearn)—a python library built upon Scipy, Numpy, and matplotlib—is a handy tool for\\npredictive data analysis [29]. This cutting edge software is free to use for the public. Common machine\\nlearning tasks, such as classiﬁcation, regression, and clustering, can be easily tackled using scikit-learn.\\nFor some machine learning classiﬁers like Support Vector Classiﬁcation and Lasso, scikit-learn outper-\\nforms other python machine learning libraries [29].\\nThe models used from the Scikit-learn module are the RandomForestClassiﬁer, Decision Tree (op-\\ntimized CART algorithm), KNeighborsClassiﬁer, SVC (Support Vector Machine), LogisticRegression,\\nLinearDiscriminantAnalysis, AdaBoostClassiﬁer, and GaussianNB (Naive Bayes).\\n5 Dataset\\nThis experiment presents ISCX-URL-2016 URL [24] dataset. Around 78 lexical features were extracted'),\n",
       " '8373206c-b5b1-4f6a-8c1b-be27a56a3bb2': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 5}, page_content='from URLs, and 5 URL classes such as benign, defacement, malware, phishing, and spam were labeled\\nin the dataset. The different classes of URLs are brieﬂy introduced below.\\nBenign URLs: Benign URLs are legitimate URLs that do not lead to any infectious websites and do\\nnot try to inject the user’s computer with any kind of harmful malware. Benign websites may contain\\nadvertisements and adware which are typically harmless to a computer.\\nDefacement URLs: Website defacement usually means changing a certain aspect of the website\\nsuch as it’s visual appearances and some contents on it. Hacktivists try to deface a website for numerous\\nreasons [32]. This kind of act is done when some content in the web page needs to be changed without\\nthe permission of the original owner of the website which technically means penetrating a website.\\nMalware URLs: Malware URLs take a user to the malicious website that typically installs some'),\n",
       " '9fc1cb93-f2f6-4ff3-b9d5-be64b794c8e8': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 5}, page_content='malware on that user’s device which can be used for identity theft, corrupting ﬁles, and even logging\\nkeystrokes. Malware can indirectly be a dangerous software that can harm a computer and steal some-\\none’s private information [25]. Some threats such as harmful biological agents, a terrorist cell intent on\\ndisrupting operations, etc. can be considered as malware [26]. Some of the examples of malware are\\nransomware, spyware, scareware, among others.\\nPhishing URLs: Phishing URLs conventionally entice a user to visit a fake website and will try to\\nsteal as much information they can get from the user. Sometimes a user can easily be led to phishing\\nwebsites just by having a typo in a URL. Phishing can be deﬁned as the intent of the hacker to steal some\\nprivate information like credit card number and other digital identity by employing social engineering\\ntechniques [40].\\nSpam URLs: Spam is a way of sending unsolicited emails to the user with the intent of advertise-'),\n",
       " 'af1ee550-b05d-4a38-9ab6-fee0a02e28bd': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 5}, page_content='ments or for serious harm to the computer [19]. Spam URLs are usually seen in the spam email. Some\\nspam URLs can be harmful and can infect the computer of the user with spyware and adware.\\n5.1 Lexical Analysis\\nMalicious URLs may contain some pattern in their URL text which gives us a hint that the URL is not\\nlegitimate. Various lexical features such as query length, domain token count, path token count, URL\\nlength, domain length, path length, and many more were used in the experiment for better performance\\nand results. Let’s consider the following URL to extract lexical features, e.g.:\\nhttp://www.example.site.com/path dir1/path dir2/ﬁle.php\\nThe URL is 56 characters long, thus the ‘URLLen’ feature is 56. The length of the domain name\\n“domainlength” example.site.com is 16, “domainUrlRatio” is 0.2857, and “NumberofDotsinURL” is 4.\\nThis process of extracting all 78 lexical features is thoroughly discussed in [24].\\n36'),\n",
       " 'a3718489-e21f-41ee-bd35-62040de5cfc7': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 6}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nAs discussed previously in the Related Works section, traditional blacklisting techniques have clear\\ndisadvantages in the detection and classiﬁcation problems. Thus, taking a closer look at lexical analysis\\nwould be a better choice for identifying those URLs.\\n5.2 Data Preprocessing\\nOriginal dataset had a total sample of 36,707. From this, many entries were NaN, inﬁnity, or empty.\\nAll the entries and attributes with NaN, Inﬁnity, and missing values were eliminated from the dataset\\nwith the aim of getting more precise results. During the data cleansing process, around 17K rows with\\nNaN and redundant values were dropped from the dataset. In addition, 7 attributes of columns with NaN\\nvalues were eliminated leaving only 72 attributes in the dataset. Table 1 illustrates the number of samples\\nbefore and after data cleanup.\\nURL Type Raw Samples Filtered Samples\\nBenign 7,781 2,709\\nDefacement 7,930 2,477'),\n",
       " '1b6ce932-f684-445c-8f40-32bb72b193e5': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 6}, page_content='Malware 6,712 4,440\\nPhishing 7,586 4,014\\nSpam 6,698 5,342\\nTable 1: Number of samples before and after data cleanup\\n6 Experiment and Results\\nThe experiments demonstrate a two-layered approach to the malicious URLs research problem: (1)\\nExperiment 1 involves the detection of malicious URLs and (2) Experiment 2 involves the classiﬁcation\\nof malicious URLs. Experiment 1 is a binary classiﬁcation problem with two classes: Benign and\\nMalicious. All non-benign categories of URLs are labelled as malicious. Experiment 2 is a multi-class\\nclassiﬁcation problem where malicious URLs are categorized into speciﬁc classes such as: Defacement,\\nMalware, Phishing, and Spam.\\nBoth the experiments utilize the same set of machine learning models: Random Forest [3] (RF),\\nDecision Tree/CART [4] (DT), k-Nearest Neighbors [7] (kNN), Support Vector Machine (SVM), Lo-\\ngistic Regression (LR), Linear Discriminant Analysis (LDA), AdaBoost [13] (AB), Naive Bayes (NB).'),\n",
       " '33105ffd-6a97-40fb-af2e-a6d0096c020f': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 6}, page_content='In addition, two deep learning models, Fast.ai [14] and Keras-TensorFlow [20] [36] [1], are also used.\\nThe models are trained and evaluated on the ISCX-URL-2016 dataset using stratiﬁed 10-fold cross val-\\nidation. All experiments were conducted on the free tier of Google Colaboratory system using Jupyter\\nNotebooks. The Jupyter Notebooks and scripts used for the experiments can be found on GitHub.com\\n[2].\\n6.1 Performance Metrics\\nVarious metrics are collected to compare and determine the most appropriate model meeting real-world\\ndeployment constraints. Initially, all models are trained and evaluated on both Experiments 1 and 2, gen-\\nerating accuracy using 10-fold cross validation. Confusion matrices are presented to clearly demonstrate\\nthe performance of the DNN models. An additional experiment is performed on the highest-performing\\ntraditional machine learning and the DNN algorithms in which the benign samples are removed from the'),\n",
       " '69528255-0f16-4152-a15f-261abf6c1168': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 6}, page_content='multi-class classiﬁcation problem. The goal of the experiment is to see how the best classiﬁers would\\n37'),\n",
       " 'ec193249-f18a-4e36-b628-0a614e5ff886': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 7}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nperform on multi-class classiﬁcation of just the malicious URLs. Then, time metrics are calculated on\\nthese three models. These time metrics describe two different characteristics of the models. These time\\nvalues are collected across CPU, GPU, and TPU architectures to further assist an organization’s invest-\\nment decisions in projects in which architecture is a consideration.\\nThe ﬁrst time metrics used to evaluate the models is the total time required to train each model. This\\ntraining time metric is calculated by summing the total training time elapsed for each fold. Training time\\nexcludes object instantiation and testing since the focus is on training speciﬁcally. Even though training\\nis often described as a single event, training time is included as a metric because retraining with new data\\nmay occur multiple times for a model implemented by a system. Thus, the total amount of time required'),\n",
       " 'c05a5dd2-632f-4a4d-9f09-f4b710c092e2': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 7}, page_content='to train a model has a direct impact on the organization.\\nThe second time metric is the average time required for a model to predict the classiﬁcation of a pro-\\nvided sample. If a model was deployed and used to predict the malevolence of a URL, the time required\\nto generate this prediction has a clear impact on the ability of the organization to ﬁlter URLs. This effect\\nis exponential for larger organizations with growing attack surfaces. For simplicity, we assume that each\\ndeployed model would predict on a single sample at a time, as opposed to batches of samples. Addition-\\nally, since this metric is calculated by timing the ‘predict’ method of each object on samples from the\\nISCX-URL-2016 dataset, we do not report the time required to generate the features used in the dataset.\\nHowever, time required to generate features may be minimized by balancing the trade-off between the\\nnumber of features used and reported accuracy, as discussed in the feature analysis section.\\n6.2 Results'),\n",
       " '020ec790-f497-41fd-abe3-1c19fce95c48': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 7}, page_content='6.2 Results\\nAs seen in Table 2, Random Forest demonstrates superior accuracy among all machine learning algo-\\nrithms used for both the experiments. This result is well documented in the literature [21, 40] and further\\nsupported by our experiments. Both DNN models show high accuracy in Experiment 1, performing sim-\\nilar to DT, kNN, and AB, while still being eclipsed by RF. Experiment 2 sees performance decreases for\\nall models except Fast.ai, with both Fast.ai and RF performing comparably. Keras-TensorFlow’s accu-\\nracy decreased substantially to 91.5%, around ﬁve percentage points lower than that from RF and Fast.ai.\\nAn additional observation is the higher standard deviation for Keras-TensorFlow.\\nClassiﬁer Binary-Class Accuracy (%) Multi-Class Accuracy (%)\\nRF 98.68±0.22 96.26±0.53\\nDT 97.63±0.24 92.81±0.62\\nkNN 97.47±0.39 92.52±0.79\\nSVM 93.96±0.60 80.77±0.63\\nLR 90.50±0.30 69.54±0.97\\nLDA 94.34±0.33 82.31±0.78\\nAB 96.21±0.39 76.58±1.60\\nNB 68.73±0.83 59.55±0.91\\nfast.ai 96.88±0.33 96.77±0.51'),\n",
       " '02f21331-c29d-46d7-8187-8f21cdf34934': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 7}, page_content='Keras-TensorFlow 97.13±1.93 91.45±2.94\\nTable 2: Accuracy Results for Binary and Multi-Class Classiﬁcation\\nRemoving the benign samples contained in the multi-class dataset, as seen in Table 3, appears to\\nhave little effect on the performance of the classiﬁers. While all three classiﬁers appear to perform better\\nwithout the benign samples, the performance is within one standard deviation of the previous results.\\nThus, this improvement appears to be insigniﬁcant. Nevertheless, RF, Fast-AI, and Keras-TensorFlow\\nclassiﬁers clearly demonstrate high accuracy metrics when classifying malicious URLs.\\n38'),\n",
       " 'e227d3a2-567c-4503-ae21-825b092b9074': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 8}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nClassiﬁer Accuracy (%)\\nRF 96.99±0.50\\nfast.ai 97.55±0.37\\nKeras-TensorFlow 93.81±2.34\\nTable 3: Top Classiﬁer’s Accuracies Results for Multi-class Malicious URL Classiﬁcation (without Be-\\nnign Samples)\\nFigure 2: Confusion Matrices for Keras-TensorFlow and Fast.ai DNN Models for Experiment 1\\nThe confusion matrices for the deep learning models are presented in Figure 2 and Figure 3. While\\nan overwhelming majority of the samples are classiﬁed correctly—seen through the main diagonal of\\nthe matrix— the Phishing URL classiﬁcation has the clearest rate of failure. Both deep learning models\\ndemonstrate a pattern of both over- and under-predicting Phishing URLs.\\nClassiﬁer Experiment CPU GPU TPU\\nRF Binary 75.83 61.58 95.19\\nMulti 87.26 71.02 106.83\\nfast.ai Binary 323.36 221.52 360.47\\nMulti 320.91 220.00 362.29\\nKeras-TensorFlow Binary 445.78 135.63 123.17\\nMulti 451.37 135.65 124.09'),\n",
       " '9173b5fc-045e-4a23-adf1-7ba3e60ebba9': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 8}, page_content='Table 4: Training Time across Runtime Environments in Seconds\\nTable 4 presents the total time taken for Random Forest and the DNN models to train across the 10\\nfolds on a CPU, GPU, and TPU run-time environments. Keras-TensorFlow demonstrates its lowest train\\ntimes on the TPU, as expected since Google develops both TensorFlow and the TPU architecture [18].\\nHowever, the best metric for Keras-TensorFlow is still twice the fastest time recorded for RF. Both RF and\\nFast.ai have their best training times on the GPU, which is likely because there is currently no support\\nor documentation for the TPU architecture through Scikit-Learn or Fast.ai’s PyTorch version (Fast.ai\\n39'),\n",
       " '13266901-9802-4329-bce5-10288c47ab70': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 9}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nFigure 3: Confusion Matrix for Keras-Tensorﬂow (left) and Fast.ai (right) for Multi-Class Classiﬁcation\\nExperiments\\nClassiﬁer Experiment CPU GPU TPU\\nRF Binary 8.12±0.77 6.61±0.63 7.99±0.74\\nMulti 8.30±1.09 6.66±0.73 8.02±0.66\\nfast.ai Binary 53.17±9.95 44.29±6.65 54.59±8.888\\nMulti 54.62±17.31 44.34±4.98 54.53±11.05\\nKeras-TensorFlow Binary 38.53±4.71 28.81±5.51 41.52±4.30\\nMulti 40.20±5.56 28.54±4.48 41.49±4.32\\nTable 5: Average Time to Predict Classiﬁcation of a Sample in Milliseconds\\ncurrently implements PyTorch v1, however Fast.ai v2 claims it will support TPU through PyTorch v1.3+)\\n[31] [12] [14]. While Keras-TensorFlow trains faster than Fast.ai, there appears to be a large performance\\nhit, especially for Experiment 2. There is little change in performance times based on the experiment\\nconducted.\\nThe times presented in Table 5 show the average time for the RF and DNN models to predict a'),\n",
       " '954dc8ff-a709-4fda-b82b-d199f82a8d1a': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 9}, page_content='classiﬁcation for a given sample. As expected, there is a uniform drop in average time from CPU to\\nGPU. The TPU architecture, on the other hand, appears to have mixed results on the prediction times,\\nleading to all three models showing the best prediction times on the GPU. Finally, Random Forest appears\\nto be the most consistent with standard deviations less than 1.09 ms. This result is in extreme contrast\\nto the DNN models, which experience standard deviations ranging from 4.30 ms to 17.31 ms. These\\nmetrics appear to have mixed results depending on the experiment conducted.\\n6.3 Feature Analysis\\nThe features in the ISCX-URL-2016 dataset are analyzed to shine further light onto the subject of de-\\nveloping efﬁcient mechanisms to detect and classify malicious URLs. There are a total of 78 features\\navailable from the dataset. These features naturally vary in their predictive capabilities and thus their\\n40'),\n",
       " '33843724-afbe-4e7d-a74b-08a4a0acfc1b': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 10}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nusefulness for a machine learning algorithm. By applying the chi-squared test [29] [35] [34], we rank\\nthe features with highest correlation to the classiﬁcation of a malicious URL sample. The chi-squared\\nfeature ranking technique is chosen for its simplicity and availability through sklearn [35]. Additionally,\\nthe p-value calculated by the test for each feature clearly indicates its importance in classiﬁcation prob-\\nlems, where high p-values indicate independence from the target classiﬁcation and are thus poor-quality\\nfeatures and vice-versa. The effect of this process is two-fold. Using this feature selection technique,\\nwe decrease noise within a dataset and decrease the resources required to generate a prediction on a\\nsample in a real-world setting. If a model handles noisy data poorly and thus performs poorly, this will'),\n",
       " '978ab296-f5d7-4246-8c14-2d2658661964': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 10}, page_content='help remedy its sensitivity to the data. Additionally, when trying to determine if a URL in an email or a\\nwebsite is malicious, less time and computation is spent generating characteristics that are unused or not\\nas useful.\\nRank Binary-Class Feature Binary-Class p-value Multi-Class Feature Multi-Class p-value\\n1 ﬁleNameLen 1.406341e-70 Entropy Afterpath 0\\n2 domain token count 1.636373e-41 argPathRatio 1.397820e-290\\n3 tld 1.636373e-41 NumberRate AfterPath 1.983143e-290\\n4 SymbolCount Domain 1.678641e-41 NumberRate Domain 5.645027e-279\\n5 Entropy Afterpath 3.584071e-41 ArgUrlRatio 3.924478e-272\\n6 delimeter path 9.681864e-40 Extension DigitCount 6.874504e-171\\n7 argPathRatio 9.901046e-38 dldgetArg 1.534782e-147\\n8 Entropy Filename 2.679810e-37 ldlgetArg 1.714726e-147\\n9 Entropy DirectoryName 7.487584e-36 Query DigitCount 1.118545e-135\\n10 Filename LetterCount 3.891527e-33 LongestVariableValue 2.788998e-135\\n...\\n69 Entropy URL 0.161457 ldldomain 7.808014e-11'),\n",
       " 'd5a59d59-d341-4076-871d-2163c248c5a7': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 10}, page_content='70 isPortEighty 0.248489 domainlength 8.353102e-10\\n71 Directory LetterCount 0.265838 Entropy Domain 6.009946e-09\\n72 pathDomainRatio 0.305007 host letter count 1.007751e-08\\n73 dlddomain 0.339682 avgpathtokenlen 1.192005e-07\\n74 NumberRate URL 0.442371 isPortEighty 2.881362e-05\\n75sub-Directory\\nLongestWordLength0.587184sub-Directory\\nLongestWordLength3.440676e-04\\n76 Path LongestWordLength 0.868772 dlddomain 4.124519e-04\\n77 charcompvowels 0.911420 Path LongestWordLength 1.380409e-02\\n78 avgdomaintokenlen 0.993084 Entropy URL 1.272731e-01\\nTable 6: Feature Rankings across Binary and Multi-Classiﬁcation Experiments using Chi-Squared Test\\nTable 6 presents the results of applying the Chi-Squared test on the feature sets for the binary and\\nmulti-class datasets. It is interesting to note that out of the top ten highest rated features for both datasets,\\nonly two are present in both (20%). Conversely, 50% of the features in the lowest-rated—the bottom 10'),\n",
       " 'c6d2a3c3-fcba-4b5f-9ad7-2501d73c708c': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 10}, page_content='features—are shared. While the best features chosen for a model are highly dependent on the problem\\n(detection vs classiﬁcation), it appears that there is a consensus on which features to not use. The\\n“Entropy AfterPath” feature’s p-value is rounded off since the smallest ﬂoat available for the distribution\\nof Python used (3.8.3) is 1.0e-308. The ‘ISIpAddressInDomainName’ feature was removed from both\\ndatasets since there was no variation in the data.\\nThree of the top ten features for the binary classiﬁcation problem describe the entropy of various\\nsegments of a URL. The high value of these features is also reported in [17], which reported a 20%\\nincrease in malicious URL detection combined with a decrease in false negative rates after deploying the\\nmodel.\\nFigures 4 and 5 present the distribution graphs of the top four rated features from Table 6 for the\\n41'),\n",
       " '791b09e7-3a2c-4985-8663-1a86e630b332': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 11}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nmulti-class and binary-class datasets, respectively. One interesting note for the ‘argPathRatio’ graph in\\nFigure 4 is that the Defacement and Spam classiﬁcations tend towards larger values and larger variation,\\nwhile Phishing links tend towards much smaller values with the smallest variation out of all the classi-\\nﬁcations. This trend may be due to the fact that many malicious URLs may attempt to obfuscate their\\nintent through long arguments, while seemingly benign URLs may have shorter arguments with longer\\npaths. The other features shown present seemingly more complex relationships across the data. This is\\nexpected given the increased number of classiﬁcations.\\nFigure 4: Distribution Plots of Top Four Multi-class Features\\nThe binary-class features presented in Figure 5 show intriguing trends. Starting with ‘ﬁleName-'),\n",
       " 'd52f3be8-04bb-4a4e-8eee-24316666ef95': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 11}, page_content='Len’, one may expect to observe that malicious URLs have longer ﬁlenames for obfuscation reasons;\\nhowever, the distribution plot indicates otherwise. Further, there is wider variation in the benign URL\\nﬁlename lengths than malicious URLs. There are clear trends in ‘domain token count’, ‘tld’, and ‘Sym-\\nbolCount Domain’ features, all showing malicious URLs tend to have more items than benign URLs.\\nThis result is supported by [24] [17] [5], which report higher presence of special characters and usage of\\nmultiple top-level domains (tld) in malicious URLs.\\nFigures 6 and 7 present the accuracies of RF and DNN models with varying numbers of features,\\nusing the same stratiﬁed 10-fold cross validation methodology as before. The features included are in\\norder of rank as presented in Table 6; as such, the models are trained on the rank 1 feature, then the rank\\n1 and rank 2 features, etc until all 78 features are included (note that the ‘ISIpAddressInDomainName’'),\n",
       " '734f4ae8-a438-4cfa-a61e-7eed6d159a2d': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 11}, page_content='feature is omitted, for no variance occurs). It is expected that the models perform with higher accuracy\\nas we increase the number of features available; however, the goal is to determine the optimal number of\\nfeatures such that the accuracy of the model is not penalized substantially. Figure 6 presents data from\\none feature up to 20 features for both binary and multi-class problems to increase the data’s resolution,\\nwhile Figure 7 illustrates the model accuracies as we increase the feature count up to 78. We brieﬂy\\n42'),\n",
       " '94e255e4-bc92-4890-a69e-01d0c843353b': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 12}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nFigure 5: Distribution Plots of Top Four Binary-class Features\\nanalyze the results in each ﬁgure below.\\nThe multi-class problem analyzed in Figure 6 shows the expected upward trend for all models across\\nthe ﬁrst 20 features; however, most of this upward growth occurs in the ﬁrst ﬁve features. Random Forest\\ndemonstrates the signiﬁcance of these ﬁrst ﬁve features and slowly increases as we add more features.\\nFor the RF model, this is a clear indication of diminishing returns as the feature count increases. Both\\nKeras and Fast.ai severely underperform compared to RF for the ﬁrst 20 features. The Keras-TensorFlow\\nmodel has the largest growth in accuracy from 1-5 and 15-20 features. Fast.ai increases performance by\\n30 percentage points within the ﬁrst 10 features, however this accuracy stays constant with the addition'),\n",
       " 'd989e150-21d2-476c-8ae3-86fee964fca7': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 12}, page_content='of another 10 features. Finally, the variation of the accuracies across the 10 folds is shown, with the\\nKeras-TensorFlow model demonstrating extremely high variation compared to both Fast.ai and RF.\\nIn contrast to Figure 6, the accuracies of the models on the binary classiﬁcation problem indicate that\\nthe models perform similarly and require fewer features. While RF still demonstrates superior accuracy,\\nKeras is consistently within 5 percentage points. Surprisingly, these models were able to perform with\\n85-90% accuracy with only the ﬁrst feature. Fast.ai, conversely, shows very poor performance with the\\nﬁrst feature, but the performance slowly increases as the feature size increases. Additionally, all models\\nshow smaller variation across the folds compared to the data seen in the multiclass problem (Figure 6).\\nFigure 7 show the change in accuracies for each model from one feature up to all 78, with half the data'),\n",
       " 'b901ba1c-d850-4768-b393-28d74fcbabdb': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 12}, page_content='resolution of Figure 6. We see that the accuracies of all the models in Figure 7 increase with the addition\\nof more features, as expected. However, while Fast.ai shows substantial improvement in performance\\nfrom 20 to 50 features, neither Keras-TensorFlow nor RF dramatically increase in performance. The\\nissue of high variation across the folds continues for Keras-TensorFlow, while only being an issue for\\nFast.ai between 10 and 25 features.\\nWhile it is expected that the models will perform with higher accuracy when we increase the features\\navailable to the algorithms, Figure 7 presents slightly different trends. While RF, Keras-TensorFlow,\\n43'),\n",
       " '01b5ae1d-4987-4704-a896-41f000094042': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 13}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nFigure 6: Binary Classiﬁcation (left) and Multi Classiﬁcation (right) for Increasing Number of Features\\nup to n=20\\nFigure 7: Binary Classiﬁcation (left) and Multi Classiﬁcation (right) for Increasing Number of Features\\nup to n=78\\nand Fast.ai all show increases in performance from 1 to 40 features, the addition of more features ap-\\npears to have a detrimental effect on RF and Keras-TensorFlow models. While the Keras-TensorFlow\\nmodel shows high variance within the multiclass problem, the binary classiﬁcation problem sees this\\nfor the addition of speciﬁc features. In order of appearance from left to right, the features that appear\\nto have substantial, negative effects on the performance of the Keras-TensorFlow model are ’domain-\\nUrlRatio’, ‘ldl getArg’, ’File name DigitCount’, ’File name DigitCount’, ’LongestVariableValue’, and\\n’subDirLen’.\\n7 Discussion'),\n",
       " 'ba1c3f2f-fccc-4e78-8f65-a8a6a5a28166': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 13}, page_content='7 Discussion\\nThe current research sought to explore the effectiveness of the provided URL attributes and demonstrate\\nthe effectiveness lexical analysis may have in detecting and classifying malicious URLs, placing an\\nemphasis on practicality for an industrial setting. This experimental research mainly focused on the\\ndetection (Experiment 1) and classiﬁcation (Experiment 2) of various types of URLs based on lexical\\nanalysis through binary and multiclass classiﬁcation experiments, with an emphasis on the comparison\\nof popular deep learning models with traditional machine learning algorithms. Experiment 1 results\\ndemonstrated higher performance accuracy overall, with an increase of 8-10% on average across all\\nmodels. While Experiment 2 results showed lower performance with the average accuracy around >\\n44'),\n",
       " '0a6518e3-6a2c-4ce1-a13a-494f034506b4': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 14}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\n85%. Random Forest, Keras-TensorFlow, and Fast.ai consistently performed the best of all the models\\nexperimented, with >96% accuracy in both the experiments.\\nBy collecting the training and prediction times, in conjunction with the feature analysis, we conclude\\nthat, despite their popularity, deep neural networks are inferior to Random Forest due to their higher\\nvariance, count of features required to match RF performance, complexity, and overall amount of time\\nto train and predict when deployed. To minimize the effort required to potentially deploy a RF model,\\nthe feature set can be reduced down to 5-10 features with minimal cost to performance. While both\\nKeras-TensorFlow and Fast.ai are popular DNN frameworks, their deployment over RF requires more\\nresources which may be better spent elsewhere within an organization.'),\n",
       " '81be8e0b-1ed4-429b-9ff3-d936b008098e': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 14}, page_content='Overall, it is clear from the results that Random Forest is the most appropriate model for deployment\\nin an organization’s detection system. RF was a model that demonstrated lower complexity, as seen in\\nits lower training (Table 4) and prediction times (Table 5), and higher performance as seen in Tables 2\\nand 3. Further, RF typically performed up to or over 90% accuracy within the top ﬁve features for both\\nbinary and multi-class problems, whereas both DNN models required up to 30-40 features to match RF’s\\nperformance with only ﬁve in the multi-class problem (Figures 6 and 7).\\nThe results acquired from the deep neural network models indicate further work is required to clearly\\ndemonstrate one’s superiority over another. While the accuracy of the Fast-AI model exceeded that of\\nKeras-TensorFlow (Tables 2 and 3), Fast-AI experienced a substantial performance hit for both training\\nand sample prediction times (Tables 4 and 5, respectively). With the current work, a preference of one'),\n",
       " '689a523f-fdd0-4fa7-87fd-806ba6a9d16a': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 14}, page_content='DNN model over the other would indicate the priorities required from the model: accuracy at the cost\\nof time dictates Fast-AI superior while low latency at the cost of accuracy prefers the Keras-TensorFlow\\nmodel.\\nAdditionally, as the ﬁnal contribution of this work, the feature analysis of the lexical-based ISCX-\\nURL-2016 dataset shows the importance of speciﬁc characteristics of these malicious URLs. The main\\nconclusion from this section of the work clearly indicates a higher need for more features in the multi-\\nclassiﬁcation problem than the binary classiﬁcation problem. This is clearly seen from the p-values\\npresented in Table 6 and the slower increase in accuracies in Figures 6 and 7. While all three models\\nshowed large improvements within the ﬁrst 5-10 features, as ranked by their p-values, there was a drastic\\nimprovement in the accuracies when features ranked 15-25 were included.\\n8 Conclusion and Future Work'),\n",
       " '82bffe07-9735-4b0b-a5b3-401a3da7e515': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 14}, page_content='This work explored the applications of machine learning and deep learning models in detecting and clas-\\nsifying malicious URLs. By collecting performance accuracy, confusion matrices, and both training and\\npredicting times for the Random Forest, Keras-TensorFlow, and Fast-AI models, this study concludes\\nthat Random Forest is the best model to deploy by organizations seeking to build an URL ﬁlter applica-\\ntion, or those wishing to incorporate machine learning techniques to improve existing ones. Additionally,\\nthis study reports the speciﬁc lexical features contained within URLs may be used to minimize overhead\\ncost of a deployed model.\\nSome limitations attached to the present study could motivate further research; e.g., our team did not\\nexhaustively explore all the network conﬁgurations and hyperparameters available for DNNs which may\\npotentially improve their performances. While these improvements may lead to succeeding RF’s reported'),\n",
       " 'ba0e8b72-237e-4770-ae73-e97b1b41b661': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 14}, page_content='accuracy, there is an impact on training and testing times as well as additional drawback of overﬁtting\\nmodels thus reducing their real-word generalizability. Finally, we did not deploy and investigate the\\nefﬁcacy of the models with further experiments as explored in [17]; we leave this as our future research\\nwork. Importantly, we feel that more research on this front is needed to better bridge the gap between\\nacademic research and industrial applications with the goal of reducing detrimental economic impacts of\\n45'),\n",
       " '58141f0b-f673-4c82-85c3-a3bee4fb7888': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 15}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nmalicious URLs on organizations in various industries.\\nAcknowledgments\\nThis research project was supported by the state of Colorado through funds appropriated for cybersecurity\\nlaw dubbed “Cyber Coding Cryptology for State Records.” Any opinions, ﬁndings and conclusions, or\\nrecommendations expressed in this paper are those of the authors and do not necessarily reﬂect the views\\nof the funding sources. The authors would like to thank Robert Dunsky, Nathan Bellew and Karen Angels\\nfor helping with some of the early versions of the experiments.\\nReferences\\n[1] M. Abadi, A. Agarwal, et al. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems.\\nArXiv , abs/1603.04467, March 2016.\\n[2] R. Basnet. Deep learning malicious urls. \"https://github.com/rambasnet/\\nDeepLearningMaliciousURLs\" , [Online; accessed on September 10, 2020], 2019.'),\n",
       " 'ec80cf6a-fd48-44c9-b2ed-911a6ffaf16d': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 15}, page_content='[3] L. Breiman. Random forests. Machine learning , 45(1):5–32, October 2001.\\n[4] L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen. Classiﬁcation and regression trees . CRC press,\\n1984.\\n[5] Z. Chen and J. Szurdi. Cybersquatting: Attackers mimicking domains of major brands including face-\\nbook, apple, amazon and netﬂix to scam consumers. https://unit42.paloaltonetworks.com/\\ncybersquatting/ [Online; accessed on September 15, 2020], 2020.\\n[6] H. Choi, B. Zhu, and H. Lee. Detecting malicious web links and identifying their attack types. In Proc. of the\\n2nd USENIX Conference on Web Application Development (WebApps’11), Portland, Oregon, USA , page 11.\\nUSENIX, June 2011.\\n[7] T. Cover and P. Hart. Nearest neighbor pattern classiﬁcation. IEEE transactions on information theory ,\\n13(1):21–27, January 1967.\\n[8] B. Cui, S. He, X. Yao, and P. Shi. Malicious url detection with feature extraction based on machine learning.'),\n",
       " '87ec60a5-223d-4676-beca-0f3cd246fc58': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 15}, page_content='International Journal of High Performance Computing and Networking , 12(2):75–84, August 2018.\\n[9] Cybersecurity and I. S. Agency. Fbi releases ic3 2019 internet crime report. \"https://us-cert.cisa.\\ngov/ncas/current-activity/2020/02/12/fbi-releases-ic3-2019-internet-crime-report\"\\n[Online; accessed on May 08, 2020], 2019.\\n[10] B. Eshete, A. Villaﬁorita, and K. Weldemariam. Binspect: Holistic analysis and detection of malicious\\nweb pages. In Proc. of the 8th International ICST Conference (SecureComm’12), Padua, Italy , volume\\n106 of Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications\\nEngineering , pages 149–166. Springer, Berlin, Heidelberg, September 2012.\\n[11] fast.ai. About. \"https://www.fast.ai/about\" [Online; accessed on July 29, 2020].\\n[12] fast.ai. fastai v1 for pytorch: Fast and accurate neural nets using modern best practices. https://www.\\nfast.ai/2018/10/02/fastai-ai/ [Online; accessed on September 10, 2020].'),\n",
       " 'e1d0d012-0d3c-49d2-8b96-ff93d5757377': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 15}, page_content='[13] Y . Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to\\nboosting. Journal of computer and system sciences , 55(1):119–139, August 1997.\\n[14] J. Howard and S. Gugger. Fastai: A layered api for deep learning. Information , 11(2):108, February 2020.\\n[15] Infosecinstitute. Infosec iq, power to your people. \"https://www.infosecinstitute.com/iq/\" , [On-\\nline; accessed on August 18, 2020].\\n[16] S. Institute. Robust phishing awareness simulation training that changes behavior. \"https://www.sans.\\norg/security-awareness-training/products/phishing\" , [Online; accessed on August 18, 2020].\\n[17] A. Joshi, L. Lloyd, P. Westin, and S. Seethapathy. Using lexical features for malicious url detection–a\\nmachine learning approach. arXiv:1910.06277, October 2019. https://arxiv.org/abs/1910.06277\\n][Online; accessed on September 15, 2020].\\n46'),\n",
       " 'f8556e10-0e30-463b-a3b5-bb21f49b9f65': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 16}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\n[18] N. P. Jouppi, C. Young, et al. In-datacenter performance analysis of a tensor processing unit. In Proc. of the\\n44th Annual International Symposium on Computer Architecture (ISCA’17), Toronto, Canada , pages 1–12.\\nACM, June 2017.\\n[19] A. Karim, S. Azam, B. Shanmugam, K. Kannoorpatti, and M. Alazab. A comprehensive survey for intelligent\\nspam email detection. IEEE Access , 7:168261–168295, November 2019.\\n[20] Keras.io. Keras: The python deep learning api. https://keras.io [Online; accessed on August 09, 2020].\\n[21] S. KP, M. Alazab, et al. Malicious url detection using deep learning. https://www.techrxiv.org/\\narticles/preprint/Malicious_URL_Detection_using_Deep_Learning/11492622 [Online; ac-\\ncessed on September 15, 2020], January 2020.\\n[22] H. Le, Q. Pham, D. Sahoo, and S. Hoi. Urlnet: Learning a url representation with deep learning for malicious'),\n",
       " '002be835-7e59-4e47-89b3-6c1c5eb79712': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 16}, page_content='url detection. arXiv:1802.03162, March 2018. https://arxiv.org/abs/1802.03162 [Online; accessed\\non September 15, 2020]].\\n[23] J. Ma, L. Saul, S. Savage, and G. V oelker. Identifying suspicious urls: An application of large-scale online\\nlearning. In Proceedings of the 26th International Conference on Machine Learning (ICML’09), Montreal\\nQuebec, Canada , pages 681–688. ACM, June 2009.\\n[24] M. Mamun, M. Rathore, A. Lashkari, N. Stakhanova, and A. Ghorbani. Detecting malicious urls using lexical\\nanalysis. In Proc. of the 10th International Conference on Network and System Security (NSS’16), Taipei,\\nTaiwan , volume 9955 of Lecture Notes in Computer Science , pages 467–482. Springer, September 2016.\\n[25] Microsoft. Deﬁning malware: Faq. \"https://docs.microsoft.com/en-us/previous-versions/\\ntn-archive/dd632948(v=technet.10)\" [Online; accessed on August 13, 2020].\\n[26] T. Nash. An undirected attack against critical infrastructure: A case study for improving your control'),\n",
       " '1ee1ab35-5c13-47f1-8b16-e1335b6a87c8': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 16}, page_content='system security. \"https://us-cert.cisa.gov/sites/default/files/recommended_practices/\\nCaseStudy-002.pdf\" [Online; accessed on August 13, 2020], 2005.\\n[27] U. of Connecticut. Protect yourself from future phishing scams. \"https://phishingeducation.uconn.\\nedu/\" , [Online; accessed on August 18, 2020], 2020.\\n[28] F. B. of Investigation. Business email compromise the $26 billion scam. \"https://www.ic3.gov/media/\\n2019/190910.aspx\" , [Online; accessed on August 18, 2020], 2019.\\n[29] F. Pedregosa, G. Varoquaux, et al. Scikit-learn: Machine learning in python. Journal of Machine Learning\\nResearch , 12(85):2825–2830, 2011.\\n[30] PyTorch. Pytorch. https://pytorch.org [Online; accessed on August 09, 2020].\\n[31] PyTorch. Pytorch 1.3 adds mobile privacy quantization and named tensors. \"https://pytorch.\\norg/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors/\\n#speech-extensions-to-fairseq\" , [Online; accessed on October 09, 2020].'),\n",
       " '362eefa7-a73f-449a-b9c0-a68afdd47aea': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 16}, page_content='[32] M. Romagna and N. van den Hout. Hacktivism and website defacement: Motivation, capabilities and poten-\\ntial threats. In Proc. of the 27th Virus Bulletin International Conference, Madrid, Spain , October 2017.\\n[33] D. Sahoo, C. Liu, and S. Hoi. Malicious url detection using machine learning: A survey. arXiv:1701.07179,\\nAugust 2019. https://arxiv.org/abs/1701.07179 [Online; accessed on August 13, 2020].\\n[34] Scikit-Learn. 1.13. feature selection - univariate feature selection. \"https://scikit-learn.org/\\nstable/modules/feature_selection.html#univariate-feature-selection\" [Online; accessed\\non September 15, 2020], 2007.\\n[35] Scikit-Learn. sklearn.feature selection.chi2. \"https://scikit-learn.org/stable/modules/\\ngenerated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2\" [On-\\nline; accessed on September 15, 2020], 2007.\\n[36] TensorFlow. Tensorﬂow. https://www.tensorflow.org [Online; accessed on August 09, 2020].'),\n",
       " '4bacd3f4-36b9-42d3-91eb-7ba010ed49f5': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 16}, page_content='[37] E. Uc ¸ar, M. Incestas ¸, and M. Ucar. A deep learning approach for detection of malicious urls. In Proc. of the\\n6th International Management Information Systems Conference (IMISC’19), Istanbul, Turkey , pages 12–20,\\nOctober 2019.\\n[38] A. Van der Merwe, M. Loock, and M. Dabrowski. Characteristics and responsibilities involved in a phish-\\ning attack. In Proc. of the 4th International Symposium on Information and Communication Technologies\\n(WISICT’05), Cape Town, South Africa , pages 249–254. Trinity College Dublin, January 2005.\\n47'),\n",
       " '71eedc09-6168-404f-8b0d-3f4a73b6277d': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 17}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\n[39] Verizon. 2020 data breach investigations report. https://enterprise.verizon.com/resources/\\nreports/2020-data-breach-investigations-report.pdf [Online; accessed on May 08, 2020].\\n[40] R. Verma and A. Das. What’s in a url: Fast feature extraction and malicious url detection. In Proc. of the 3rd\\nACM on International Workshop on Security and Privacy Analytics (IWSPA’17), Scottsdale, Arizona, USA ,\\npages 55–63. ACM, March 2017.\\n[41] J. Zhao, N. Wang, Q. Ma, and Z. Cheng. Classifying malicious urls using gated recurrent neural networks.\\nInProc. of the 12th International Conference on Innovative Mobile and Internet Services in Ubiquitous\\nComputing (IMIS’18), Kunibiki Messe, Matsue, Japan , volume 773 of Advances in Intelligent Systems and\\nComputing , pages 385–394. Springer, Cham, June 2018.\\n——————————————————————————\\nAuthor Biography'),\n",
       " '67f6c8af-58df-4cf0-8cbe-d2d50056a2c9': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 17}, page_content='Author Biography\\nClayton Johnson is a senior undergraduate pursuing his Bachelor’s in Computer Sci-\\nence from Colorado Mesa University (CMU) and earned his Professional Certiﬁcate\\nin Cybersecurity from CMU in 2019. He is the former president of the CMU’s Com-\\nputer Science club and is a research fellow at the Cybersecurity Center at CMU. Clay-\\nton will begin pursing his Master’s in Technology, Cybersecurity, and Policy at the\\nUniversity of Colorado Boulder in 2021.\\nBishal Khadka is a senior undergraduate student pursuing his Bachelor’s in Com-\\nputer Science and Professional Certiﬁcate in Cybersecurity degrees at Colorado Mesa\\nUniversity (CMU). Bishal is currently the president of Cybersecurity club and a re-\\nsearch fellow at the Cybersecurity Center at CMU.\\nRam B. Basnet is an associate professor of Computer Science at Colorado Mesa Uni-\\nversity (CMU). He received his BS in Computer Science from CMU in 2004 and MS'),\n",
       " '8d14c522-6302-4684-9d8b-4b0cadf44fff': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 17}, page_content='and PhD in Computer Science from New Mexico Tech in 2008 and 2012, respectively.\\nHis research interests are in the areas of information assurance, machine learning, and\\ncomputer science pedagogy.\\nTenzin Doleck received his PhD from McGill University in 2017. He is currently a\\npost-doctoral fellow at the University of Southern California.\\n48'),\n",
       " 'cc20b003-27a9-4874-a5eb-2f159a906aba': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 0}, page_content='Towards Detecting and Classifying Malicious URLs\\nUsing Deep Learning\\nClayton Johnson1, Bishal Khadka1, Ram B. Basnet1*, and Tenzin Doleck2\\n1Colorado Mesa University, Grand Junction, CO 81501, USA\\n{cpjohnson, bkhadka }@mavs.coloradomesa.edu, rbasnet@coloradomesa.edu\\n2University of Southern California, Los Angeles, CA 90007, USA\\ndoleck@usc.edu\\nReceived: September 30, 2020; Accepted: December 11, 2020; Published: December 31, 2020\\nAbstract\\nEmails containing Uniform Resource Locators (URLs) pose substantial risks to organizations by po-\\ntentially compromising both credentials and network security through general and spear-phishing\\ncampaigns to their employees. The detection and classiﬁcation of malicious URLs is an important\\nresearch problem with practical applications. With an appropriate machine learning model, an orga-\\nnization may protect itself by ﬁltering incoming emails and the websites its employees are visiting'),\n",
       " '52362815-7eb2-4d58-97f7-03defaea70ff': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 0}, page_content='based on the maliciousness of URLs contained in emails and web pages. In this work, we com-\\npare the performance of traditional machine learning algorithms, such as Random Forest, CART,\\nand kNN against popular deep learning framework models, such as Fast.ai and Keras-TensorFlow\\nacross CPU, GPU, and TPU architectures. Using the publicly available ISCX-URL-2016 dataset,\\nwe present the models’ performances across binary and multiclass classiﬁcation experiments. By\\ncollecting accuracy and timing metrics, we ﬁnd that Random Forest, Keras-TensorFlow, and Fast.ai\\nmodels performed comparably and with the highest accuracies >96% in both the detection and\\nclassiﬁcation of malicious URLs, with Random Forest as the preferable model based on time, perfor-\\nmance, and complexity constraints. Additionally, by ranking and using feature selection techniques,\\nwe determine that the top 5-10 features provide the best performances compared to using all the fea-\\ntures provided in the dataset.'),\n",
       " 'e5945513-aec0-47cc-93f5-7accd3e63f85': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 0}, page_content='Keywords : Malicious URLs, Phishing URLs, Deep Learning, Web Security, Machine Learning\\n1 Introduction\\nPhishing—along with its more targeted version, spear phishing—is a social engineering attack in which\\nthe attacker attempts to compromise a user’s credentials or a system by presenting itself as a legitimate\\nbusiness communication [38]. These communications, most commonly emails, will often contain links\\nto websites controlled by the attacker that attempt to: 1) mimic a popular website to scrape the user’s cre-\\ndentials, 2) install malware onto the user’s system, or 3) spam the user. These links with malicious intents\\nare called malicious URLs. According to Verizon’s 2020 Data Breach Investigations Report [39], phish-\\ning attacks have been in the top three types of data breaches for the past 6 years and have been number\\none for the past two years, with around 96% of the social engineering attacks conducted through email.'),\n",
       " '0cc362da-cc27-4229-923e-1034c3469d69': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 0}, page_content='There is a clear increase in popularity with these types of attacks, highlighted by the Internet Crime Com-\\nplaint Center (IC3) Public Service Announcement on Business Email Compromises (BECs) [28]. The\\nJournal of Wireless Mobile Networks, Ubiquitous Computing, and Dependable Applications (JoWUA) , 11(4):31-48, Dec. 2020\\nDOI:10.22667/JOWUA.2020.12.31.031\\n*Corresponding author: Department of Computer Science and Engineering, Colorado Mesa University, Grand Junction, CO\\n81501, USA, Tel: +1-970-248-1400\\n31'),\n",
       " '3ec46e0c-a1e3-403f-9c32-0a2ea008190f': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 1}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nIC3’s 2019 Internet Crime Report [9] reported that losses from BECs and Email Account Compromises\\n(EACs) were in excess of $1.7 billion. The threat posed to an organization by these types of attacks is\\nclearly substantial, with trends indicating increase in popularity and severity over time. To address the\\nthreat phishing and malicious URLs pose to businesses, many popular websites and educational insti-\\ntutions have added “Phishing Awareness” programs, as exempliﬁed by the following examples: SANS\\nInstitute [16], InfoSec Institute [15], and University of Connecticut [27]. While providing training, edu-\\ncation, and awareness on phishing attacks have become trivial and common, such efforts are not adequate\\nto protect users. This warrants the need to apply machine learning algorithms to detect malicious URLs\\nbefore they arrive at intended targets within an organization, thus decreasing the efforts required from'),\n",
       " '1fc8039f-96d3-4620-86c0-b5c5b55149cc': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 1}, page_content='the user. The problem of detection and classiﬁcation of URLs is illustrated in Figure 1.\\nFigure 1: Detection and Classiﬁcation of URLs Problem Overview\\nThe contributions of this work are twofold: 1) we offer a direct comparison of popular deep learn-\\ning frameworks (Keras and Fast.ai) against traditional machine learning algorithms (Random Forest,\\nClassiﬁcation and Regression Tree, k-Nearest Neighbor, Support Vector Machine, Logistic Regression,\\nLinear Discriminant Analysis, AdaBoost, and Naive Bayes) in the detection and classiﬁcation of ma-\\nlicious URLs; and 2) we expand on the literature utilizing the public ISCX-URL-2016 dataset, which\\ncontains lexical features of malicious URLs. The remainder of this research is structured as follows. The\\nRelated Works section will explore recent advancements and contributions towards the detection and\\nclassiﬁcation of malicious URLs, as well as highlight any research gaps within the ﬁeld. Following this,'),\n",
       " '704605fd-154d-4291-8742-e841d6657b48': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 1}, page_content='the machine learning and deep learning frameworks utilized in the work are discussed in the Frameworks\\nsection. The ISCX-URL-2016 dataset will be discussed in depth before the presentation and explana-\\ntion of the experiments and results. Finally, the paper will conclude and suggest trajectories for further\\nresearch.\\n2 Related Works\\nWe summarize indicative studies examining malicious URLs. We follow this with a discussion of some\\nof the patterns seen across the literature and the research gaps.\\nMamun et al. [24] presented a new, public malicious URLs dataset called ISCX-URL-2016. Ad-\\nditionally, Mamun et al. experimented with the usage of RF, C4.5, and kNN algorithms to detect and\\nclassify malicious URLs on this new dataset by generating features directly from the URL, such as URL\\nlength, domain entropy, arguments, etc. In detecting malicious URLs, RF performed the highest with\\n32'),\n",
       " 'b93974ef-68f6-4782-9c2d-f26a0df0becf': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 2}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\n>0.99 precision and recall; however, both C4.5 and kNN models performed with >0.97 precision and\\nrecall. When classifying malicious trafﬁc, RF achieved the highest performance with recall and preci-\\nsion of 0.97. In this second classiﬁcation phase, all models perform with precision and recall in the range\\n0.92-0.97. Finally, the authors demonstrated that the obfuscation of malicious URLs does decrease the\\ndetection and classiﬁcation accuracies of the ML models used.\\nCui et al. [8] proposed a system to detect malicious URLs by using NB, DT, and SVM classiﬁers. The\\nreported accuracies >98.7% for all classiﬁers. Due to the high-performance of the models, the authors\\nclaim to have deployed the system, analyzing up to 2 TB worth of data per day. This work was extended\\nby Zhao et al. [41], who compared the performance of RF and a Gated Recurrent Neural Network (GRU)'),\n",
       " '3c6fd34b-1ab1-4430-91b0-b01fdda96841': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 2}, page_content='over the same dataset used in [8] in detecting malicious URLs. Zhao et al. found that the GRU model\\nwith 98.5% accuracy outperformed the RF model with 96.4% accuracy. This trend was seen across\\nvarying sizes of training datasets (from 600 to 240,000 samples). They also presented graphs showing\\nthe distributions of each classiﬁcation (Legitimate, SQL Injection, XSS Attack, Sensitive File Attack,\\nand Directory Traversal) based on the “Number of Characters” feature. Additional statistical analysis of\\neach feature may shed more light into understanding the malicious URLs detection problem—one of the\\ncontributions of this research study.\\nChoi et al. [6] experimented with the detection and classiﬁcation of malicious URLs using SVM\\nfor binary classiﬁcation and C4.5, Label Powerset, and kNN classiﬁers for multiclass classiﬁcation. The\\nfeatures used for this system contain lexical characteristics, link structures, DNS, network trafﬁc, and'),\n",
       " '3775f0b7-9c8b-489d-9f11-0af441ac387a': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 2}, page_content='content composition information. The detection of malicious trafﬁc demonstrated high performance\\nwith SVM’s accuracy exceeding 98%. The work is claimed to be the ﬁrst report of malicious URLs\\nclassiﬁcation across phishing, spamming, and malware categories. Their models for multi-class classiﬁ-\\ncation performed with accuracies >93%. Additionally, the article states that due to the details the model\\ntook into account, obfuscation techniques such as redirection, link manipulation, and fast-ﬂux hosting\\nwould hinder the effectiveness in detecting malicious URLs.\\nMa et al. [23] presented a novel technique for detecting malicious URLs by utilizing continuous,\\nonline machine learning techniques. The work experimented with Logistic Regression, SVM, NB, Con-\\nﬁdence Weighted (CW), and Perceptron models. By combining features such as DNS information, lex-\\nical characteristics, web registration dates, etc. with blacklist information, the team was able to compile'),\n",
       " '2fd06abb-484d-4c4c-b9b9-46fcc8523a78': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 2}, page_content='a total of 2.9 million features over the course of 100 days. The models were trained using a sliding\\nwindow of two weeks’ worth of training data. While all models showed decreasing error rates over the\\n100 day period, CW consistently maintained the lowest error rates with a minimum of 1%. Due to the\\ndynamic landscape of the malicious URLs problem, Ma et al. addressed the need for large, fresh datasets\\nto emphasize and encourage the continuous training of models.\\nUc ¸ar et al. [37] applied two deep learning models, Long Short-Term Memory (LSTM) and Con-\\nvolutional Neural Network (CNN), in the detection and classiﬁcation of malicious URLs. Detection of\\nmalicious URLs demonstrated high performance, with accuracies of 97.25% and 98.86% for the LSTM\\nand CNN models, respectively. The CNN model showed an accuracy of 95.37% on the classiﬁcation of\\nmalicious URLs, higher than 91.13% for LSTM. The work is signiﬁcant for two reasons. First, there'),\n",
       " 'a4bb73ed-d8ed-4d3e-932f-48a5b3b8fbd3': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 2}, page_content='are few applications of deep learning in this research problem and secondly, few works have used the\\nISCX-URL-2016 dataset.\\nVinayakumar et al. [21] evaluated the performance of multiple deep learning architectures (LSTM,\\nCNN, Bidirectional Recurrent Structures, LSTM-CNN hybrid, and Stacked CNNs) against their pro-\\nposed deep learning architecture in the detection of malicious URLs. The architectures explored were\\nimplemented through Keras, described in the Frameworks section. The two-part dataset includes data\\nfrom Alexa.com, DMOZ Directory, Sophos, and other related sources. The models perform in the range\\nof 95 - 99% accuracy over the ﬁrst dataset, with LSTM as the highest performing model. The authors\\nplace an emphasis on the difference between the random-split and time-split versions of the dataset,\\n33'),\n",
       " '61a9d902-1c7d-4e65-b11f-342ba2afc1f9': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 3}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nshowing that there is a higher variance in the accuracy results from the time-split than the random-split\\nversions of the second dataset. The models’ accuracy ranges from 95-96.6% in the random-split section\\nof the second dataset, while the time-split version has models ranging from 93-97.1% accuracy.\\nSahoo et al. [33] conducted a survey of the work done within the realm of detecting and classifying\\nmalicious URLs. The work discussed in depth the multitude of issues with existing, commercial blacklist\\nand heuristic solutions, which lead to ungeneralized, nonﬂexible models that are susceptible to novel or\\nobfuscated attacks. For simplicity and security, most of the recent efforts have been focused on static\\nanalysis of URLs using various machine learning techniques. Almost half of the works analyzed utilized'),\n",
       " 'f7267020-e18a-4f74-9638-664cbee2f1a3': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 3}, page_content='either lexical or host-based features [33]. There is a clear preference for these types of features within\\nthe ﬁeld, while other works are spread out among HTML, JavaScript, and Context-based features. While\\nthere have been some efforts to apply deep learning architectures, such as LSTMs and CNNs to the\\nmalicious URLs problem, computational complexity is the largest constraint [33]. However, there are\\nfew works exploring and comparing the results of deep neural network frameworks against traditional\\nmachine learning algorithms such as RF, SVM, kNN, etc, in the context of the malicious URLs problem.\\nThis is the primary contribution of our work.\\nHung et al. [22] performed a series of deep learning experiments and neural networks like Convolu-\\ntional Neural Networks (CNN) so that the model would learn URL embedding. They use the large dataset\\nfrom VirusTotal anti-virus group. The team attempted to ﬁnd the limitations in identifying the malicious'),\n",
       " '966a75bc-ebc6-4777-9701-94716cd50e0a': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 3}, page_content='URLs using lexical properties like inability to identify the proper semantic meaning and patterns in URL\\nand they use end-to-end deep learning framework to address this kind of problem. The team addressed\\nthe problem of identifying whether a URL is malicious or not by formulating a problem as a binary\\nclassiﬁcation task [22]. They use the technique called character-level CNN which learns the frequency\\nand sequence of the characters in the URL. Moreover, word-level CNN is also applied for identifying the\\nunique words in the URL. The main goal of the paper was to use character-level and word-level CNN to\\naddress the limitations produced by previous methods and precisely identify the malicious URLs.\\nBirhanu et al. [10] employed a technique called BINSPECT, which is a combination of static analysis\\nand minimalistic emulation and uses supervised learning techniques, for detecting malicious web pages.'),\n",
       " '5da35785-eb46-45d9-9d25-0f145244f235': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 3}, page_content='The team was able to generate accuracy of more than 97% when using this technique with low false\\nsignals [10]. Furthermore, the team also mentioned that, with this approach, it took them only 3-5\\nseconds to analyze a single web page.\\n3 Motivation\\nWhen addressing the problem, most of the works available attempt to only detect malicious URLs, while\\nfew works experiment with classifying malicious URLs. While detection is a substantial issue, the\\nclassiﬁcation of these malicious URLs indicates where the direction and priority of actions within an\\norganization should be to best protect its network. For instance, defaced URLs may require more urgent\\nreaction due to potential credential compromises than spam URLs, which don’t need immediate action.\\nThere is a clear consensus that blacklisting malicious URLs is a poor solution since it fails to address\\nissues such as obfuscation or novel attacks. As a consequence of this, most of the recent work has'),\n",
       " 'f0fdca43-99d5-4507-bce1-f2900e31c40b': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 3}, page_content='focused on applications of machine learning algorithms. Of the popular algorithms, Random Forest (RF)\\nappears to be one of the most effective. The survey of relevant work revealed few studies that address\\napplications of deep learning within the ﬁeld and even fewer that directly compare the performance of\\ndeep learning models to traditional machine learning models. This is a key gap in the literature that the\\npresent study aims to address. Additionally, this work compares various metrics, such as training and\\nprediction times, across multiple architectures (CPU, GPU, and TPU) to determine which model would\\nbe most practical to use in an industry setting.\\n34'),\n",
       " '411fb1c0-75d9-44a6-b475-eba3279a5a75': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 4}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nThe features used in the ﬁeld are varied and derived from many different sources. Despite this,\\nfeatures may be derived from both static analysis of the URL and dynamic analysis from visiting the\\nwebsite. Most of the research works analyze static features for security and cost reasons. It is safer and\\ncomputationally cheaper to statically analyze a suspicious URL than visit the website and execute the\\nattacker’s code, which may take an arbitrary amount of time to complete. The majority of the related\\nworks presented create custom datasets with different types of features (statistical and lexical, host-\\nbased, WHOIS, etc.), demonstrating a lack of standardization within the datasets used for the research\\nproblem. One of the reasons for our usage of the ISCX-URL-2016 dataset is to show support for usage\\nof a standardized, lexicon-based dataset for static analysis of suspicious URLs. This work explores the'),\n",
       " '301fd54b-6fe1-4d42-b039-6e81dd74ef8c': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 4}, page_content='effectiveness of the provided URL attributes and demonstrates the effectiveness lexical analysis may\\nhave in detecting malicious URLs, as well as the limitations.\\n4 Machine and Deep Learning Frameworks\\n4.1 fast.ai\\nfast.ai is one of the famous python libraries which includes various machine learning and deep learning\\nlibraries. fast.ai is a Python module that implements a high-level API to a PyTorch backend. The goal of\\nfast.ai is to easily allow experts in other ﬁelds, such as virologists or astronomers, to implement popular\\ndeep learning techniques within their respective settings. This framework has seen multiple popular\\nsuccesses in research and industry [11].\\n4.2 Keras\\nKeras is a deep learning framework for Python that allows for the implementation of both TensorFlow\\nand Theano in the backend [20]. Keras runs Tensorﬂow 2.0 in the backend for easy machine learning\\nworkﬂow and for proper data management. Keras allows for easy and fast prototyping through user'),\n",
       " '23b850bb-7ce8-4f7f-bc2d-65cd7def2503': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 4}, page_content='friendliness, modularity, and extensibility. It usually runs seamlessly on CPU and GPU.\\n4.3 TensorFlow\\nTensorﬂow was originally developed by Google and it is free to use. Tensorﬂow uses static computa-\\ntional graphs also known as the deﬁne-and-run approach. The libraries and large community resources\\navailable in the Tensorﬂow allows its user to build powerful and advanced machine learning models and\\napplications. It builds and trains ML models easily using intuitive high-level APIs like Keras with ea-\\nger execution, which makes for immediate model iteration and easy debugging [36]. One of the key\\naffordances of Tensorﬂow is that it not only uses CPU but also GPU which helps in gaining much more\\ncomputing power. Tensorﬂow 2.0 further uses TPU, also known as Tensor Processing Unit, which adds\\ncomputational power and improves performance. Using this deep learning module, even fairly compli-\\ncated models can be created with very little coding effort.\\n4.4 PyTorch'),\n",
       " 'd4bd1e7b-1bfc-40c9-a08d-d724fdd1f20d': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 4}, page_content='4.4 PyTorch\\nPyTorch is an open source machine learning framework that accelerates the path from research prototyp-\\ning to production deployment [30]. This machine learning framework was ﬁrst developed by Facebook.\\nPyTorch uses dynamic computational graphs which lets the user process variable length input and out-\\nput. PyTorch has many libraries like captum, pytorch geometric, and skorch which are all open source\\nand help in the tasks like model interpretability, performing deep learning on irregular input data, and\\nproviding scikit-learn compatibility.\\n35'),\n",
       " '822f67a0-0bc5-47c3-b1c7-da1651b3b6e0': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 5}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\n4.5 Scikit-Learn\\nScikit-learn (sklearn)—a python library built upon Scipy, Numpy, and matplotlib—is a handy tool for\\npredictive data analysis [29]. This cutting edge software is free to use for the public. Common machine\\nlearning tasks, such as classiﬁcation, regression, and clustering, can be easily tackled using scikit-learn.\\nFor some machine learning classiﬁers like Support Vector Classiﬁcation and Lasso, scikit-learn outper-\\nforms other python machine learning libraries [29].\\nThe models used from the Scikit-learn module are the RandomForestClassiﬁer, Decision Tree (op-\\ntimized CART algorithm), KNeighborsClassiﬁer, SVC (Support Vector Machine), LogisticRegression,\\nLinearDiscriminantAnalysis, AdaBoostClassiﬁer, and GaussianNB (Naive Bayes).\\n5 Dataset\\nThis experiment presents ISCX-URL-2016 URL [24] dataset. Around 78 lexical features were extracted'),\n",
       " 'fbc52009-a072-458b-a1f4-696be1ab4740': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 5}, page_content='from URLs, and 5 URL classes such as benign, defacement, malware, phishing, and spam were labeled\\nin the dataset. The different classes of URLs are brieﬂy introduced below.\\nBenign URLs: Benign URLs are legitimate URLs that do not lead to any infectious websites and do\\nnot try to inject the user’s computer with any kind of harmful malware. Benign websites may contain\\nadvertisements and adware which are typically harmless to a computer.\\nDefacement URLs: Website defacement usually means changing a certain aspect of the website\\nsuch as it’s visual appearances and some contents on it. Hacktivists try to deface a website for numerous\\nreasons [32]. This kind of act is done when some content in the web page needs to be changed without\\nthe permission of the original owner of the website which technically means penetrating a website.\\nMalware URLs: Malware URLs take a user to the malicious website that typically installs some'),\n",
       " '3c0dcaee-944f-4fe2-aa8f-5e03946e89c6': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 5}, page_content='malware on that user’s device which can be used for identity theft, corrupting ﬁles, and even logging\\nkeystrokes. Malware can indirectly be a dangerous software that can harm a computer and steal some-\\none’s private information [25]. Some threats such as harmful biological agents, a terrorist cell intent on\\ndisrupting operations, etc. can be considered as malware [26]. Some of the examples of malware are\\nransomware, spyware, scareware, among others.\\nPhishing URLs: Phishing URLs conventionally entice a user to visit a fake website and will try to\\nsteal as much information they can get from the user. Sometimes a user can easily be led to phishing\\nwebsites just by having a typo in a URL. Phishing can be deﬁned as the intent of the hacker to steal some\\nprivate information like credit card number and other digital identity by employing social engineering\\ntechniques [40].\\nSpam URLs: Spam is a way of sending unsolicited emails to the user with the intent of advertise-'),\n",
       " '73586019-7537-4146-9127-1500ddf4b8b7': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 5}, page_content='ments or for serious harm to the computer [19]. Spam URLs are usually seen in the spam email. Some\\nspam URLs can be harmful and can infect the computer of the user with spyware and adware.\\n5.1 Lexical Analysis\\nMalicious URLs may contain some pattern in their URL text which gives us a hint that the URL is not\\nlegitimate. Various lexical features such as query length, domain token count, path token count, URL\\nlength, domain length, path length, and many more were used in the experiment for better performance\\nand results. Let’s consider the following URL to extract lexical features, e.g.:\\nhttp://www.example.site.com/path dir1/path dir2/ﬁle.php\\nThe URL is 56 characters long, thus the ‘URLLen’ feature is 56. The length of the domain name\\n“domainlength” example.site.com is 16, “domainUrlRatio” is 0.2857, and “NumberofDotsinURL” is 4.\\nThis process of extracting all 78 lexical features is thoroughly discussed in [24].\\n36'),\n",
       " 'd2370d13-74ac-46ef-9f7c-16b65e268692': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 6}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nAs discussed previously in the Related Works section, traditional blacklisting techniques have clear\\ndisadvantages in the detection and classiﬁcation problems. Thus, taking a closer look at lexical analysis\\nwould be a better choice for identifying those URLs.\\n5.2 Data Preprocessing\\nOriginal dataset had a total sample of 36,707. From this, many entries were NaN, inﬁnity, or empty.\\nAll the entries and attributes with NaN, Inﬁnity, and missing values were eliminated from the dataset\\nwith the aim of getting more precise results. During the data cleansing process, around 17K rows with\\nNaN and redundant values were dropped from the dataset. In addition, 7 attributes of columns with NaN\\nvalues were eliminated leaving only 72 attributes in the dataset. Table 1 illustrates the number of samples\\nbefore and after data cleanup.\\nURL Type Raw Samples Filtered Samples\\nBenign 7,781 2,709\\nDefacement 7,930 2,477'),\n",
       " '6acbab85-98d4-4bf4-a1ab-f494b33568d7': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 6}, page_content='Malware 6,712 4,440\\nPhishing 7,586 4,014\\nSpam 6,698 5,342\\nTable 1: Number of samples before and after data cleanup\\n6 Experiment and Results\\nThe experiments demonstrate a two-layered approach to the malicious URLs research problem: (1)\\nExperiment 1 involves the detection of malicious URLs and (2) Experiment 2 involves the classiﬁcation\\nof malicious URLs. Experiment 1 is a binary classiﬁcation problem with two classes: Benign and\\nMalicious. All non-benign categories of URLs are labelled as malicious. Experiment 2 is a multi-class\\nclassiﬁcation problem where malicious URLs are categorized into speciﬁc classes such as: Defacement,\\nMalware, Phishing, and Spam.\\nBoth the experiments utilize the same set of machine learning models: Random Forest [3] (RF),\\nDecision Tree/CART [4] (DT), k-Nearest Neighbors [7] (kNN), Support Vector Machine (SVM), Lo-\\ngistic Regression (LR), Linear Discriminant Analysis (LDA), AdaBoost [13] (AB), Naive Bayes (NB).'),\n",
       " '09e2af55-0c48-49a0-93f2-51fd7d6dd44b': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 6}, page_content='In addition, two deep learning models, Fast.ai [14] and Keras-TensorFlow [20] [36] [1], are also used.\\nThe models are trained and evaluated on the ISCX-URL-2016 dataset using stratiﬁed 10-fold cross val-\\nidation. All experiments were conducted on the free tier of Google Colaboratory system using Jupyter\\nNotebooks. The Jupyter Notebooks and scripts used for the experiments can be found on GitHub.com\\n[2].\\n6.1 Performance Metrics\\nVarious metrics are collected to compare and determine the most appropriate model meeting real-world\\ndeployment constraints. Initially, all models are trained and evaluated on both Experiments 1 and 2, gen-\\nerating accuracy using 10-fold cross validation. Confusion matrices are presented to clearly demonstrate\\nthe performance of the DNN models. An additional experiment is performed on the highest-performing\\ntraditional machine learning and the DNN algorithms in which the benign samples are removed from the'),\n",
       " '5f1c8847-c635-420d-9f07-0df81912b29f': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 6}, page_content='multi-class classiﬁcation problem. The goal of the experiment is to see how the best classiﬁers would\\n37'),\n",
       " '5422b306-cae5-4ad5-8f7c-c4e0c5efb57f': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 7}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nperform on multi-class classiﬁcation of just the malicious URLs. Then, time metrics are calculated on\\nthese three models. These time metrics describe two different characteristics of the models. These time\\nvalues are collected across CPU, GPU, and TPU architectures to further assist an organization’s invest-\\nment decisions in projects in which architecture is a consideration.\\nThe ﬁrst time metrics used to evaluate the models is the total time required to train each model. This\\ntraining time metric is calculated by summing the total training time elapsed for each fold. Training time\\nexcludes object instantiation and testing since the focus is on training speciﬁcally. Even though training\\nis often described as a single event, training time is included as a metric because retraining with new data\\nmay occur multiple times for a model implemented by a system. Thus, the total amount of time required'),\n",
       " '5a2421d9-8921-4252-a9d8-7b4754e0fba7': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 7}, page_content='to train a model has a direct impact on the organization.\\nThe second time metric is the average time required for a model to predict the classiﬁcation of a pro-\\nvided sample. If a model was deployed and used to predict the malevolence of a URL, the time required\\nto generate this prediction has a clear impact on the ability of the organization to ﬁlter URLs. This effect\\nis exponential for larger organizations with growing attack surfaces. For simplicity, we assume that each\\ndeployed model would predict on a single sample at a time, as opposed to batches of samples. Addition-\\nally, since this metric is calculated by timing the ‘predict’ method of each object on samples from the\\nISCX-URL-2016 dataset, we do not report the time required to generate the features used in the dataset.\\nHowever, time required to generate features may be minimized by balancing the trade-off between the\\nnumber of features used and reported accuracy, as discussed in the feature analysis section.\\n6.2 Results'),\n",
       " 'a01e2a8e-92bd-45ca-9eb3-bcac4dc1a24c': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 7}, page_content='6.2 Results\\nAs seen in Table 2, Random Forest demonstrates superior accuracy among all machine learning algo-\\nrithms used for both the experiments. This result is well documented in the literature [21, 40] and further\\nsupported by our experiments. Both DNN models show high accuracy in Experiment 1, performing sim-\\nilar to DT, kNN, and AB, while still being eclipsed by RF. Experiment 2 sees performance decreases for\\nall models except Fast.ai, with both Fast.ai and RF performing comparably. Keras-TensorFlow’s accu-\\nracy decreased substantially to 91.5%, around ﬁve percentage points lower than that from RF and Fast.ai.\\nAn additional observation is the higher standard deviation for Keras-TensorFlow.\\nClassiﬁer Binary-Class Accuracy (%) Multi-Class Accuracy (%)\\nRF 98.68±0.22 96.26±0.53\\nDT 97.63±0.24 92.81±0.62\\nkNN 97.47±0.39 92.52±0.79\\nSVM 93.96±0.60 80.77±0.63\\nLR 90.50±0.30 69.54±0.97\\nLDA 94.34±0.33 82.31±0.78\\nAB 96.21±0.39 76.58±1.60\\nNB 68.73±0.83 59.55±0.91\\nfast.ai 96.88±0.33 96.77±0.51'),\n",
       " '361b6a9f-ba8e-48a6-b6ec-a492a7f58b9e': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 7}, page_content='Keras-TensorFlow 97.13±1.93 91.45±2.94\\nTable 2: Accuracy Results for Binary and Multi-Class Classiﬁcation\\nRemoving the benign samples contained in the multi-class dataset, as seen in Table 3, appears to\\nhave little effect on the performance of the classiﬁers. While all three classiﬁers appear to perform better\\nwithout the benign samples, the performance is within one standard deviation of the previous results.\\nThus, this improvement appears to be insigniﬁcant. Nevertheless, RF, Fast-AI, and Keras-TensorFlow\\nclassiﬁers clearly demonstrate high accuracy metrics when classifying malicious URLs.\\n38'),\n",
       " '214ab573-0296-4013-8456-ff5fea320897': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 8}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nClassiﬁer Accuracy (%)\\nRF 96.99±0.50\\nfast.ai 97.55±0.37\\nKeras-TensorFlow 93.81±2.34\\nTable 3: Top Classiﬁer’s Accuracies Results for Multi-class Malicious URL Classiﬁcation (without Be-\\nnign Samples)\\nFigure 2: Confusion Matrices for Keras-TensorFlow and Fast.ai DNN Models for Experiment 1\\nThe confusion matrices for the deep learning models are presented in Figure 2 and Figure 3. While\\nan overwhelming majority of the samples are classiﬁed correctly—seen through the main diagonal of\\nthe matrix— the Phishing URL classiﬁcation has the clearest rate of failure. Both deep learning models\\ndemonstrate a pattern of both over- and under-predicting Phishing URLs.\\nClassiﬁer Experiment CPU GPU TPU\\nRF Binary 75.83 61.58 95.19\\nMulti 87.26 71.02 106.83\\nfast.ai Binary 323.36 221.52 360.47\\nMulti 320.91 220.00 362.29\\nKeras-TensorFlow Binary 445.78 135.63 123.17\\nMulti 451.37 135.65 124.09'),\n",
       " '8de2ba55-c157-4f39-95a6-c40a6232a634': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 8}, page_content='Table 4: Training Time across Runtime Environments in Seconds\\nTable 4 presents the total time taken for Random Forest and the DNN models to train across the 10\\nfolds on a CPU, GPU, and TPU run-time environments. Keras-TensorFlow demonstrates its lowest train\\ntimes on the TPU, as expected since Google develops both TensorFlow and the TPU architecture [18].\\nHowever, the best metric for Keras-TensorFlow is still twice the fastest time recorded for RF. Both RF and\\nFast.ai have their best training times on the GPU, which is likely because there is currently no support\\nor documentation for the TPU architecture through Scikit-Learn or Fast.ai’s PyTorch version (Fast.ai\\n39'),\n",
       " '35824ae8-c23a-45ee-bb99-eba914721912': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 9}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nFigure 3: Confusion Matrix for Keras-Tensorﬂow (left) and Fast.ai (right) for Multi-Class Classiﬁcation\\nExperiments\\nClassiﬁer Experiment CPU GPU TPU\\nRF Binary 8.12±0.77 6.61±0.63 7.99±0.74\\nMulti 8.30±1.09 6.66±0.73 8.02±0.66\\nfast.ai Binary 53.17±9.95 44.29±6.65 54.59±8.888\\nMulti 54.62±17.31 44.34±4.98 54.53±11.05\\nKeras-TensorFlow Binary 38.53±4.71 28.81±5.51 41.52±4.30\\nMulti 40.20±5.56 28.54±4.48 41.49±4.32\\nTable 5: Average Time to Predict Classiﬁcation of a Sample in Milliseconds\\ncurrently implements PyTorch v1, however Fast.ai v2 claims it will support TPU through PyTorch v1.3+)\\n[31] [12] [14]. While Keras-TensorFlow trains faster than Fast.ai, there appears to be a large performance\\nhit, especially for Experiment 2. There is little change in performance times based on the experiment\\nconducted.\\nThe times presented in Table 5 show the average time for the RF and DNN models to predict a'),\n",
       " 'b71c57ef-fdd3-436b-8727-4d064416675a': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 9}, page_content='classiﬁcation for a given sample. As expected, there is a uniform drop in average time from CPU to\\nGPU. The TPU architecture, on the other hand, appears to have mixed results on the prediction times,\\nleading to all three models showing the best prediction times on the GPU. Finally, Random Forest appears\\nto be the most consistent with standard deviations less than 1.09 ms. This result is in extreme contrast\\nto the DNN models, which experience standard deviations ranging from 4.30 ms to 17.31 ms. These\\nmetrics appear to have mixed results depending on the experiment conducted.\\n6.3 Feature Analysis\\nThe features in the ISCX-URL-2016 dataset are analyzed to shine further light onto the subject of de-\\nveloping efﬁcient mechanisms to detect and classify malicious URLs. There are a total of 78 features\\navailable from the dataset. These features naturally vary in their predictive capabilities and thus their\\n40'),\n",
       " '58eacf02-ca21-4f1b-81f3-d2981c3bd27b': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 10}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nusefulness for a machine learning algorithm. By applying the chi-squared test [29] [35] [34], we rank\\nthe features with highest correlation to the classiﬁcation of a malicious URL sample. The chi-squared\\nfeature ranking technique is chosen for its simplicity and availability through sklearn [35]. Additionally,\\nthe p-value calculated by the test for each feature clearly indicates its importance in classiﬁcation prob-\\nlems, where high p-values indicate independence from the target classiﬁcation and are thus poor-quality\\nfeatures and vice-versa. The effect of this process is two-fold. Using this feature selection technique,\\nwe decrease noise within a dataset and decrease the resources required to generate a prediction on a\\nsample in a real-world setting. If a model handles noisy data poorly and thus performs poorly, this will'),\n",
       " 'b2cf8709-a3e9-460c-9410-3197e71bf2c7': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 10}, page_content='help remedy its sensitivity to the data. Additionally, when trying to determine if a URL in an email or a\\nwebsite is malicious, less time and computation is spent generating characteristics that are unused or not\\nas useful.\\nRank Binary-Class Feature Binary-Class p-value Multi-Class Feature Multi-Class p-value\\n1 ﬁleNameLen 1.406341e-70 Entropy Afterpath 0\\n2 domain token count 1.636373e-41 argPathRatio 1.397820e-290\\n3 tld 1.636373e-41 NumberRate AfterPath 1.983143e-290\\n4 SymbolCount Domain 1.678641e-41 NumberRate Domain 5.645027e-279\\n5 Entropy Afterpath 3.584071e-41 ArgUrlRatio 3.924478e-272\\n6 delimeter path 9.681864e-40 Extension DigitCount 6.874504e-171\\n7 argPathRatio 9.901046e-38 dldgetArg 1.534782e-147\\n8 Entropy Filename 2.679810e-37 ldlgetArg 1.714726e-147\\n9 Entropy DirectoryName 7.487584e-36 Query DigitCount 1.118545e-135\\n10 Filename LetterCount 3.891527e-33 LongestVariableValue 2.788998e-135\\n...\\n69 Entropy URL 0.161457 ldldomain 7.808014e-11'),\n",
       " '778de5df-71a2-4785-be86-c907992d9118': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 10}, page_content='70 isPortEighty 0.248489 domainlength 8.353102e-10\\n71 Directory LetterCount 0.265838 Entropy Domain 6.009946e-09\\n72 pathDomainRatio 0.305007 host letter count 1.007751e-08\\n73 dlddomain 0.339682 avgpathtokenlen 1.192005e-07\\n74 NumberRate URL 0.442371 isPortEighty 2.881362e-05\\n75sub-Directory\\nLongestWordLength0.587184sub-Directory\\nLongestWordLength3.440676e-04\\n76 Path LongestWordLength 0.868772 dlddomain 4.124519e-04\\n77 charcompvowels 0.911420 Path LongestWordLength 1.380409e-02\\n78 avgdomaintokenlen 0.993084 Entropy URL 1.272731e-01\\nTable 6: Feature Rankings across Binary and Multi-Classiﬁcation Experiments using Chi-Squared Test\\nTable 6 presents the results of applying the Chi-Squared test on the feature sets for the binary and\\nmulti-class datasets. It is interesting to note that out of the top ten highest rated features for both datasets,\\nonly two are present in both (20%). Conversely, 50% of the features in the lowest-rated—the bottom 10'),\n",
       " '6d6ae3c3-7866-41b0-bfd1-4c083c168cb9': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 10}, page_content='features—are shared. While the best features chosen for a model are highly dependent on the problem\\n(detection vs classiﬁcation), it appears that there is a consensus on which features to not use. The\\n“Entropy AfterPath” feature’s p-value is rounded off since the smallest ﬂoat available for the distribution\\nof Python used (3.8.3) is 1.0e-308. The ‘ISIpAddressInDomainName’ feature was removed from both\\ndatasets since there was no variation in the data.\\nThree of the top ten features for the binary classiﬁcation problem describe the entropy of various\\nsegments of a URL. The high value of these features is also reported in [17], which reported a 20%\\nincrease in malicious URL detection combined with a decrease in false negative rates after deploying the\\nmodel.\\nFigures 4 and 5 present the distribution graphs of the top four rated features from Table 6 for the\\n41'),\n",
       " 'c46c99b7-30fd-405d-8fc8-2bca969de837': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 11}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nmulti-class and binary-class datasets, respectively. One interesting note for the ‘argPathRatio’ graph in\\nFigure 4 is that the Defacement and Spam classiﬁcations tend towards larger values and larger variation,\\nwhile Phishing links tend towards much smaller values with the smallest variation out of all the classi-\\nﬁcations. This trend may be due to the fact that many malicious URLs may attempt to obfuscate their\\nintent through long arguments, while seemingly benign URLs may have shorter arguments with longer\\npaths. The other features shown present seemingly more complex relationships across the data. This is\\nexpected given the increased number of classiﬁcations.\\nFigure 4: Distribution Plots of Top Four Multi-class Features\\nThe binary-class features presented in Figure 5 show intriguing trends. Starting with ‘ﬁleName-'),\n",
       " 'e5516bc7-b3b7-4bc1-b055-c311e8bfaca3': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 11}, page_content='Len’, one may expect to observe that malicious URLs have longer ﬁlenames for obfuscation reasons;\\nhowever, the distribution plot indicates otherwise. Further, there is wider variation in the benign URL\\nﬁlename lengths than malicious URLs. There are clear trends in ‘domain token count’, ‘tld’, and ‘Sym-\\nbolCount Domain’ features, all showing malicious URLs tend to have more items than benign URLs.\\nThis result is supported by [24] [17] [5], which report higher presence of special characters and usage of\\nmultiple top-level domains (tld) in malicious URLs.\\nFigures 6 and 7 present the accuracies of RF and DNN models with varying numbers of features,\\nusing the same stratiﬁed 10-fold cross validation methodology as before. The features included are in\\norder of rank as presented in Table 6; as such, the models are trained on the rank 1 feature, then the rank\\n1 and rank 2 features, etc until all 78 features are included (note that the ‘ISIpAddressInDomainName’'),\n",
       " '80ffda75-4b5b-4ad3-a2c6-9880a6b40dfc': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 11}, page_content='feature is omitted, for no variance occurs). It is expected that the models perform with higher accuracy\\nas we increase the number of features available; however, the goal is to determine the optimal number of\\nfeatures such that the accuracy of the model is not penalized substantially. Figure 6 presents data from\\none feature up to 20 features for both binary and multi-class problems to increase the data’s resolution,\\nwhile Figure 7 illustrates the model accuracies as we increase the feature count up to 78. We brieﬂy\\n42'),\n",
       " '10317b27-b1f4-49f5-9778-ab526fe226b5': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 12}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nFigure 5: Distribution Plots of Top Four Binary-class Features\\nanalyze the results in each ﬁgure below.\\nThe multi-class problem analyzed in Figure 6 shows the expected upward trend for all models across\\nthe ﬁrst 20 features; however, most of this upward growth occurs in the ﬁrst ﬁve features. Random Forest\\ndemonstrates the signiﬁcance of these ﬁrst ﬁve features and slowly increases as we add more features.\\nFor the RF model, this is a clear indication of diminishing returns as the feature count increases. Both\\nKeras and Fast.ai severely underperform compared to RF for the ﬁrst 20 features. The Keras-TensorFlow\\nmodel has the largest growth in accuracy from 1-5 and 15-20 features. Fast.ai increases performance by\\n30 percentage points within the ﬁrst 10 features, however this accuracy stays constant with the addition'),\n",
       " 'aa99a4d1-4601-4dcc-a54c-f6d6c9fc9e9e': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 12}, page_content='of another 10 features. Finally, the variation of the accuracies across the 10 folds is shown, with the\\nKeras-TensorFlow model demonstrating extremely high variation compared to both Fast.ai and RF.\\nIn contrast to Figure 6, the accuracies of the models on the binary classiﬁcation problem indicate that\\nthe models perform similarly and require fewer features. While RF still demonstrates superior accuracy,\\nKeras is consistently within 5 percentage points. Surprisingly, these models were able to perform with\\n85-90% accuracy with only the ﬁrst feature. Fast.ai, conversely, shows very poor performance with the\\nﬁrst feature, but the performance slowly increases as the feature size increases. Additionally, all models\\nshow smaller variation across the folds compared to the data seen in the multiclass problem (Figure 6).\\nFigure 7 show the change in accuracies for each model from one feature up to all 78, with half the data'),\n",
       " 'e8c1ce1c-3b62-4897-b349-5e18181bf24c': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 12}, page_content='resolution of Figure 6. We see that the accuracies of all the models in Figure 7 increase with the addition\\nof more features, as expected. However, while Fast.ai shows substantial improvement in performance\\nfrom 20 to 50 features, neither Keras-TensorFlow nor RF dramatically increase in performance. The\\nissue of high variation across the folds continues for Keras-TensorFlow, while only being an issue for\\nFast.ai between 10 and 25 features.\\nWhile it is expected that the models will perform with higher accuracy when we increase the features\\navailable to the algorithms, Figure 7 presents slightly different trends. While RF, Keras-TensorFlow,\\n43'),\n",
       " 'cdbb3e4c-05e0-48f4-b156-6d69a57768ce': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 13}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nFigure 6: Binary Classiﬁcation (left) and Multi Classiﬁcation (right) for Increasing Number of Features\\nup to n=20\\nFigure 7: Binary Classiﬁcation (left) and Multi Classiﬁcation (right) for Increasing Number of Features\\nup to n=78\\nand Fast.ai all show increases in performance from 1 to 40 features, the addition of more features ap-\\npears to have a detrimental effect on RF and Keras-TensorFlow models. While the Keras-TensorFlow\\nmodel shows high variance within the multiclass problem, the binary classiﬁcation problem sees this\\nfor the addition of speciﬁc features. In order of appearance from left to right, the features that appear\\nto have substantial, negative effects on the performance of the Keras-TensorFlow model are ’domain-\\nUrlRatio’, ‘ldl getArg’, ’File name DigitCount’, ’File name DigitCount’, ’LongestVariableValue’, and\\n’subDirLen’.\\n7 Discussion'),\n",
       " 'e3d4403c-6817-49cd-8e3b-625962325e50': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 13}, page_content='7 Discussion\\nThe current research sought to explore the effectiveness of the provided URL attributes and demonstrate\\nthe effectiveness lexical analysis may have in detecting and classifying malicious URLs, placing an\\nemphasis on practicality for an industrial setting. This experimental research mainly focused on the\\ndetection (Experiment 1) and classiﬁcation (Experiment 2) of various types of URLs based on lexical\\nanalysis through binary and multiclass classiﬁcation experiments, with an emphasis on the comparison\\nof popular deep learning models with traditional machine learning algorithms. Experiment 1 results\\ndemonstrated higher performance accuracy overall, with an increase of 8-10% on average across all\\nmodels. While Experiment 2 results showed lower performance with the average accuracy around >\\n44'),\n",
       " '717705c8-7ddd-41cc-8a56-a638bd71b7b9': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 14}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\n85%. Random Forest, Keras-TensorFlow, and Fast.ai consistently performed the best of all the models\\nexperimented, with >96% accuracy in both the experiments.\\nBy collecting the training and prediction times, in conjunction with the feature analysis, we conclude\\nthat, despite their popularity, deep neural networks are inferior to Random Forest due to their higher\\nvariance, count of features required to match RF performance, complexity, and overall amount of time\\nto train and predict when deployed. To minimize the effort required to potentially deploy a RF model,\\nthe feature set can be reduced down to 5-10 features with minimal cost to performance. While both\\nKeras-TensorFlow and Fast.ai are popular DNN frameworks, their deployment over RF requires more\\nresources which may be better spent elsewhere within an organization.'),\n",
       " 'fcf2a4f4-f047-47f1-af67-721d6e508f71': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 14}, page_content='Overall, it is clear from the results that Random Forest is the most appropriate model for deployment\\nin an organization’s detection system. RF was a model that demonstrated lower complexity, as seen in\\nits lower training (Table 4) and prediction times (Table 5), and higher performance as seen in Tables 2\\nand 3. Further, RF typically performed up to or over 90% accuracy within the top ﬁve features for both\\nbinary and multi-class problems, whereas both DNN models required up to 30-40 features to match RF’s\\nperformance with only ﬁve in the multi-class problem (Figures 6 and 7).\\nThe results acquired from the deep neural network models indicate further work is required to clearly\\ndemonstrate one’s superiority over another. While the accuracy of the Fast-AI model exceeded that of\\nKeras-TensorFlow (Tables 2 and 3), Fast-AI experienced a substantial performance hit for both training\\nand sample prediction times (Tables 4 and 5, respectively). With the current work, a preference of one'),\n",
       " '16eeb0f2-1cee-48dd-bd54-6a21dcf940b1': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 14}, page_content='DNN model over the other would indicate the priorities required from the model: accuracy at the cost\\nof time dictates Fast-AI superior while low latency at the cost of accuracy prefers the Keras-TensorFlow\\nmodel.\\nAdditionally, as the ﬁnal contribution of this work, the feature analysis of the lexical-based ISCX-\\nURL-2016 dataset shows the importance of speciﬁc characteristics of these malicious URLs. The main\\nconclusion from this section of the work clearly indicates a higher need for more features in the multi-\\nclassiﬁcation problem than the binary classiﬁcation problem. This is clearly seen from the p-values\\npresented in Table 6 and the slower increase in accuracies in Figures 6 and 7. While all three models\\nshowed large improvements within the ﬁrst 5-10 features, as ranked by their p-values, there was a drastic\\nimprovement in the accuracies when features ranked 15-25 were included.\\n8 Conclusion and Future Work'),\n",
       " '1c252667-c8a6-4f6e-841a-4399ee9c9252': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 14}, page_content='This work explored the applications of machine learning and deep learning models in detecting and clas-\\nsifying malicious URLs. By collecting performance accuracy, confusion matrices, and both training and\\npredicting times for the Random Forest, Keras-TensorFlow, and Fast-AI models, this study concludes\\nthat Random Forest is the best model to deploy by organizations seeking to build an URL ﬁlter applica-\\ntion, or those wishing to incorporate machine learning techniques to improve existing ones. Additionally,\\nthis study reports the speciﬁc lexical features contained within URLs may be used to minimize overhead\\ncost of a deployed model.\\nSome limitations attached to the present study could motivate further research; e.g., our team did not\\nexhaustively explore all the network conﬁgurations and hyperparameters available for DNNs which may\\npotentially improve their performances. While these improvements may lead to succeeding RF’s reported'),\n",
       " '3e8c35d3-9f8e-4427-ab24-c17b3c8a8365': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 14}, page_content='accuracy, there is an impact on training and testing times as well as additional drawback of overﬁtting\\nmodels thus reducing their real-word generalizability. Finally, we did not deploy and investigate the\\nefﬁcacy of the models with further experiments as explored in [17]; we leave this as our future research\\nwork. Importantly, we feel that more research on this front is needed to better bridge the gap between\\nacademic research and industrial applications with the goal of reducing detrimental economic impacts of\\n45'),\n",
       " '1e0993c9-373c-4b5b-af01-755ccdea296f': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 15}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\nmalicious URLs on organizations in various industries.\\nAcknowledgments\\nThis research project was supported by the state of Colorado through funds appropriated for cybersecurity\\nlaw dubbed “Cyber Coding Cryptology for State Records.” Any opinions, ﬁndings and conclusions, or\\nrecommendations expressed in this paper are those of the authors and do not necessarily reﬂect the views\\nof the funding sources. The authors would like to thank Robert Dunsky, Nathan Bellew and Karen Angels\\nfor helping with some of the early versions of the experiments.\\nReferences\\n[1] M. Abadi, A. Agarwal, et al. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems.\\nArXiv , abs/1603.04467, March 2016.\\n[2] R. Basnet. Deep learning malicious urls. \"https://github.com/rambasnet/\\nDeepLearningMaliciousURLs\" , [Online; accessed on September 10, 2020], 2019.'),\n",
       " '5e9fd424-2c0f-47c9-b1f6-710e1f2bff23': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 15}, page_content='[3] L. Breiman. Random forests. Machine learning , 45(1):5–32, October 2001.\\n[4] L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen. Classiﬁcation and regression trees . CRC press,\\n1984.\\n[5] Z. Chen and J. Szurdi. Cybersquatting: Attackers mimicking domains of major brands including face-\\nbook, apple, amazon and netﬂix to scam consumers. https://unit42.paloaltonetworks.com/\\ncybersquatting/ [Online; accessed on September 15, 2020], 2020.\\n[6] H. Choi, B. Zhu, and H. Lee. Detecting malicious web links and identifying their attack types. In Proc. of the\\n2nd USENIX Conference on Web Application Development (WebApps’11), Portland, Oregon, USA , page 11.\\nUSENIX, June 2011.\\n[7] T. Cover and P. Hart. Nearest neighbor pattern classiﬁcation. IEEE transactions on information theory ,\\n13(1):21–27, January 1967.\\n[8] B. Cui, S. He, X. Yao, and P. Shi. Malicious url detection with feature extraction based on machine learning.'),\n",
       " 'fc9325b4-ca68-453f-8c3e-fbb23607eb4a': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 15}, page_content='International Journal of High Performance Computing and Networking , 12(2):75–84, August 2018.\\n[9] Cybersecurity and I. S. Agency. Fbi releases ic3 2019 internet crime report. \"https://us-cert.cisa.\\ngov/ncas/current-activity/2020/02/12/fbi-releases-ic3-2019-internet-crime-report\"\\n[Online; accessed on May 08, 2020], 2019.\\n[10] B. Eshete, A. Villaﬁorita, and K. Weldemariam. Binspect: Holistic analysis and detection of malicious\\nweb pages. In Proc. of the 8th International ICST Conference (SecureComm’12), Padua, Italy , volume\\n106 of Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications\\nEngineering , pages 149–166. Springer, Berlin, Heidelberg, September 2012.\\n[11] fast.ai. About. \"https://www.fast.ai/about\" [Online; accessed on July 29, 2020].\\n[12] fast.ai. fastai v1 for pytorch: Fast and accurate neural nets using modern best practices. https://www.\\nfast.ai/2018/10/02/fastai-ai/ [Online; accessed on September 10, 2020].'),\n",
       " '9cfc522a-30c2-4ca5-aca7-0ca09696ae3e': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 15}, page_content='[13] Y . Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to\\nboosting. Journal of computer and system sciences , 55(1):119–139, August 1997.\\n[14] J. Howard and S. Gugger. Fastai: A layered api for deep learning. Information , 11(2):108, February 2020.\\n[15] Infosecinstitute. Infosec iq, power to your people. \"https://www.infosecinstitute.com/iq/\" , [On-\\nline; accessed on August 18, 2020].\\n[16] S. Institute. Robust phishing awareness simulation training that changes behavior. \"https://www.sans.\\norg/security-awareness-training/products/phishing\" , [Online; accessed on August 18, 2020].\\n[17] A. Joshi, L. Lloyd, P. Westin, and S. Seethapathy. Using lexical features for malicious url detection–a\\nmachine learning approach. arXiv:1910.06277, October 2019. https://arxiv.org/abs/1910.06277\\n][Online; accessed on September 15, 2020].\\n46'),\n",
       " '8a1f21ca-dce3-4902-8f3c-204a6c7500e1': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 16}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\n[18] N. P. Jouppi, C. Young, et al. In-datacenter performance analysis of a tensor processing unit. In Proc. of the\\n44th Annual International Symposium on Computer Architecture (ISCA’17), Toronto, Canada , pages 1–12.\\nACM, June 2017.\\n[19] A. Karim, S. Azam, B. Shanmugam, K. Kannoorpatti, and M. Alazab. A comprehensive survey for intelligent\\nspam email detection. IEEE Access , 7:168261–168295, November 2019.\\n[20] Keras.io. Keras: The python deep learning api. https://keras.io [Online; accessed on August 09, 2020].\\n[21] S. KP, M. Alazab, et al. Malicious url detection using deep learning. https://www.techrxiv.org/\\narticles/preprint/Malicious_URL_Detection_using_Deep_Learning/11492622 [Online; ac-\\ncessed on September 15, 2020], January 2020.\\n[22] H. Le, Q. Pham, D. Sahoo, and S. Hoi. Urlnet: Learning a url representation with deep learning for malicious'),\n",
       " '663ff745-49c3-46e3-bd31-589fe2d5708f': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 16}, page_content='url detection. arXiv:1802.03162, March 2018. https://arxiv.org/abs/1802.03162 [Online; accessed\\non September 15, 2020]].\\n[23] J. Ma, L. Saul, S. Savage, and G. V oelker. Identifying suspicious urls: An application of large-scale online\\nlearning. In Proceedings of the 26th International Conference on Machine Learning (ICML’09), Montreal\\nQuebec, Canada , pages 681–688. ACM, June 2009.\\n[24] M. Mamun, M. Rathore, A. Lashkari, N. Stakhanova, and A. Ghorbani. Detecting malicious urls using lexical\\nanalysis. In Proc. of the 10th International Conference on Network and System Security (NSS’16), Taipei,\\nTaiwan , volume 9955 of Lecture Notes in Computer Science , pages 467–482. Springer, September 2016.\\n[25] Microsoft. Deﬁning malware: Faq. \"https://docs.microsoft.com/en-us/previous-versions/\\ntn-archive/dd632948(v=technet.10)\" [Online; accessed on August 13, 2020].\\n[26] T. Nash. An undirected attack against critical infrastructure: A case study for improving your control'),\n",
       " '672e9736-4843-4a39-bb02-d8e536b37466': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 16}, page_content='system security. \"https://us-cert.cisa.gov/sites/default/files/recommended_practices/\\nCaseStudy-002.pdf\" [Online; accessed on August 13, 2020], 2005.\\n[27] U. of Connecticut. Protect yourself from future phishing scams. \"https://phishingeducation.uconn.\\nedu/\" , [Online; accessed on August 18, 2020], 2020.\\n[28] F. B. of Investigation. Business email compromise the $26 billion scam. \"https://www.ic3.gov/media/\\n2019/190910.aspx\" , [Online; accessed on August 18, 2020], 2019.\\n[29] F. Pedregosa, G. Varoquaux, et al. Scikit-learn: Machine learning in python. Journal of Machine Learning\\nResearch , 12(85):2825–2830, 2011.\\n[30] PyTorch. Pytorch. https://pytorch.org [Online; accessed on August 09, 2020].\\n[31] PyTorch. Pytorch 1.3 adds mobile privacy quantization and named tensors. \"https://pytorch.\\norg/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors/\\n#speech-extensions-to-fairseq\" , [Online; accessed on October 09, 2020].'),\n",
       " '670bf1c9-8027-45b3-8f98-2b8ecf41550d': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 16}, page_content='[32] M. Romagna and N. van den Hout. Hacktivism and website defacement: Motivation, capabilities and poten-\\ntial threats. In Proc. of the 27th Virus Bulletin International Conference, Madrid, Spain , October 2017.\\n[33] D. Sahoo, C. Liu, and S. Hoi. Malicious url detection using machine learning: A survey. arXiv:1701.07179,\\nAugust 2019. https://arxiv.org/abs/1701.07179 [Online; accessed on August 13, 2020].\\n[34] Scikit-Learn. 1.13. feature selection - univariate feature selection. \"https://scikit-learn.org/\\nstable/modules/feature_selection.html#univariate-feature-selection\" [Online; accessed\\non September 15, 2020], 2007.\\n[35] Scikit-Learn. sklearn.feature selection.chi2. \"https://scikit-learn.org/stable/modules/\\ngenerated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2\" [On-\\nline; accessed on September 15, 2020], 2007.\\n[36] TensorFlow. Tensorﬂow. https://www.tensorflow.org [Online; accessed on August 09, 2020].'),\n",
       " 'd8e4b450-8d07-4e08-8adb-7768991b95c4': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 16}, page_content='[37] E. Uc ¸ar, M. Incestas ¸, and M. Ucar. A deep learning approach for detection of malicious urls. In Proc. of the\\n6th International Management Information Systems Conference (IMISC’19), Istanbul, Turkey , pages 12–20,\\nOctober 2019.\\n[38] A. Van der Merwe, M. Loock, and M. Dabrowski. Characteristics and responsibilities involved in a phish-\\ning attack. In Proc. of the 4th International Symposium on Information and Communication Technologies\\n(WISICT’05), Cape Town, South Africa , pages 249–254. Trinity College Dublin, January 2005.\\n47'),\n",
       " '6b78f11f-d399-4356-8c7f-0d81e61627fc': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 17}, page_content='Detecting and Classifying Malicious URLs Johnson, Khadka, Basnet, and Doleck\\n[39] Verizon. 2020 data breach investigations report. https://enterprise.verizon.com/resources/\\nreports/2020-data-breach-investigations-report.pdf [Online; accessed on May 08, 2020].\\n[40] R. Verma and A. Das. What’s in a url: Fast feature extraction and malicious url detection. In Proc. of the 3rd\\nACM on International Workshop on Security and Privacy Analytics (IWSPA’17), Scottsdale, Arizona, USA ,\\npages 55–63. ACM, March 2017.\\n[41] J. Zhao, N. Wang, Q. Ma, and Z. Cheng. Classifying malicious urls using gated recurrent neural networks.\\nInProc. of the 12th International Conference on Innovative Mobile and Internet Services in Ubiquitous\\nComputing (IMIS’18), Kunibiki Messe, Matsue, Japan , volume 773 of Advances in Intelligent Systems and\\nComputing , pages 385–394. Springer, Cham, June 2018.\\n——————————————————————————\\nAuthor Biography'),\n",
       " '899dec62-59d9-4dd4-b417-a44d63ff6d34': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 17}, page_content='Author Biography\\nClayton Johnson is a senior undergraduate pursuing his Bachelor’s in Computer Sci-\\nence from Colorado Mesa University (CMU) and earned his Professional Certiﬁcate\\nin Cybersecurity from CMU in 2019. He is the former president of the CMU’s Com-\\nputer Science club and is a research fellow at the Cybersecurity Center at CMU. Clay-\\nton will begin pursing his Master’s in Technology, Cybersecurity, and Policy at the\\nUniversity of Colorado Boulder in 2021.\\nBishal Khadka is a senior undergraduate student pursuing his Bachelor’s in Com-\\nputer Science and Professional Certiﬁcate in Cybersecurity degrees at Colorado Mesa\\nUniversity (CMU). Bishal is currently the president of Cybersecurity club and a re-\\nsearch fellow at the Cybersecurity Center at CMU.\\nRam B. Basnet is an associate professor of Computer Science at Colorado Mesa Uni-\\nversity (CMU). He received his BS in Computer Science from CMU in 2004 and MS'),\n",
       " '19af5715-240c-4774-bcb6-845250888245': Document(id=None, metadata={'source': 'uploaded_pdfs\\\\Towards_Detecting_and_Classifying_Malici.pdf', 'page': 17}, page_content='and PhD in Computer Science from New Mexico Tech in 2008 and 2012, respectively.\\nHis research interests are in the areas of information assurance, machine learning, and\\ncomputer science pedagogy.\\nTenzin Doleck received his PhD from McGill University in 2017. He is currently a\\npost-doctoral fellow at the University of Southern California.\\n48'),\n",
       " '77e9dc66-a4f0-4737-9ec2-4944581f08ad': Document(page_content='Annual Survey of Indian Law 192 [2021\\n9\\nCYBER LA W\\nDeepa Kharb*\\nI INTRODUCTION\\nCYBER LA W, a swiftly progressing field that intersects with numerous conventional\\nlegal disciplines, has under gone substantial transformations since 2014.This survey\\nexamines the evolving landscape of cyber law by analyzing the judicial decisions in\\n2021. It delves into crucial areas such as online privacy , data protection, cybercrimes,\\nand electronic evidence, providing valuable insights into the development of cyber\\nlaw in India. This survey serves as a practical guide for navigating the intricate\\nchallenges of the digital realm. Additionally , it presents a critical perspective on the\\ncourt’ s reasoning, identifying points that may be subject to further debate. Overall,\\nthe survey underscores the dynamic nature of cyber law and its significant impact on\\nthe legal framework, reflecting the judiciary’ s efforts to adapt to the challenges posed\\nby the digital age.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 0}),\n",
       " '64756220-7d4c-461d-bcb4-2cb00a724e57': Document(page_content='by the digital age.\\nII ADMISSIBILITY  OF ELECTRONIC EVIDENCE: SECTION 65B IEA\\nThe introduction of sections 65A  and 65B in the Evidence Act in 2000 provided\\na framework for the admissibility of electronic evidence, with section 65B (1) allowing\\nfor the admissibility of a paper printout of information contained in electronic records\\nsubject to the conditions specified in section 65B(2).\\nHowever , the requirement for a certificate under section 65B of the Indian\\nEvidence Act for electronic records to be admissible in court has been a matter of\\ndebate among legal scholars and courts. While the Supreme Court in Anvar P .V. v.\\nP.K. Basheer1 (Anvar  hereinafter) held that such records cannot be admitted as\\nsecondary evidence unless the requirements of section 65B are met, the court in Shafhi\\nMohammad 2 (Shafhi  hereinafter) concluded that the certificate requirement may be\\nwaived wherever the interest of justice so justifies say when the electronic device', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 0}),\n",
       " 'd0397772-750d-4f67-8761-b12326977b0e': Document(page_content='storing the records is inaccessible or  where the electronic device is produced by a\\nparty who is not in possession of such device, as a result of which such party would\\nnot be in a position to secure the requisite certificate.\\n* Assistant Professor , The Indian Law Institute, New Delhi.\\n1 (2014) 10 SCC 473.\\n2 (2018) 2 SCC 801 (decided on Apr. 25, 2018).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 0}),\n",
       " 'd2ec426d-f981-42dd-a74a-189d6133442d': Document(page_content='Cyber Law Vol. LVII] 193\\nIn a recent decision, Arjun Panditrao Khotkar  v. Kailash Kushanrao Gorantyal3\\n(Arjun Panditrao Khotkar hereinafter), a three-judge bench4 of the Supreme Court\\nclarified that the certificate required under section 65B(4) is a prerequisite for the\\nadmissibility of electronic evidence. The bench af firmed the correctness of the Anvar\\nruling in its interpretation, while finding that the Shafhi decision’ s division bench\\nhad erroneously “clarified” the requirement. The court also noted that the certificate\\nis not necessary if the original electronic record is produced in court, however ,\\ncompliance with section 65B is compulsory before a ‘computer output’, which is\\nconsidered secondary evidence of an electronic record, can be admitted as evidence.\\nThe court stated that if a person refuses to provide the certificate required under\\nsection 65B (4) of the Indian Evidence Act, a party can make an application to the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 1}),\n",
       " '4f8ec2ca-baab-4ee1-9a69-5cf6bb49f25f': Document(page_content='judge requesting the production of the certificate. The court also explained that if it is\\nimpossible for the person to provide the certificate, or if the law excuses the person\\nfrom doing so, then the party should be excused from the mandatory requirement of\\nsection 65B (4).5 The court instructed trial courts to summon the person(s) specified\\nin section 65B (4) when a defective certificate is given or when a certificate is refused,\\nand require them to provide the necessary certificate. The court clarified that since\\nsection 65(B) does not talk about the stage at which such certification can take place,\\nthis is subject to the discretion exercised by the courts in civil cases, and in criminal\\ntrials, the accused must be supplied with all documents that the prosecution seeks to\\nrely upon before the trial. The courts must balance the rights of the parties while\\nexamining any application by the prosecution under sections 91 or 31 1 of the Criminal', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 1}),\n",
       " '8f428983-bcea-4d2e-9c57-0c6b3cc675fd': Document(page_content='Procedure Code, 1973 or section 165 of the Evidence Act, ensuring no serious or\\nirreversible prejudice to the accused.\\nAlthough the relaxation of the strict requirements under section 65B (4) was\\naimed at easing the burden on parties who have made best ef forts to obtain a certificate\\nbut failed to do so, it has been ar gued that such an exception goes beyond what the\\nstatute permits and creates further ambiguity . Furthermore, the obligation on the courts\\nto summon the authorized person(s) to produce the certificate could result in a\\nprolonged mini-trial within the trial, adding to the already overburdened judicial system\\nand causing delays and additional expenses for the parties involved.\\nAfter the Arjun Panditrao Khotkar6 case, various high courts in India have\\nfollowed its ratio in their respective judgments especially on the exception created.\\nThey have held that electronic evidence must be accompanied by a certificate under', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 1}),\n",
       " '6bcd789c-3c1f-4cc4-b76a-a6c366c3ac0b': Document(page_content='section 65B of the Indian Evidence Act to be admissible in court. Failure to comply\\nwith this requirement results in the electronic evidence being inadmissible.\\n3 2020 SCC OnLine SC 571(decided on July14, 2020).\\n4 Bench consisting of RF Nariman, S. Ravindra Bhat, and V. Ramasubramanian\\n5 Due to the applicability of the Latin maxims ‘lex non cogit ad impossibilia’  (the law does not\\ndemand the impossible) and ‘impotentia excusat legem’  (when there is a disability that makes\\nit impossible to obey the law , the alleged disobedience of the law is excused).\\n6 Supra note 3; 2020 SCC OnLine SC 571(decided on July14, 2020).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 1}),\n",
       " '4c5e1810-2d35-45f3-ac7b-5acc9ce1cc85': Document(page_content='Annual Survey of Indian Law 194 [2021\\nIn Rakesh Kumar Singla  v. Union of India7  the petitioner , under the NDPS Act,\\nfiled a bail petition claiming they were unlawfully detained as no contraband was\\nfound in their possession at the time of arrest. The Narcotics Control Bureau (NCB)\\npresented statements from a co-accused and the petitioner as evidence. However , the\\ncourt emphasized that the determination of the petitioner ’s guilt or innocence should\\nbe based on the evidence presented during the trial. The NCB opposed the bail\\napplication, citing WhatsApp chat screenshots as evidence connecting the petitioner\\nto the illicit drugs. Nevertheless, the court stated that WhatsApp messages cannot be\\nconsidered as valid evidence without a certificate under section 65B of the Indian\\nEvidence Act, as per the recent Supreme Court judgment in the case of Arjun Panditrao\\nKhotkar  v. Kailash Kushanrao Gorantyal.8 Therefore, the court concluded that the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 2}),\n",
       " '381d3fdb-cc16-4701-b8b9-9fe10d3ef626': Document(page_content='WhatsApp messages, in their current state, hold no evidentiary value. Investigating\\nagencies may rely on WhatsApp messages during a crime investigation, but a certificate\\nunder section 65B of the Indian Evidence Act is necessary for their admissibility .\\n In Mahendra N. Par deshi v. State of Maharashtra9 the prosecution attempted\\nto prove the content of a DVR as evidence in a bribery case under section 7 and\\n13(1)(d) read with 13(2) of Prevention of Corruption Act, 1988, but failed to produce\\nthe original DVR or a certificate under section 65-B(4) of the Indian Evidence Act.\\nThe court held that verbal evidence about the contents of an electronic record is\\nconsidered secondary evidence and the prosecution must prove that the contents of\\nthe DVR were heard. The court found that the prosecution failed to provide evidence\\nthat the contents of the DVR were heard and the witness could not confirm the\\nconversation’ s content. Additionally , since the record was deleted, the court drew an', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 2}),\n",
       " 'f222649d-edfd-4969-95bb-8295560346b8': Document(page_content='adverse inference against the prosecution and dismissed the case.\\nIn Yogesh Arun W akure v. State of Maharashtra ,10 the appellant, an accused\\nfacing the char ge of murder punishable under section 302, 201, 323, 143, 147, 149\\nread with section 135 of the IPC with others, tried to establish his presence inside the\\nhotel on the basis of the CCTV  footage. However , an eye-witness claimed to have\\nseen the present appellant at the crime spot around the time of murder .\\nIt was contended from the appellant side that the trial court should not have\\ndisregarded the IO’ s report based on call detail records (CDR) and subscriber detail\\nrecords (SDR) without considering section 65B and relied solely on the word of mouth\\nof the eye-witness  as it is often said that “humans may lie, but documents would not\\nlie” or “documents would speak louder than words”. The apex court has held in Anvar11\\nthat electronic records must be produced according to section 65B, after which their', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 2}),\n",
       " '9e5f2fe1-f7a9-44eb-ad10-78656e3594fc': Document(page_content='genuineness can be questioned and resort can be made to section 45A of the Evidence\\nAct for seeking an opinion of the examiner of electronic evidence. The court directed\\nthe Registrar to transmit the documents, including the CDR/SDR record and DVD, to\\n7 MANU/PH/001 1/2021.\\n8 Supra note 3.\\n9 2020 SCC OnLine Bom 7873(Decided on Oct. 23,2020).\\n10 2021 SCC OnLine Bom 354 (Decided on Mar . 10, 2021).\\n11 Supra note 1.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 2}),\n",
       " 'e6ec985b-e697-4745-b007-ede4d165916e': Document(page_content='Cyber Law Vol. LVII] 195\\nthe trial court in a sealed envelope. The trial court will open the envelope upon receipt,\\nand the contents will be a part of the original record.\\nIn the case of Pramod v. State of Maharashtra,12 the accused faced char ges\\nunder various sections of the Indian Penal Code, 1860 including murder , kidnapping,\\nand destruction of evidence. The prosecution sought to rely on electronic evidence in\\nthe form of Call Data Records (CDR) to prove their case.\\nThe defense counsel ar gued that the electronic evidence presented by the\\nprosecution is not admissible because the certificates do not comply with section 65B\\n(4) of the Evidence Act. The certificates do not identify the electronic record containing\\nthe statements and do not specify the devices or computers over which they had control.\\nThe defense counsel also pointed out that none of the certificates are signed by a\\nperson occupying a responsible of ficial position in relation to the operation of the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 3}),\n",
       " '38884ced-dc0e-42e2-b0f4-9dc0c3c0fb19': Document(page_content='relevant device or the management of the relevant activities. Therefore, the certificates\\nrelied upon by the prosecution in support of electronic evidence are not admissible in\\nevidence. The defense counsel did not dispute the exchange of calls between the\\naccused persons or the findings of the trial court regarding the cell phone locations of\\nthe accused persons and exchange of calls between them at the relevant time.\\nSections 65A  and 65B of the Evidence Act, deal with the admissibility and\\ncontents of electronic records as evidence in court. Electronic records are considered\\nto be complete in themselves and can be admissible as evidence subject to the\\nprovisions of Section 65B (4) of the Evidence Act. Section 65B(1) of the Evidence\\nAct distinguishes between the original electronic record and the output from such\\ndevices, which is a copy or data derived from the original document. The original\\nelectronic record is the one on which the information is first stored, and the secondary', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 3}),\n",
       " '6e0fb6b2-203a-4302-8e90-08c1d95c394a': Document(page_content='document is the one that contains the information derived from the original electronic\\nrecord. The same was expounded by the Supreme Court in the case of Arjun Panditrao\\nKhotkar  in the following words:13\\n73.2. The clarification referred to above is that the required certificate\\nunder Section 65B(4) is unnecessary if the original document itself is\\nproduced. This can be done by the owner of a laptop computer , computer\\ntablet or even a mobile phone, by stepping into the witness box and\\nproving that the concerned device, on which the original information\\nis first stored, is owned and/or operated by him. In cases where the\\n“computer” happens to be a part of a “computer system” or “computer\\nnetwork” and it becomes impossible to physically bring such system\\nor network to the Court, then the only means of providing information\\ncontained in such electronic record can be in accordance with Section\\n65B(1), together with the requisite certificate under Section 65B(4).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 3}),\n",
       " 'cd7c4028-f093-49d3-a751-b14fc6435e63': Document(page_content='The last sentence in Anvar P .V. (supra)  which reads as “…if an\\nelectronic record as such is used as primary evidence under Section 62\\nof the Evidence Act…” is thus clarified; it is to be read without the\\n12 2021 SCC OnLine Bom 3344\\n13 Arjun Panditrao Khotkar   supra note 3 in para no. 73.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 3}),\n",
       " '238cd3ba-2a7b-4a27-8695-dc3ff4ab93fc': Document(page_content='Annual Survey of Indian Law 196 [2021\\nwords “under Section 62 of the Evidence Act,…” With this clarification,\\nthe law stated in paragraph 24 of Anvar P .V. (supra)  does not need to\\nbe revisited.\\nSections 65A  and 65B of the Evidence Act deal with the admissibility and\\ncontents of electronic evidence. A section 65B(4) certificate is mandatory for secondary\\nevidence and can be given by a person in a responsible position related to device\\noperation or management. The court relied on previous rulings Arjun Panditrao\\nKhotkar  14 and Engineering Analysis Centr e15 to hold that the prosecution should be\\nrelieved of the obligation to provide a section 65B(4) certificate if they have made\\nefforts to obtain it but have no control over the relevant third-party companies. The\\ncourt found that the electronic evidence produced by the prosecution was admissible\\nin and suf ficiently corroborated the circumstantial evidence presented. The certificates', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 4}),\n",
       " '8b5a4fda-d8d7-4ccd-bc57-f33c989827f3': Document(page_content='produced by the prosecution were found to identify the electronic records and describe\\nthe manner in which they were produced as mandated by apex court in Anvar . The\\ncourt noted that section 65B(4) is mandatory but any infirmity in the certificates can\\nbe overlooked given the circumstances.\\n The prosecution in State of Maharashtra, thr ough the Police S tation Officer v.\\nSagar V ishwanath Borkar16 case relied on CCTV  footage, which was copied onto a\\npen drive and a certificate under section 65B of the Evidence Act was taken. However ,\\nthe prosecution failed to produce primary evidence in the form of the hard disc of the\\nCCTV  footage. The court held that the prosecution needed to comply with sub-section\\n(4) of section 65B of the Evidence Act, which requires evidence of a person in a\\nresponsible of ficial position in relation to the operation or management of the CCTV\\nsystem. The mere exhibition of the CCTV  footage by the trial court and the absence', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 4}),\n",
       " '7781cb9a-f76d-4e75-bdc0-ae5cc6d84f3e': Document(page_content='of objections by the accused are not suf ficient to make the footage admissible. The\\ncourt cited Arjun Panditrao Khotkar  17to support this ruling. As a result, the CCTV\\nfootage cannot be relied upon as admissible evidence by the prosecution as it was not\\nsupported by a certificate under section 65B(4) of the Evidence Act. The court opined\\nthat the trial court was correct in refusing to rely on this evidence for non-compliance\\nwith section 65B(4) of the Evidence Act.\\nIn Sanjib Sarkar v. Rajasr ee Roy ,18 a matrimonial dispute concerning the\\nannulment of marriage under section 25(III) of the Special Marriage Act, the\\nadmissibility of electronic evidence, including Facebook posts and pictures, submitted\\nby the respondent/wife was challenged by the appellant/husband’ s counsel on the\\ngrounds of lack of certification under section 65B (4) of the Indian Evidence Act. The\\ncourt considered the ar guments and referred to the law laid down in the Arjun Panditrao', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 4}),\n",
       " '42c10dbd-863d-488e-9d0f-c168e35d503b': Document(page_content='Khotkar  case,19 which distinguished between the manner of tendering primary and\\n14 Supra note 3.\\n15 2021 SCC OnLine SC 159(Decided on Mar . 2, 2021).\\n16 2021 SCC OnLine Bom 2725(Decided on Sep. 7, 2021).\\n17 Supra note 3.\\n18 2021 SCC OnLine Cal 2916(Decided on Nov . 11, 2021).\\n19 (2020) 7 SCC 1, supra note 3.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 4}),\n",
       " '7297b3a0-3120-46ec-8157-58a813f0e301': Document(page_content='Cyber Law Vol. LVII] 197\\nsecondary evidence in electronic form. The court held that the required certification\\nwas not necessary for original documents produced by the owner of the device who\\ncan prove ownership and operation by stepping into the witness box. However , in\\nsituations where the source of information is part of a computer or computer network,\\ncertification under section 65B(4) is required. In the present case, the electronic\\nevidence relied upon by the respondent was sourced from her original electronic device\\nand therefore, certification was not required. The court found that the evidence\\npresented by the respondent was admissible and she had proved her contention relating\\nto fraud practiced on her .\\nIn another case of Sachin Makade Bablu Bhagwan Dangr e v. Nar cotics Contr ol\\nBureau,20 the accused individuals were char ged with dealing in illegal medical drugs,\\nit was ar gued by accused that no such drugs were found in their possession or at their', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 5}),\n",
       " 'b75a16dc-0011-48f1-a38a-67436a62c9a9': Document(page_content='places of residence or work. The sole evidence against them was supposedly retrieved\\nfrom their electronic devices, which have been contested as inadmissible without a\\ncertificate under section 65B of the Indian Evidence Act. The defense ar gued that the\\npossibility of future criminal behavior must be taken into account under section 37\\nNDPS, and that the recovered Tramadol tablets from Dipu Singh cannot be linked to\\nthe accused individuals. Additionally , the information extracted from their electronic\\ndevices cannot be accepted without the requisite certificate under section 65B of the\\nIndian Evidence Act.\\nThe court, citing the cases of Arjun Panditrao Khotkar  21 and Engineering\\nAnalysis Centr e of Excellence Private Limited v. The Commissioner of Income T ax,22\\nstated that the mandatory obligation under section 65B(4) of the Indian Evidence Act\\nmay be waived if the respondent has made all reasonable attempts to obtain the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 5}),\n",
       " 'd2356312-adcf-4a39-8006-7567d43a1810': Document(page_content='necessary certificate, even if it was to be provided by a third party who was not under\\ntheir control.\\nAlthough the National Control Bureau (NCB) obtained section 65B certificates\\nfrom a cyber forensic expert who analyzed the electronic devices and extracted the\\ndata, the court concluded that it was not suf ficient grounds to grant the accused\\nindividuals bail at this time. The court dismissed both petitions, but granted them the\\nfreedom to reapply for bail after examining public witnesses regarding the recovery .\\nAll pending applications were also resolved.\\nIn an interesting application filed by the petitioner under article 227, Sitanshi v.\\nVandana Sharma,23 seeking directions for Bharti Airtel Limited to preserve and produce\\nthe CDR of the respondent’ s mobile number . The petitioner ar gued that the CDRs\\nshould be preserved since they may be relevant and required at the time of the trial.\\nHowever , the Trial Court rejected the application, as filed under Section 151 of the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 5}),\n",
       " '8e873108-597d-4136-8d3f-d020ecbf8a27': Document(page_content='Civil Procedure Code, 1908, stating that it amounted to a roving inquiry and invasion\\nof privacy . The Delhi High Court  noted that the directions given by the Supreme\\n20 2021 SCC OnLine Del 5121(Decided on Nov . 29, 2021).\\n21 Supra note 3.\\n22 2021 SCC OnLine Bom 2725(Decided on Sep. 7, 2021).\\n23 2021 SCC OnLine Del 4497(Decided on Sep. 20, 2021).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 5}),\n",
       " '59cbdb72-ee73-4945-9dc9-b783e9211ddb': Document(page_content='Annual Survey of Indian Law 198 [2021\\nCourt in Arjun Panditrao Khotkar  case on preserving CDRs were in the context of\\nrecords seized during investigation and cannot be invoked in this case. The Supreme\\nCourt’ s directions on maintaining CDR and relevant records were only for those seized\\nduring investigation, as stated in paragraph 62. Paragraph 72 directs courts dealing\\nwith electronic evidence to ensure preservation and production of certificates for such\\nrecords seized during investigation. The high court found no infirmity in the trial\\ncourt’ s order and did not interfere.\\nThe High Court of Delhi in Megha Enterprises v. Haldiram Snacks Pvt. Ltd.,24\\nheard a petition under section 34 of the Arbitration and Conciliation Act, 1996. The\\npetitioner challenged an arbitral award dated October 26, 2020, ar guing that the arbitral\\ntribunal erred in accepting an electronic letter as evidence without proof and af fidavit', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 6}),\n",
       " 'ddf6f15e-8ecd-45f5-8d46-6b3cfcad6581': Document(page_content='under section 65B of the Evidence Act. However , the court rejected the ar gument,\\nstating that the Indian Evidence Act does not apply to arbitrations, and the petitioners\\ndid not raise any objections before the arbitrator . The court found evidence showing\\nthat the respondent sent an email acknowledging the balance confirmation of Rs.\\n19,03,77,000/-, which was mentioned in a letter issued by the respondent. The email\\nand letter are admissible as evidence under various provisions, including section 4 of\\nthe Information and Technology Act, 2000. The respondent did not dispute the\\ntransmission of information in electronic form. Therefore, emails acknowledging the\\ndebt due to the petitioner also meet the requirements under section 18 of the Limitation\\nAct, 1963.\\nThe petitioner in Lalu v. Sheeja25 had filed an original petition to cancel a divorce\\ndecree obtained by the first respondent, citing fraud. The petitioner had submitted an', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 6}),\n",
       " '64a375b8-4d35-41d7-b7b2-d29540567030': Document(page_content='application under section 45 of the Evidence Act to submit two CDs for voice\\nidentification. However , the court below had dismissed the application, stating that it\\ndid not comply with section 65B(4) of the Indian Evidence Act. The petitioner\\nchallenged this order in the original petition.\\nThe petitioner had also filed an application to send a CD to an expert for voice\\ncomparison, but the court had dismissed the application because it did not comply\\nwith section 65B(4) of the Indian Evidence Act. Nevertheless, the petitioner ar gued\\nthat a certificate was not required at this stage, as the CD only needed to be examined\\nby an expert. Additionally , the petitioner had produced the mobile phone containing\\nthe primary evidence. According to section 14 of the Evidence Act, the court should\\nrely on relevant evidence produced by the parties in matrimonial disputes. The court\\nshould not prevent a party from adducing relevant evidence to prove their case. As', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 6}),\n",
       " '780232e5-54e4-4728-bffd-01dc218404c8': Document(page_content='such, the court was wrong in disallowing the prayer sought in the application.\\nThe apex court, in Arjun Panditrao Khotkar  v. Kailash Kushanrao Gorantyal,26\\nhad held that a certificate under section 65B (4) was unnecessary if the original\\ndocument itself was produced. In proceedings under the family court, the technicalities\\nof the Indian Evidence Act regarding the admissibility or relevancy of evidence were\\n24 2021 SCC OnLine Del 2641(Decided on Apr. 15, 2021).\\n25 2021 SCC OnLine Ker 9833(Decided on Sep.17, 2021).\\n26 Supra note 3.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 6}),\n",
       " '5a74ca36-8f6a-4700-b1b2-22f339112710': Document(page_content='Cyber Law Vol. LVII] 199\\nnot strictly applicable. The court had the discretion to rely on the documents produced\\nif it was required to assist the court in ef fectively dealing with the dispute. The petitioner\\nhad wanted an expert opinion on the disputed conversation between the parties to the\\nproceedings, which was relevant under section 45 of the Evidence Act. The court\\nshould not preclude a party from adducing evidence that may be relevant in accordance\\nwith the Evidence Act to prove their case. Thus, the court below was wrong in\\ndisallowing the prayer sought for in the application.\\nThe court allowed the petitioner ’s application and set aside the impugned order .\\nIt directed the court below to summon Jeena along with the petitioner ’s power of\\nattorney holder and respondent no. 3 to record their voices. The recorded conversation\\nand CD will be sent to an examiner for electronic evidence opinion, with the petitioner\\nbearing the expenses.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 7}),\n",
       " 'e595ce0a-c6ec-4254-a7ca-23203659e8a7': Document(page_content='In M.P. Mathew v. Central Bur eau of Investigation27 the public prosecutor filed\\nan application to summon a witness to produce a certificate under section 65B of the\\nEvidence Act for certain documents already marked during the trial. The accused\\nchallenged this order but the special court allowed it. The court held that the certificate\\ncan be produced at any stage of the trial, but the rights of all parties must be balanced.\\nThe petitioner ’s senior counsel ar gued that they should be allowed to challenge the\\nadmissibility and marking of documents during the final hearing. The court dismissed\\nthe petition but allowed the petitioner to raise these contentions during the final hearing\\nof the case.\\nIn Rajendra Agrawal v.  State of Chhattisgar h28 the petitioner and co-accused,\\nboth, were char ge-sheeted for the aforesaid of fences under sections 500 read with\\nsection 120B of the IPC and 67 of the IT  Act. The court found that no of fense under', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 7}),\n",
       " '591a0e47-00c2-4e6a-bb2f-0a88e1ce0f88': Document(page_content='section 67 of the IT  Act was made out against the petitioner based on the contents of\\nthe FIR as it was neither found obscene nor lascivious. As a result, the char ge under\\nsection 67 of the IT  Act was quashed.\\nAdditionally , the certificate under section 65-B(4) of the Evidence Act was\\nmandatory to be filed with the char ge-sheet, which was not done in this case. The\\ncourt reiterated that the requirement of producing a certificate under section 65-B of\\nthe Evidence Act is mandatory in cases where secondary evidence is presented and\\noral evidence in the place of such certificate cannot possibly suf fice as section 65-\\nB(4) is a mandatory requirement of the law as established in the Anvar P .V. case. It\\nquoted ratio of the Supreme Court from Arjun Panditrao Khotkar29 to clarify the\\nposition of law on admissibility of electronic evidence under section 65B: 30\\n61. We may reiterate, therefore, that the certificate required under', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 7}),\n",
       " '3c9d7cdd-69bc-46b0-bd66-c4bf2080b96f': Document(page_content='Section 65-B(4) is a condition precedent to the admissibility of evidence\\nby way of electronic record, as correctly held in Anvar P .V. (supra),\\nand incorrectly “clarified” in Shafhi Mohammed  (supra). Oral evidence\\n27 2021 SCC OnLine Ker 4035(Decided on Nov . 1, 2021).\\n28 2021 SCC OnLine Chh 903(Decided on Apr. 6, 2021).\\n29 Arjun Panditrao Khotkar  v. Kailash Kushanrao Gorantyal, supra  note 3 at para 22.\\n30 Ibid.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 7}),\n",
       " '571e11e6-78e3-430c-b2e1-4e688896f313': Document(page_content='Annual Survey of Indian Law 200 [2021\\nin the place of such certificate cannot possibly suf fice as Section 65-\\nB(4) is a mandatory requirement of the law . Indeed, the hallowed\\nprinciple in Taylor v . Taylor , which has been followed in a number of\\nthe judgments of this Court, can also be applied. Section 65-B(4) of\\nthe Evidence Act clearly states that secondary evidence is admissible\\nonly if led in the manner stated and not otherwise. To hold otherwise\\nwould render Section 65-B(4) otiose.”\\nIII OBSCENITY -SECTION 67,67A\\nScope of pr ovisions-Interpr eting ‘sexually explicit’  act/conduct under  section 67A\\nand B\\nIn Sanjay Zacharias v. Stephen Geor ge31 the petitioner filed a petition under\\nsection 482 of the Criminal Procedure Code, 1973 to quash the proceedings initiated\\nagainst them based on a complaint and FIR filed by S tephen Geor ge, a former MLA.\\nThe petitioner , who is the General Secretary of the Kerala Congress (M) and a former', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 8}),\n",
       " 'd284e283-aed7-4983-9287-7621e2af6269': Document(page_content='MLA, claims that the accused for ged electronic records containing sexually explicit\\nmaterials with the intent to defame prominent political figures. They ar gue that the\\nallegations are politically motivated and deny any association with the social media\\naccount in question. The petitioner ’s counsel disputes the application of section 67A\\nof the IT  Act, which is a non-bailable of fense, and ar gues that if any of fenses were\\ncommitted, they would fall under the bailable of fense of section 67. The petitioner\\nalso asserts that they are a victim of social bullying and harassment by political\\nopponents.\\nThe senior counsel for the petitioner ar gues that the decision in Majeesh K.\\nMathew  v. State of Kerala32 has little relevance to the current case as it involved\\ndifferent facts and emphasized that the facts of that case involved oblique utterances\\nmade against a woman through social media, whereas the current case does not involve\\nsimilar circumstances.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 8}),\n",
       " '6cefdcf5-3716-457a-bf19-beaf0be28535': Document(page_content='During the proceedings, the petitioner ’s counsel ar gued that the content in\\nquestion does not have a sexual tone and asserted that the petitioner is being falsely\\ninterpreted and harassed. However , the counsel for the first respondent ar gued that\\nthe of fense under section 67A  of the IT  Act is applicable, as the petitioner intended to\\ndefame a prominent political figure. The director general of prosecution also opposed\\nthe application, emphasizing the importance of understanding the content in its context.\\nThe court rejected the petitioner ’s counsel’ s interpretation, considering it distorted\\nand taken out of context to suit the petitioner ’s convenience.\\nThe court observed that the crucial point revolves around whether the petitioner\\nmade sexually explicit postings on social media, particularly a caricature and an image\\ndepicting a song. The court rejected the interpretation made by the petitioner ’s counsel,', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 8}),\n",
       " '49d81ac6-ea6b-4841-b1af-4e02787153eb': Document(page_content='stating that every word should be understood in its context. The court considered the\\ninterpretation presented by the petitioner ’s counsel as distorted and taken out of context\\n31 2021 SCC OnLine Ker 13947(Decided on Nov . 25, 2021).\\n32 2018 (4) KHC 253.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 8}),\n",
       " '5dd928de-e6b4-46ed-b617-3c05700c0cec': Document(page_content='Cyber Law Vol. LVII] 201\\nto suit the petitioner ’s convenience. The settled proposition of law , as established by\\nthe Supreme Court in Devidas Ramachandra T uljapurkar  v. State of Maharashtra,33\\nemphasizes that an objective assessment must be made to determine whether the matter\\nin question is obscene. The court must consider contemporary community standards\\nand eliminate subjective elements or personal preferences. In this case, the expression\\n“KER_380669_6.png” is ar gued to have a sexually explicit tone, resembling the\\nopening lines of a Malayalam film song, which suggests oral sex. Therefore, it is\\ncontended that Section 67A  of the IT  Act is applicable, and the ar gument that it has\\nno application cannot be accepted.\\nMoreover , the court highlights that the petitioner ’s previous application for\\nanticipatory bail was dismissed, and the same ar guments cannot be used to quash the\\nproceedings. Ultimately , the court finds no merit in the application and dismisses it,', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 9}),\n",
       " '344e307e-6a64-4267-9014-0baf5c75c6dd': Document(page_content='allowing the proceedings to continue.\\nIn Suvojit Chowdhur y v. State of Maharashtra34 (with Sherlin Chopra v. State\\nof Maharashtra)  the applicants/accused sought protection from arrest for of fenses\\nunder sections 292 of the Indian Penal Code, sections 67 and 67A of the Information\\nTechnology Act, 2000, and sections 3 and 4 of the Indecent Women Representation\\nAct, 1986 related to broadcasting and exhibiting indecent videos, audio files, and\\nmessages containing sexually explicit content through Over -The-T op (OTT) platforms\\non the internet for illegal financial gains. Raj Kundra, the Director of Arms Prime,\\nwas implicated by co-accused for instigating them to act in obscene films. The\\napplicants did not cooperate in providing details about the creation of vulgar videos.\\nStatements of co-accused and witnesses established their involvement in video\\ngraphing and publishing objectionable obscene material on both free and paid apps,', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 9}),\n",
       " 'c5a439aa-5d56-4a28-86c1-252f3315641a': Document(page_content='satisfying the elements of the alleged of fense, particularly under section 67A. The\\nrequest for pre-arrest bail was rejected, but an ad-interim order of protection was\\nextended for four weeks from the date of the order .\\nIn another case Pramod Anand Dhumal v. State of Maharashtra ,35 The applicant,\\nan editor of a local Marathi newspaper and a social activist, sought pre-arrest bail for\\noffenses under section 354-D of the Indian Penal Code (IPC) and section 67A  of the\\nInformation Technology Act (IT  Act). The complainant had received of fensive and\\nsexually explicit messages with images from the applicant’ s cell phone on her Facebook\\naccount. Despite expressing her disinterest, the applicant continued to send obscene\\nmessages along with a hyperlink containing lascivious material. The complainant\\nfiled a complaint, resulting in the registration of a case against the applicant under\\nsection 354-D of the IPC and section 67A  of the IT  Act.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 9}),\n",
       " '7247b6da-c024-4275-baa1-b269668927e7': Document(page_content='The court observed that the material sent by the applicant did not meet the\\ncriteria for “material containing sexually explicit acts” required by section 67A of the\\nIT Act. Instead, it fell under section 67, as it tended to excite lust but did not directly\\ndepict sexual activity in a detailed manner . Therefore, prima facie , the of fense may\\n33 (2015) 6 SCC 1.\\n34 2021 SCC On Line Bom 1 1930(Decided on Nov . 25, 2021).\\n35 2021 SCC OnLine Bom 34.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 9}),\n",
       " 'aaf864c0-a22e-47af-959a-d3f82065083f': Document(page_content='Annual Survey of Indian Law 202 [2021\\nattract section 67 and not section 67A  of the IT  Act. The court found the applicant\\nprima facie involved in the of fense of stalking under Section 354-D of the IPC, which\\nis bailable as a first of fense. Considering the evidence and the punishment prescribed\\nfor the of fense under section 67 of the IT  Act, the court granted pre-arrest bail to the\\napplicant, as custodial interrogation was not required for electronic evidence.\\n      A prayer was filed in Vijesh  v. State of Kerala36 to quash all proceedings\\nagainst the accused under sections 66(A) and 67(A) of the IT  Act, 2000, among others,\\nusing section 482 of the Criminal Procedure Code (Cr . PC). The second respondent,\\na lady , was the de facto  complainant in the case. According to the prosecution’ s case,\\nduring the inaugural function of a jewelry store, celebrities from television and cinema\\nwere invited, resulting in a crowd where people took photos with them on their mobile', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 10}),\n",
       " 'c929bee1-b5be-40d1-a814-48bf94a0dfc3': Document(page_content='phones. Subsequently , in January and March 2012, videos titled “Mallu Aunti\\nHarassed” and “Paravoor Peedanam” were uploaded on YouTube, allegedly containing\\nthe second respondent’ s photographs from the inauguration along with derogatory\\nremarks. The petitioner was accused of of fenses under sections 66(A) and 67(A) of\\nthe IT  Act.\\nHowever , it is important to note that section 66(A) of the IT  Act has been declared\\nunconstitutional by the Supreme Court in the case of Shreya Singhal  v. Union of\\nIndia.37 Therefore, the criminal proceedings related to the of fense under Section 66(A)\\nof the IT  Act cannot be sustained. The only remaining of fense alleged against the\\npetitioner is the one under section 67(A) of the IT  Act, which deals with punishment\\nfor publishing or transmitting sexually explicit material in electronic form.\\nAccording to section 67(A) of the IT  Act, the accused must have published or', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 10}),\n",
       " 'ac6c444f-be24-469b-900c-5e29dfa8daa1': Document(page_content='transmitted sexually explicit material in electronic form. The alleged publication of\\nthe respondent’ s photograph during the jewelry store’ s inaugural function, as admitted\\nby the respondent, does not qualify as sexually explicit material.\\nThe prosecution’ s argument that uploading photographs of the respondent with\\nsexually colored remarks like “Mallu Aunti Harassed” and “Paravoor Peedanam” etc\\nfulfills the requirements of Section 67(A) of the IT  Act is invalid. The use of sexually\\ncolored remarks does not amount to the publication or transmission of sexually explicit\\nmaterial. The term “sexually explicit” has a specific meaning and does not include\\nnews or informational material. Since the of fense under section 66(A) of the IT  Act\\nhas already been declared unconstitutional, the continuation of criminal proceedings\\nagainst the petitioner is an abuse of the court process and should be quashed under\\nsection 482 of the Cr PC.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 10}),\n",
       " 'db929bb4-20ce-49d9-8ae0-dc233f271a68': Document(page_content='The court concluded that the initiation and continuation of criminal proceedings\\nagainst the petitioner were an abuse of the court process. Exercising its inherent\\nextraordinary powers under section 482 of the Cr PC, the court ordered that the char ge\\nsheet against the petitioner/accused and all subsequent proceedings stemming from it\\nbe quashed and set aside.\\n36 2021 SCC OnLine Ker 854.\\n37 (2015) 2 KL T 1 (SC).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 10}),\n",
       " '37db0267-7b5c-46d8-8fc2-1e647c2c14c5': Document(page_content='Cyber Law Vol. LVII] 203\\nIn Imran Shabbir Gauri v. State of Maha rashtra38 the appellant, who was the\\nfather of the victim, had been convicted by the trial court for several of fences, including\\nthe possession of pornographic images of the victim on his mobile phone. Specifically ,\\nhe was found guilty under section 376(2)(i) and 506 of the Indian Penal Code (IPC),\\nas well as under section 4 of the Protection of Children from Sexual Of fences (POCSO)\\nAct, 2012. Additionally , he was convicted for the of fence punishable under section\\n67-B of the IT  Act, 2000, as he had obtained nude photographs of the victim on his\\nmobile handset on multiple occasions.\\nDuring the trial, the court took into consideration the evidence presented,\\nincluding the Forensic Science Laboratory (FSL) report that confirmed the presence\\nof pornographic images and video clips on the appellant’ s mobile phone. However ,\\nthe court expressed concerns about the evidentiary value of the FSL report, as it was', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 11}),\n",
       " '7a0bf4db-6c4d-4eee-a521-3ac9b57928c7': Document(page_content='unclear whether it constituted substantive evidence or merely corroborative evidence.\\nThe court emphasized that the testimony of the individual who witnessed the incident\\nor the victim herself would be crucial as substantive evidence, while the recorded\\nmaterial stored on the memory card could serve as corroborative evidence.\\nWhile acknowledging the presence of pornographic images on the appellant’ s\\nmobile phone, the high court hesitated to establish a direct connection between those\\nimages and the victim due to a lack of identification. Although the forensic analysis\\nconfirmed the existence of pornographic content to some extent, the court found it\\nchallenging to attribute those specific images to the victim. Nonetheless, the appellant\\nwas convicted under section 67-B of the Information Technology Act, which pertains\\nto the depiction of sexually explicit acts involving children in electronic form. Even', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 11}),\n",
       " '322963be-ab76-42ac-a566-d628a92c4592': Document(page_content='though there was no evidence of the appellant uploading or transmitting these images\\nto anyone else, the act of possessing or depicting children in an obscene, indecent, or\\nsexually explicit manner in electronic form is punishable under clause (b) of section\\n67-B. Therefore, the court upheld the conviction of the appellant under section 67-B\\nof the IT  Act, 2000.\\nSection 67 and liability of admin of WhatsApp gr oup\\nIn Kishor v. State of Maharashtra39 the Nagpur Bench of the High Court of\\nBombay recently examined the legal responsibility of a WhatsApp group administrator\\nin relation to objectionable content posted by group members. In this case, the applicant\\n(accused No. 2) filed an application to quash a char ge sheet and FIR filed against him\\nfor of fenses under sections 354-A(1)(iv), 509, and 107 of the IPC, 1860, and section\\n67 of the IT  Act, 2000.\\nThe allegations in the FIR stated that as an administrator of a WhatsApp group,', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 11}),\n",
       " '7b586aef-8018-415c-85d7-1f56c2efbf89': Document(page_content='the applicant allowed another member (accused No. 1) to use of fensive language\\nagainst a non-applicant (No. 2) in th e group. It was further alleged that despite being\\naware of accused No. 1’ s actions, the applicant took no action against them, such as\\n38 2021 SCC OnLine Bom 51 1(Decided on Mar . 31, 2021).\\n39 2021 SCC OnLine Bom 654(Decided on Mar . 1, 2021).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 11}),\n",
       " '87cd6170-7e86-4431-9f46-ed25231bb25b': Document(page_content='Annual Survey of Indian Law 204 [2021\\nremoving them from the group or asking for an apology . The FIR was lodged by the\\nnon-applicant against both the applicant and accused no. 1.\\nTo address the issue of potential criminal liability of a WhatsApp group\\nadministrator , the court first examined the operational dynamics of WhatsApp. It\\nacknowledged that WhatsApp is an instant messaging platform that allows mass\\ncommunication through chat groups. The group administrator has the authority to\\nadd or remove members but does not possess the power to regulate or censor content\\nbefore it is posted. The court emphasized that individual members can be held liable\\nfor their own posts if they violate the law . Without specific provisions establishing\\nvicarious liability , an administrator cannot be held responsible for objectionable content\\nposted by group members. The court stated that establishing vicarious liability would\\nrequire demonstrating a common intention or pre-arranged plan between the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 12}),\n",
       " 'e984c531-25ff-4a2c-a748-c64342263fb7': Document(page_content='administrator and the group member involved. Merely being a group administrator\\ndoes not establish common intention, and it is unreasonable to expect administrators\\nto anticipate or have prior knowledge of the criminal actions of group members.\\nAdditionally , the liability of an administrator as a creator of objectionable content\\ndoes not apply in this case.\\nRegarding the of fense under section 67 of the IT  Act, the court analyzed the\\nspecific language of the section, which punishes the transmission or publication of\\nobscene material in electronic form. The allegations and evidence presented did not\\nsupport the claim that the applicant disseminated or caused the dissemination of any\\nlascivious or obscene material. Section 67 prescribes that an individual may be\\nsubjected to punishment for transmitting, publishing, or causing to be transmitted or\\npublished, any material that is obscene in electronic form. One could discern on a', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 12}),\n",
       " 'f8986a91-4ee0-4003-961d-d515548fe3a2': Document(page_content='careful analysis of the allegations in the FIR and the evidence presented in the form\\nof a char ge sheet that there is no claim or evidence that the applicant disseminated or\\ncaused to be disseminated any material in electronic form that is lascivious, appeals\\nto prurient interests, or is likely to corrupt or deprave individuals who may view , read,\\nor hear it. The definition of an intermediary also did not apply to the applicant, as\\nthere was no accusation of involvement in transmitting or receiving any record or\\nproviding related services.\\nAfter reviewing the material in the char ge sheet, it was clear that the essential\\nelements of the alleged of fenses were not disclosed. Continuing with the proceedings\\nagainst the applicant would amount to an abuse of the court process. As a result, the\\ncourt quashed the FIR, char ge sheet, and all proceedings against the applicant for\\noffenses under sections 354-A  (1)(iv), 509, and 107 of the IPC and section 67 of the\\nIT Act.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 12}),\n",
       " 'ccd4b9d7-616b-4c33-98c3-610c8c58d278': Document(page_content='IT Act.\\nThe petitioner in Rajendra Agrawal  v. State of Chhattisgar h40 case sought the\\nquashing of char ges against them under section 67 of the Information Technology\\n(IT) Act and section 500 of the IPC through section 482 of the Cr PC. Their ar gument\\nwas that the WhatsApp messages allegedly sent by their co-accused on their behalf\\n40 2021 SCC OnLine Chh 903(Decided on Arp. 6, 2021).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 12}),\n",
       " '34042e2b-f06b-407e-b84b-c15915b36d76': Document(page_content='Cyber Law Vol. LVII] 205\\nwere not of an obscene or lascivious nature, and therefore, no of fence under section\\n67 of the IT  Act was established against them.\\nThe court carefully examined the provisions of section 67 of the IT  Act and\\ncompared them with sections 294 and 292(1) of the IPC, which deal with the concept\\nof obscenity . It noted that while there are similarities between these provisions, there\\nare also distinct dif ferences in their language and requirements. Specifically , in order\\nfor an act to fall within the scope of section 67 of the IT  Act, it must have the potential\\nto deprave and corrupt individuals who are likely , considering all relevant\\ncircumstances, to read, see, or hear the content in question.\\nTherefore, the court highlighted that the mere publication, transmission, or\\ncausing of publication or transmission in electronic form is not suf ficient to bring an\\nact within the purview of section 67 of the IT  Act. The content must possess a quality', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 13}),\n",
       " 'cf35c01f-1997-46fb-84a8-5f6042543afd': Document(page_content='that tends to deprave and corrupt individuals who are likely to come across it. This\\ndistinction is important in determining whether the alleged WhatsApp messages can\\nbe considered an of fence under section 67 of the IT  Act.\\nIn essence, the court emphasized that the content in question should have a\\npotentially harmful impact on the moral and ethical standards of the readers, viewers,\\nor listeners, taking into account all relevant factors. The petitioner ’s argument relied\\non the assertion that the WhatsApp messages did not meet this standard of obscenity\\nor lasciviousness required by section 67 of the IT  Act. Setting aside the FIR and the\\nconsequent criminal proceedings initiated against the petitioner , the court held: 41\\nThe words in the said WhatsApp message are not capable of arousing\\nsexual thoughts or feelings in the minds of the petitioner or respondent\\nNo. 2 or other four persons to whom the message has been sent by the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 13}),\n",
       " '9a0f55c2-9694-446c-a908-1ca15e350189': Document(page_content='co-accused and it does not involve lascivious elements arousing sexual\\nthoughts or feelings or the words in the said message have no ef fect of\\ndepraving persons, and defiling morals by sex appeal or lustful desires,\\nthough the words may be extremely un parliamentary , unprintable and\\nabusive in nature, but it cannot be brought within the broad contours\\nof the penal provisions as contained in Sections 294 & 292 of the IPC\\ncorresponding to Section 67 of the IT  Act. Even according to the\\ncomplainant, it is only defamatory and as such, the ingredients of\\noffence under Section 67 of the IT  Act are not at all attracted.\\nViewing child pornography privately - an of fence?\\nIn P.G. Sam Infant Jones v. State42  the prosecution alleged that the petitioner\\naccessed, a M.E degree holder and pursuing Ph.D. at that time, downloaded, and\\nshared child pornographic material using an Airtel SIM card and his email and\\nFacebook accounts.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 13}),\n",
       " '78bbe35c-bc55-48a1-ae47-5bae3a0dabbf': Document(page_content='Facebook accounts.\\nThe petitioner ’s counsel ar gued that the petitioner was present in the hostel\\nduring the relevant time and that the evidence provided thus far was insuf ficient to\\n41 Ibid.\\n42 2021 SCC OnLine Mad 2241(decided on June 1 1, 2021).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 13}),\n",
       " 'dff28da6-909e-424f-abce-565f380a928c': Document(page_content='Annual Survey of Indian Law 206 [2021\\nprove that the petitioner personally committed the alleged acts. Furthermore, there is\\nno evidence indicating that the content in question involved child pornography . The\\nact of viewing pornography in private does not typically constitute an of fense, as\\nthere is no specific provision in place that prohibits such private acts, the counsel for\\naccused ar gued.\\nWhile there are ar guments suggesting that child pornography falls under an\\nindividual’ s freedom of expression and privacy , it is important to highlight that child\\npornography is an exception to this principle. The high court observed that section\\n67-B of the Information Technology Act, 2000 deals with child pornography and\\nimposes penalties for various acts related to it. The provision covers publishing,\\ntransmitting, creating, collecting, seeking, browsing, downloading, advertising,\\npromoting, exchanging, or distributing material in any electronic form that depicts', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 14}),\n",
       " '29702b38-5c20-45cf-93bd-70249043fafc': Document(page_content='children engaged in sexually explicit acts. It also includes activities such as cultivating,\\nenticing, or inducing children into online relationships for sexually explicit acts,\\nfacilitating online abuse of children, and recording one’ s own abuse or the abuse of\\nothers involving sexually explicit acts with children. Consequently , viewing child\\npornography is considered an of fense and is punishable under the law .\\nAfter considering the circumstances of the case, the court found that the incident\\noccurred nearly a year ago and it seems to be an isolated incident. Even the prosecution\\ndid not allege that the possession or transmission of the material was for commercial\\npurposes. The court made a distinction between individuals who consume child\\npornography on a one-time basis and those who actively transmit, distribute, or show\\nsuch material in the digital realm. It emphasized the seriousness of the issue and the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 14}),\n",
       " 'e9f92c91-675a-473c-8cd1-65c0bcca83be': Document(page_content='need for a strong approach to combat child pornography . The court acknowledged\\nthat once someone enters the digital space, their activities can be monitored by either\\nthe government or the operators of social networking sites. It further highlighted that:43\\nIt is obvious that the moment one steps into digital space, one comes\\nunder the surveillance either of the S tate or those manning the social\\nnetworking sites. If one is zealous about privacy , the only option is to\\nstay outside such networks. Of course, in the current world, it is not a\\nviable option.\\nCourt mandated  both the central and state governments to raise awareness about\\nthe provisions of the POCSO Act under section 43, however , it was acknowledged\\nthat this alone may not be enough and emphasized that moral education is considered\\nto be the only ef fective solution to address this issue: 44\\n11…..It is only the Bharatiya culture that can act as a bulwark. The', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 14}),\n",
       " '0634dc68-170d-4c1f-9f4b-2446495db136': Document(page_content='menace of child pornography can be tackled only if all of us inculcate\\nthe right values.\\n43 Ibid.\\n44 Ibid.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 14}),\n",
       " '81bfd030-870b-4231-9367-f47dfd40b40e': Document(page_content='Cyber Law Vol. LVII] 207\\nIV RIGHT  TO BE FORGOTTEN\\nS.K. Kaul  J.,in  K.S. Puttaswamy    v. UOI observed:\\nRight of an individual to exercise control over his personal data and to\\nbe able to control his/her own life would also encompass his right to\\ncontrol his existence on the Internet.\\nThe introduction of GDPR in the European Union triggered a discussion on\\nprivacy concerns in India and led lawmakers to consider the need for a data protection\\nframework. However , India presently lacks such a framework.  While some courts\\nhave recognized it as part of the right to privacy , others have rejected pleas for removal\\nof personal information due to the lack of legislative sanction. The Information\\nTechnology Act, 2000, which regulates the cyber world in India, does not mention the\\nright to be for gotten. The Supreme Court’ s landmark ruling in the case of K.S.\\nPuttaswamy45 however , established that the right to privacy includes the right to be', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 15}),\n",
       " 'b1615e92-04e5-4d42-bdb3-724c311bab3b': Document(page_content='left alone, which is an essential aspect of an individual’ s privacy . Also the Indian\\nPersonal Data Protection Bill, 2019, does mention the right to erasure.\\nThe right to be for gotten is evolving in India and struggling to be considered a\\nfundamental right, but with increasing concerns about data privacy and exploitation,\\nit is a relief that can be claimed against illegal or unwanted sharing of personal\\ninformation only . It is important to legally recognize the right to be for gotten as a core\\npart of the right to privacy and a fundamental right.\\nIn Karthick Theodr e v. The Registrar General46 the petitioner was char ged with\\na criminal of fence under sections 417 and 376 of the IPC and the trial court found\\nhim guilty and punished him. The petitioner was acquitted of all char ges but his name\\nstill appears in the judgment as an accused. He approached the High Court of Madras\\nwith the request that his name be removed from the judgment as it harms his reputation,', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 15}),\n",
       " '4cc813b6-b4b0-4deb-92a4-07eb9e762592': Document(page_content='despite being acquitted.\\nThe high court agreed that an accused who has been exonerated of all char ges\\nhas the right to have their name redacted from records to safeguard their right to\\nprivacy . However , the court stated that the “right to be for gotten” cannot exist in the\\nadministration of justice and giving such a broad directive would open the floodgates\\nof demands. India does not have a system to erase an accused person’ s records once\\nthey have been acquitted, and only “The Juvenile Justice [Care and Protection of\\nChildren] Act, 2015” allows for such erasure. It was observed that:47\\n31......This Court honestly feels that our criminal justice system is yet\\nto reach such standards where courts can venture to pass orders for\\nredaction of name of an accused person on certain objective criteria\\nprescribed by rules or regulations. It will be more appropriate to await\\nthe enactment of the Data Protection Act and Rules thereunder , which', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 15}),\n",
       " '86140d49-5652-4792-b46c-85151f1205b6': Document(page_content='may provide an objective criterion while dealing with the plea of\\n45 (2017) 10 SCC 1.\\n46 2021 SCC OnLine Mad. 2755\\n47 Ibid.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 15}),\n",
       " 'cea229fd-fc23-46ef-b5c5-7a082d13dc63': Document(page_content='Annual Survey of Indian Law 208 [2021\\nredaction of names of accused p ersons who are acquitted from criminal\\nproceedings. If such uniform standards are not followed across the\\ncountry , the constitutional courts will be riding an unruly horse which\\nwill prove to be counterproductive to the existing system.\\nThe high court decided that it cannot issue a broad order to redact names from\\ncourt records without appropriate statutory backing. The court felt that a proper policy\\nmust be established to prevent confusion when carrying out such an exercise. The\\ncourt concluded that without clear guidelines, such a broad order could lead to many\\ncomplications and that the government must create a statutory framework for such a\\npolicy .\\nIn Jorawer Singh Mundy v. Union of India48 the petitioner , an American citizen\\nof Indian origin with a background in real estate, filed a petition to remove a judgment\\nCustom  v. Jorawar Singh Mundy  49 from online platforms like Google, Indian Kanoon,\\nand Vlex.in.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 16}),\n",
       " '713e2e24-4e34-46c4-87ff-e3464e1a5c5a': Document(page_content='and Vlex.in.\\nThe petitioner , Jorawer Singh, an American citizen of Indian origin was char ged\\nunder the Narcotics Drugs and Psychotropic Substances Act (NDPS Act) in India\\nduring a visit in 2009. He was later acquitted of all char ges by the trial court and the\\nHigh Court of Delhi. However , he faced dif ficulty finding employment due to the\\nonline availability of the judgment regarding his involvement in the drug case on\\nplatforms such as Google, Indian Kanoon, and vLex.in. He filed a writ petition under\\narticle 226 of the Indian Constitution before the High Court of Delhi, requesting the\\nplatforms to take down the judgment as it violated his right to privacy under article 21\\nof the Constitution. The petitioner issued legal notices to the aforementioned platforms.\\nVlex.in claimed to have removed the judgment, but it remained available on other\\nplatforms.\\nThe central question in this case was\\ni.whether the right to privacy under article 21 of the Indian Constitution includes', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 16}),\n",
       " 'a4ebe887-95f7-4799-ba64-44ebc848fe74': Document(page_content='the right to be for gotten, and\\nii.whether a court has the authority to order the removal of information from\\nonline platforms?\\nThe High Court of Delhi had to balance the right to privacy against the right to\\ninformation available to the public and maintenance of transparency in judicial records.\\nWhile the right to privacy is recognized as a fundamental right, the right to be for gotten\\nis not explicitly mentioned in the Indian Constitution. However , in some cases,50 courts\\nhave recognized the right to be for gotten as a part of the right to privacy . It cited\\nZulfiqar Ahman Khan  v. Quintillion Businessman Media Pvt. Ltd.,51 where this court\\nhad held as un der: 52\\n48 2021 SCC OnLine Del 2306.\\n49 Crl.A. No. 14/2013.\\n50 Karthick Theodr e v. Registrar General\\n51 2021 SCC OnLine Mad. 2755 , and Subhranshu Rout v . State of  Odisha  2020 SCC OnLIne\\nOri. 878\\n52 2019 SCC OnLine Del. 8494.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 16}),\n",
       " 'd3d45bc3-9317-40c0-93f6-db17fe1aea7a': Document(page_content='Cyber Law Vol. LVII] 209\\n8. In fact, it is the submission of ld. Counsel for the Plaintif f that the\\nPlaintif f’s personal and professional life has been hampered irreparably\\nand further damage is likely to be caused if appropriate relief is not\\ngranted against the republication of these two articles. The original\\npublisher having already agreed to pull down the same, this Court\\nhaving directed that the same ought not to be republished, the Plaintif f,\\nthus, has a right to ensure that the articles are not published on multiple\\nelectronic/digital platforms as that would create a permanent\\natmosphere of suspicion and animosity towards the Plaintif f and also\\nseverely prejudice his personal and professional life. The printouts of\\nthe articles from www .newsdogapp.com, which have been shown to\\nthe Court, leave no doubt in the mind of the Court that these are identical\\nto the articles published on www .thequint.com, which has already been\\npulled down.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 17}),\n",
       " 'ec5007dd-129f-4a44-bd98-260ba50e4bdd': Document(page_content='pulled down.\\n9. Accordingly , ecognizing the Plaintif f’s Right to privacy , of which\\nthe ‘Right to be for gotten’  and the ‘Right to be left alone’  are inherent\\naspects, it is directed that any republication of the content of the\\noriginally impugned articles dated 12 October 2018 and 31 October\\n2018, or any extracts/or excerpts thereof, as also modified versions\\nthereof, on any print or digital/electronic platform shall stand restrained\\nduring the pendency of the present suit.\\n10. The Plaintif f is permitted to communicate this order to any print or\\nelectronic platform including various search engines in order to ensure\\nthat the articles or any excerpts/search results thereof are not republished\\nin any manner whatsoever . The Plaintif f is permitted to approach the\\ngrievance of ficers of the electronic platforms and portals to ensure\\nimmediate compliance of this order .\\nThe court opined that the petitioner may face irreversible harm to his social life', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 17}),\n",
       " '451011dd-dacb-4d24-b440-381ca5318a37': Document(page_content='and career prospects, despite being acquitted in a case. The court held that the petitioner\\nis entitled to interim protection while the legal issues are pending and accordingly\\ndirected Google and Google LLC to remove the judgment titled Custom  v. Jorawar\\nSingh Mundy53 from their search results and India Kanoon to block access to the said\\njudgment through search engines.\\nIn X  v. YouTube,54 the plaintif f, a popular Bengali film actor , was promised the\\nlead role in a web series by Ram Gopal Verma S tudios. She participated in a\\ndemonstration video which included explicit scenes of complete nudity , but the project\\nwas later shelved. However , the producer uploaded the video to his YouTube channel\\nand website, and although he removed it upon the plaintif f’s request, others uploaded\\nit to dif ferent websites without her consent. The plaintif f applied to the court seeking\\ninterim protection and a takedown of the video due to the violation of her privacy ,', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 17}),\n",
       " 'be282efa-40e0-42ed-baf5-72e6abe505f9': Document(page_content='53 Id. Cited at para 9 in Jorawer Singh Mundy v . Union of India\\n54  Crl.A. No. 14/2013.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 17}),\n",
       " 'bac718cc-bc0e-4717-8b57-4b8377c112f5': Document(page_content='Annual Survey of Indian Law 210 [2021\\ndamage to her reputation, and harassment she faced as a result. The defendants included\\nwebsites, internet service providers, and search engines.\\nThe plaintif f argued that the right to privacy includes the right to be for gotten,\\nwhich has been recognized by the Indian Supreme Court and several high courts. She\\nalso cited Rule 3(2)(b) of the Information Technology (Intermediary Guidelines and\\nDigital Media Ethics Code) Rules, 2021, which requires intermediaries to remove or\\ndisable access to content that exposes an individual’ s private areas within 24 hours of\\nreceiving a complaint. The defendants, including websites, ISPs, and search engines,\\nwere thus obliged to take measures to remove the suit videos. The plaintif f also cited\\na High Court of Delhi decision where Google was directed to remove URLs/websites\\nunder an interim order . The plaintif f argued that the three-part test for an interim', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 18}),\n",
       " '4b05454f-ba8c-4397-885b-c5854d74304e': Document(page_content='injunction was satisfied, and the court should issue such an order against the defendants.\\nDefendants Google relying on Karthick Theodr e v. Registrar General,55 and\\nSubhranshu Rout  v. State of Odisha56 argued that they were not under any obligation\\nto prevent the republication of the Suit Videos since they were unaware of any\\nagreement permitting the broadcast. They also ar gued that the plaintif f had no valid\\nstatutory protection to enforce the right to be for gotten and that the plaintif f should\\nhave approached the publishing platforms instead of the search engine defendant.\\nThey relied on case law showing that courts had rejected the disabling of search\\nresults in the manner sought by the plaintif f. Lastly , they ar gued that the plaintif f had\\nconsented to the filming of the videos, and Rule 3(2)(b) of the Rules 2021 required\\nthe victim or an authorised representative to complain to the intermediary , which was', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 18}),\n",
       " 'db8afe93-daee-406e-8681-6100d06943c5': Document(page_content='not satisfied in the present case. They further submitted that Rule 3(3)(b) should be\\nread alongside Sections 67 and 67A  of the Information Technology Act, 2000, which\\nexcluded material published in the interest of science, literature, art or learning or\\nother objects of general concern.\\nThe Court found that the explicit nature of the Suit Videos fell under Rule\\n(3)(2)(b) of the Rules 2021, and rejected the defendants’  argument that the plaintif f’s\\nconsent to filming barred her from legal recourse. The Court drew parallels between\\nthis case and Zulfiqar Ahman Khan  v. Quintillion Business Media (P) Ltd .,57 which\\nillustrated the severe impact of publication on personal and professional life. The\\ncourt found that the plaintif f’s right to privacy should be protected, given the explicit\\nnature of the videos and the impact on her reputation. While neighbouring high courts\\nhad found no statutory right to be for gotten, the court endorsed the right to be for gotten', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 18}),\n",
       " '6c009f45-faed-4822-9595-75379fa0e2cc': Document(page_content='and the right to be left alone as inherent aspects of the right to privacy .\\nCourt granted interim relief to the plaintif f, finding that the suit videos were of\\nan explicit nature and that their circulation had a clear and immediate impact on the\\nplaintif f’s reputation. The court rejected the defendants’  arguments that the plaintif f\\nhad consented to the filming of the videos and that she had no valid statutory protection\\n55 2021 SCC OnLine Del 4193(Decided on Aug. 23, 2021).\\n56 2021 SCC OnLine Mad 2755.\\n57 (2020) SCC Online Ori 878).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 18}),\n",
       " '5cfc89ef-580c-4859-abf2-d6b457d29bd6': Document(page_content='Cyber Law Vol. LVII] 211\\nto enforce her right to be for gotten. The court found her consent to have since been\\nexpressly withdrawn, as the producer of the series had also removed the videos upon\\nher request and held that the plaintif f’s right to privacy should be protected. It therefore\\npassed an interim order directing the defendants to take down all the suit videos from\\ntheir websites, channels, digital platforms, and search engines and to stop uploading,\\npublishing, streaming, transmitting, broadcasting, or communicating the videos to\\nthe public. The defendants were given 36 hours to comply with the order , and the\\nplaintif f was given the right to communicate the order to any other platforms found to\\nbe publishing, streaming, or transmitting the suit videos.\\nV INTERMEDIAR Y LIABILITY -SECTION 79\\nThe issue of intermediary liability has been a subject of uncertainty since the\\nintroduction of the Information Technology Act in 2000. In recent years, intermediaries', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 19}),\n",
       " 'faec483a-a48a-40e0-b1ce-2329878a30a2': Document(page_content='have become increasingly significant due to the widespread use of social media\\nplatforms for communication and information sharing. The emer gence of digital media\\nhas also made it a mainstream concern, leading the government to focus on regulating\\nthese platforms. The 2021 Rules represent an initial ef fort to regulate such platforms.\\nThe 2021 IT Rules\\nIn February 2021, the Indian government introduced new regulations for social\\nmedia platforms, digital news media, and other online content providers called the\\nInformation Technology (Intermediary Guidelines and Digital Media Ethics Code)\\nRules, 2021 (IT  Rules 2021). These rules aim to hold intermediaries more accountable\\nto both internet users and the Indian government. They require social media platforms\\nand messaging apps to appoint Indian residents as grievance of ficers, compliance\\nofficers, and nodal of ficers and to remove content within 36 hours of receiving a', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 19}),\n",
       " 'c840c245-dfed-4dbd-9029-7a198c2abdc5': Document(page_content='legal order or court directive and maintaining records of all removed content for a\\nminimum of 180 days.\\nThere are also specific rules for publishers of news and current af fairs content\\nand online curated content. Significant Social Media Intermediaries (SSMI) in\\ncomparison to Social Media Intermediaries (SMI) need to observe additional due\\ndiligence requirements and comply with stringent residency requirements for\\ncompliance of ficers and nodal contact persons.  The rules empower the government\\nto direct intermediaries and publishers to delete, modify , or block content, either\\nthrough a grievance procedure or through emer gency blocking orders passed without\\na hearing.\\nHowever , the rules have faced criticism from human rights groups and digital\\nrights advocates who ar gue that they could be used to suppress free speech and\\nexpression. The rules have also been challenged in petitions filed in dif ferent high\\ncourts including the High Court of Bombay , Kerala, Delhi, and Madras.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 19}),\n",
       " '479c2648-f326-4863-9722-78c1cb1d7782': Document(page_content='Rule 4(2) of the IT Rules 2021 requires SSMI providing messaging services to\\nenable the identification of the first originator of information on their computer . This\\nprovision has been challenged by companies like WhatsApp on the grounds that it\\ninfringes upon users’ fundamental right to privacy and freedom of speech. Rule 4(4),\\nwhich requires the use of technology-based measures to proactively identify certain', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 19}),\n",
       " '35f662d2-02b3-4e67-a030-c119e2d1db39': Document(page_content='Annual Survey of Indian Law 212 [2021\\ntypes of content, raises concerns about the accuracy and consequences of fully\\nautomated tools.\\nIn the case of Live Law Media Pvt. Ltd. v. Union of India,58 the High Court of\\nKerala has passed an interim order directing that no coercive action be taken against\\nLive Law , under Part III of the IT  Rules 2021 (dealing with digital media), as Live\\nLaw is a publisher of law reports and legal literature.\\nIn Foundation for Independent Journalism  v. Union of India59 and Sanjay Kumar\\nSingh v. Union of India,60 High Court of Delhi directed Central Government to file a\\nreply . In another petition before High Court of Kerala titled Praveen Arimbrathodiyil\\nv. Union of India61 where a free and open source software (FOSS) developer who\\nfiled a petition against India’ s IT Rules 2021 has claimed that the regulations unfairly\\nburden small-scale FOSS developers and communities. The petitioner has ar gued', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 20}),\n",
       " '55b5e5b3-0281-418e-9fd9-db4c7d447209': Document(page_content='that the rules’ content moderation requirements could weaken data security and privacy\\nmeasures, ultimately infringing on the right to freedom of trade and profession under\\narticle 19(1)(g) of India’ s constitution. The government filed a transfer petition under\\narticle 139A(1) of the Constitution, seeking a transfer of the four petitions mentioned\\nabove, on the ground that they are substantially similar to justice for rights foundation,\\ninitiated long before the government notified IT Rules, 2021.62\\nTwo petitions Agij Pr omotion of Nineteenonea Media Pvt. Ltd. v. Union of\\nIndia63  and Nikhil Mangesh W agle v. Union of India64 were filed in High Court of\\nBombay to challenge the IT  Rules 2021 on the ground that they are ultra vir es the\\nInformation Technology Act, 2000( ‘IT  Act’) and the provisions of articles 14, 19(1)(a)\\nand 19(1)(g) of the Constitution and go beyond the restrictive ambit of section 69A of', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 20}),\n",
       " '6e2ac6d4-7bdf-4c22-b423-bb50b10c326c': Document(page_content='the IT  Act. 2021 Rules have a terrible chilling ef fect in their applicability to the internet\\nas they bring about a manifestly unreasonable and an arbitrary regime amounting to\\nan af front to the constitutional guarantee of right of citizens to exercise freedom of\\nfree speech and expression. The Central Government justified the Rules by stating\\nthat they aimed to create a level playing field between online and of fline publishers\\nand combat fake news.\\nThe High Court of Bombay granted an interim stay on Rules 9(1) and 9(3) of\\nthe IT  Rules 2021, stating that they are ultra vir es the IT  Act. Bench stated that Rule\\n9 of the IT Rules, 2021 appeared to infringe upon the constitutional guarantee of\\nfreedom of speech and expression, as enshrined in article 19(1)(a). This infringement\\nwas evident in the fact that the rule subjected publishers of news and current af fairs\\ncontent and online curated content to action under the Press Council Act, 1978 and', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 20}),\n",
       " 'ffef4f89-7f6e-4189-a3f3-691a48c66d5b': Document(page_content='the Cable TV Networks Regulation Act, 1995 which already had their own independent\\n58 2019 SCC Online Del 8494.\\n59 W.P.(C) 6272 of 2021.\\n60 W.P.(C) 3125 / 2021.\\n61 W.P.(C) 3483 of 2021.\\n62 Praveen Arimbrathodiyil v. Union of India  WP (C) 18084/2021).\\n63 Union of India v. Sudesh Kumar Singh , Transfer Petition (C) No. 100-105/2021.\\n64 2021 SCC OnLine Bom 2938.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 20}),\n",
       " 'decaf848-3acf-4ce6-bed3-dca27b077cba': Document(page_content='Cyber Law Vol. LVII] 213\\nmechanisms for dealing with violations, and a subordinate legislation like Rule 9\\ncould not disrupt or override the powers granted by those laws.65\\n The court also noted that Rule 14, which deals with the formation of an inter -\\ndepartmental committee, and Rule 16, which deals with blocking information during\\nemer gencies, do not require immediate action. However , Rule 9 imposes an obligation\\non publishers to adhere to a Code of Ethics that is not part of the IT  Act and may\\npreclude them from criticizing public figures, which the court found problematic as it\\ngoes beyond the powers laid down in section 69A  of the IT  Act:66\\n28. “Dissent in democracy is vital. It is, however , the checks and\\nbalances that make a democracy work. There can be no two opinions\\nthat a healthy democracy is one which has developed on criticism and\\nacceptance of contra views. Opinion based on criticism reinforces its', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 21}),\n",
       " '6e558c19-4820-4cbb-a20d-17524a751ad6': Document(page_content='acceptance in a democratic society . For proper administration of the\\nState, it is healthy to invite criticism of all those who are in public\\nservice for the nation to have a structured growth but with the 2021\\nRules in place, one would have to think twice before criticizing any\\nsuch personality , even if the writer/editor/publisher may have good\\nreasons to do so without resorting to defamation and without inviting\\naction under any other provision of law . Allowing the operation of the\\n2021 Rules in its form and substance to operate would result in the\\nwriter/editor/publisher standing the risk of being punished and\\nsanctioned, should the inter -departmental committee be not in favour\\nof criticism of any public figure. It is, therefore, quite possible that the\\nwriter/editor/publisher on contravention of the provisions of clause\\n(1) of Rule 9 of 2021 Rules, but without even transgressing the\\nboundaries set by clause (2) of Article 19 of the Constitution, may', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 21}),\n",
       " '6c035d48-243b-4839-b855-1f52a46665ef': Document(page_content='expose himself/itself to punishment/sanction under the 2021 Rules.\\nThe indeterminate and wide terms of the Rules bring about a chilling\\neffect qua the ight of freedom of speech and expression of writers/\\neditors/publishers because they can be hauled up for anything if such\\ncommittee so wishes. The 2021 Rules are, thus, manifestly unreasonable\\nand go beyond the IT  Act, its aims and provisions.\\n29. A democracy would thrive only if the people of India regulate their\\nconduct in accordance with the preambular promise that they took while\\ngiving to themselves the Constitution. Liberty of thought is one of\\nsuch promises. Exercising this liberty , expressions take shape. Should\\nat least a part of Rule 9 of the 2021 Rules be not interdicted even at the\\ninterim stage, it would generate a pernicious ef fect. As it is, the constant\\nfear of being hauled up for contravention of the Code of Ethics is a\\ndistinct possibility now . People would be starved of the liberty of', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 21}),\n",
       " '9e6261c5-45d7-4567-8430-8f917a82407e': Document(page_content='thought and feel suf focated to exercise their right of freedom of speech\\n65 Public Interest Litigation (L) No. 14204 of 2021.\\n66 Agij Pr omotion of Nineteenonea Media Pvt. Ltd. v . Union of India, supra note 63  at para 31.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 21}),\n",
       " '5719420b-1868-49ae-ab77-7182a8160446': Document(page_content='Annual Survey of Indian Law 214 [2021\\nand expression, if they are made to live in present times of content\\nregulation on the internet with the Code of Ethics hanging over their\\nhead as the Sword of Damocles. This regime would run clearly contrary\\nto the well-recognized Constitutional ethos and principles.”67\\nThe High Court of Bombay therefore, stayed the operation of sub-rules (1) and\\n(3) of Rule 9. However , the court did not stay Rule 7 of the 2001 Rules as the petitioner\\nhad not demonstrated that they were an intermediary as defined under section 2(w) of\\nthe IT  Act. The court emphasized that Rule 9 was an exception to the general\\npresumption of subordinate legislation’ s constitutionality and did not comply with\\nthe IT  Act’s provisions or the constitutional rights guaranteed under article 19(1)(a).\\nFinally , the court emphasized that subordinate legislation could not transgress the\\npowers occupied by dif ferent statutes.68', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 22}),\n",
       " '9c3b00d8-35ad-428e-a8ff-d79bf3941552': Document(page_content='In yet another petition titled Digital News Publishers Assn. v. Union of India69\\nbefore High Court of Madras filed by TM Krishna and Digital News Publishers\\nAssociation (DNP A) against the IT  Rules 2021, which is similar to the High Court of\\nBombay case.70 The court noted that the sub-rules (1) and (3) of Rule 9 of the IT\\nRules 2021 have already been stayed by the High Court of Bombay , and this stay\\norder should have a pan-India ef fect, so there was no need for an independent order .\\nHowever , the petitioners ar gued that they had received notices requiring them to comply\\nwith the IT  Rules and Rule 9. The digital news platforms expressed concern over the\\nthree-tier grievance redressal mechanism, which gives excessive power to government\\nofficials to punish them. This mechanism involves self-regulation by publishers in\\nthe first level, self-regulating bodies established by publishers in the second level,', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 22}),\n",
       " 'fe9354ec-3b11-49b7-877a-8adf788996bc': Document(page_content='and oversight by the Central Government in the third level. The petitioners were\\nspecifically concerned about sub-clause (x) of Rule 3(1)(b) which states that:71\\n“(x) is patently false and untrue, and is written or published in any\\nform, with the intent to mislead or harass a person, entity or agency for\\nfinancial gain or to cause any injury to any person;”\\nThe petitioners ar gued that this provision, along with the requirement\\nfor intermediaries to terminate access or usage rights for non-\\ncompliance72, and the strict grievance redressal mechanism, creates\\nundue pressure on intermediaries. Additionally , Rule 7 makes\\nintermediaries liable for punishment if they fail to comply with the\\naforementioned rules.\\nSection 79 of the Information Technology Act provides protection to\\nintermediaries from liability in certain cases. However , this exemption would not\\napply if the intermediary does not observe the guidelines prescribed by the Central\\n67 Ibid. at para 28\\n68 Ibid.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 22}),\n",
       " '66e3940f-88a9-410b-aabf-70a786eab4b2': Document(page_content='68 Ibid.\\n69 Id . at para 31.\\n70 2021 SCC OnLine Mad 16337(Decided on Aug. 14, 2021).\\n71 Agij Pr omotion of Nineteenonea Media Pvt. Ltd. v. Union of India, supra note 63.\\n72 R. 3(1)(b).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 22}),\n",
       " 'fac7f9da-3c09-41a6-96aa-11738f85c4df': Document(page_content='Cyber Law Vol. LVII] 215\\nGovernment. In Shreya Singhal v. Union of India,73 it was observed that any unlawful\\nacts beyond what is laid down in article 19(2) of the Constitution cannot form any\\npart of section 79 of the Act. The Supreme Court has acknowledged in the judgment\\nthat it would be challenging for intermediaries such as Google and Facebook to\\ndetermine the legitimacy of the millions of requests they receive.\\nThe High Court of Madras noted that there is a “substantial basis” for assertions\\nthat Rule 9 violates article 19(1)(a) of the Constitution and may be applied to\\nintermediaries coercively . In accordance with this, the Madras High Court on 16th\\nSeptember 2021 issued an interim order that any action under Rules 3 and 7 would be\\nsubject to the outcome of the challenge of constitutional validity as the main matter\\nwas likely to be taken up by the Supreme Court in coming days. Pursuant to requests', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 23}),\n",
       " '341fe6a2-1327-4abc-a560-75d160ce8359': Document(page_content='from the Central Government, the Supreme Court has transferred cases for regulation\\nof content on OTT  Platforms pending in dif ferent high courts to the Supreme Court,\\nand has passed orders prohibiting the relevant high courts from hearing these cases\\nwhile they are pending before the Supreme Court.74\\nIn the case of Omanakuttan K.G . v. Union of India,75 the petitioner filed a writ\\npetition in the public interest, seeking various reliefs, including mandamus or other\\nappropriate writs to compel WhatsApp to comply with the IT  Rules, 2021 and to\\nprohibit the reliance on WhatsApp contents by investigating agencies and courts. The\\npetitioner raised concerns about user manipulations, lack of security , traceability of\\nmessages, and compliance with the IT  Rules, 2021, citing the right to privacy .\\nWhatsApp ar gued that it is not bound by the IT  Rules, 2021 due to end-to-end\\nencryption and that the Rules infringe upon the right to privacy .', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 23}),\n",
       " '33b5921a-09af-4110-ba63-3b19bf686300': Document(page_content='The court, considering the provisions of the Rules, observed that the petitioner\\nmade vague allegations without providing supporting evidence. It noted that the\\ngovernment has already made provisions in the Rules to regulate and control the\\nplatform in question. The fact that the Rules are being challenged in the High Court\\nof Delhi does not automatically entitle the petitioner to the mandamus requested in\\nprayer .\\nThe court refused to grant the petitioner ’s requests for directions to investigating\\nagencies or courts to not rely on WhatsApp contents in their functioning, as it would\\ninterfere with the statutory framework of the criminal justice system, and the registrar\\ngeneral does not possess supervisory powers in this regard.\\nThe court found that the petitioner disregarded core judicial aspects and sought\\ndirections that exceeded the comprehension of the Constitution and laws concerning\\nthe issue of end-to-end encryption. It deemed it premature to issue the mandamus as', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 23}),\n",
       " '17840ce6-cccc-4e8f-8019-0f579c36477c': Document(page_content='sought by the petitioner and concluded that the petitioner failed to demonstrate any\\narbitrariness or illegality on the part of the respondents, justifying judicial review .\\nAccordingly , the court dismissed the writ petition, and the petitioner ’s request for a\\nwrit of prohibition was deemed unwarranted.\\n73 R. 3(1)(c), 2021 IT Rules.\\n74 (2015) 5 SCC 1.\\n75 Union of India  v. Sudesh Kumar Singh , Transfer Petition (C) No. 100-105/2021).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 23}),\n",
       " '00a44d22-00bc-472a-99e1-1af60edc3a4b': Document(page_content='Annual Survey of Indian Law 216 [2021\\nRemoval of content posted unlawfully fr om pornographic websites and de-\\nindexing fr om sear ch engine r esults\\nThe High Court of Delhi in X v. Union of India76 heard a case involving a\\nwoman (referred to as X) whose photos were posted on a pornographic website without\\nher consent. Despite court orders to remove the content, it continued to resurface on\\nthe internet. The court appointed an advocate as amicus curiae to address the issue.\\nThe Delhi Police requested directions to intermediaries, under sections 79(3)(a) and\\n(b) of the IT  Act, to remove and prevent the posting of unlawful content and to share\\nactual unlawful content, metadata, data dump, and basic subscriber information for\\ninvestigation purposes.  Google ar gued that search engines are not publishers but only\\nindex existing information, and they should only remove specific URLs upon request.\\nThey emphasized the need to consider context and avoid pre-emptive banning of', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 24}),\n",
       " '8ec6dbb2-25ee-44f0-b692-f985d44dbdb0': Document(page_content='content as it would jeopardize free speech and be contrary to the law quoting para 62\\nand 66 from Myspace.77 ISPAI stated that blocking content at the sub-page level is\\ntechnically challenging due to encryption mechanisms and suggested global source\\nblocking at the content provider level. The Ministry of Electronics and Information\\nTechnology suggested granting petitioners the right to request content removal. The\\ncourt discussed the extraterritorial jurisdiction of the IT  Act, the responsibilities of\\nintermediaries, and the exemptions (as outlined in section 79(1) and Rule 10 of the\\n2009 Rules) and liabilities (sections 67, 67A, and 67B) outlined in the Act and the\\n2021 IT  Rules. The court emphasized the need for intermediaries to expeditiously\\nremove or disable access to of fending content upon receiving ‘actual knowledge’  as\\nheld in Shreya Singhal  case.\\nThe court appointed amicus curiae78 highlighted the relevant provisions of the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 24}),\n",
       " 'bd143d32-84d2-493d-9065-068446acc479': Document(page_content='Information Technology Act, 2000, as amended, and the associated rules, including\\nthe 2021 Rules, which have increased the liabilities and obligations of intermediaries\\nin dealing with unlawful content. The 2021 Rules have set a shorter timeframe of 24\\nhours for removing or disabling access to such content. Failure to comply with these\\nrules can result in the revocation of the intermediary’ s liability exemption. The amicus\\ncuriae  also referenced legal precedents from both Indian and foreign jurisdictions\\nthat address intermediary liability and the responsibility to remove unlawful content.\\nThe court proposed directions to ensure the ef fective removal of unlawful content\\nwhile balancing the obligations of intermediaries and the rights of victims.79 These\\ndirections include immediate content removal within 24 hours of a court order ,\\npreservation of information related to the content, de-indexing by search engines,', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 24}),\n",
       " '7c7d0347-0da7-4968-81e1-905a21b1238a': Document(page_content='proactive monitoring by intermediaries, information sharing with law enforcement,\\nremoval from other platforms upon request, filing complaints on the National Cyber -\\nCrime Reporting Portal, and potential liability for non-compliance with court orders.\\nThese measures aim to strike a fair balance and facilitate meaningful compliance\\nwithout placing undue burden on intermediaries.\\n76 2021 SCC OnLine Ker 2758(decided on June 28, 2021).\\n77 2021 SCC OnLine Del 1788.\\n78 Myspace Inc  v. Super Cassettes Industries , 2017 (69) PTC 1 (Del) (DB).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 24}),\n",
       " '38e7c83f-3067-4f9a-a598-ad5459ef37ce': Document(page_content='Cyber Law Vol. LVII] 217\\nThe court ordered the petitioner to provide information to the Investigating\\nOfficer within 24 hours. The Delhi Police/CyP AD Cell was instructed to remove/\\ndisable access to the content within 24 hours. Search engines were directed to de-\\nindex the content globally and disable access to identical content on other platforms\\nwithin 24 hours. The investigating of ficer was tasked with sharing relevant URLs\\nwith other entities. The Delhi Police was instructed to obtain necessary information\\nfrom websites and search engines. The petitioner can request removal of similar content\\nfrom other platforms, with corresponding directions to the investigating of ficer. Non-\\ncompliance will result in loss of exemption and liability under the IT  Act. Parties can\\nseek clarification from the court if needed.\\nIntermediary liability on e-marketplaces\\nThe central issue in Kunal Bahl v. State of Karnataka80 case was whether an', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 25}),\n",
       " '32eab1a6-24af-4382-8336-89eaa1755118': Document(page_content='intermediary (the online marketplace www .snapdeal.com here) would be held liable\\nfor the sale of drugs that did not comply with the requirements under the Drugs and\\nCosmetics Act, 1949. The complaint was filed by the Inspector of Drugs based on\\ninformation received from the Deputy Drugs Controller , Mysore. It alleged that a\\nseller on Snapdeal’ s platform had sold SuHAGRA-10P  tablets. Since Snapdeal did\\nnot possess a license to sell drugs, it was accused of violating section 18(c) of the\\nDrugs and Cosmetics Act, 1940, which is punishable under section 27(b)(ii).\\nSnapdeal ar gued that it has fulfilled its obligations as an intermediary under the\\nInformation Technology Act, 2000 and the Intermediaries Guidelines Rules, 201 1. It\\nclaimed exemption from liability under section 79 of the Act for the following reasons:\\ni.Snapdeal had no involvement in the specific transaction in question.\\nii.Snapdeal merely provides a platform for communication and information sharing', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 25}),\n",
       " '59bb437c-2223-43fd-a518-277a58b6997e': Document(page_content='between sellers and buyers. The information about the products of fered for sale\\nby the accused seller was made available on Snapdeal’ s online marketplace.\\niii.As an intermediary , Snapdeal does not have control over the content posted by\\nusers on its platform.\\niv.Snapdeal has exercised “due diligence” as required by Section 79(2)(c) of the\\nInformation Technology Act, 2000, along with the Intermediaries Guidelines\\nRules, 201 1.\\nSnapdeal ar gues that as an intermediary , its liability under section 79(3)(b) of\\nthe Information Technology Act, 2000 is limited to the removal of third-party content\\nupon receipt of a court order or notice from a government authority . It cannot be held\\nresponsible for the listing and sale of products by independent third-party sellers on\\nits marketplace. Snapdeal cited the decisions in Sharat Babu Digumar ti v. Govt. (NCT\\nof Delhi)81 and Shreya Singhal  v. Union of India82 to support its position.\\n79 Supra  note 76.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 25}),\n",
       " '6ebe6cd1-ee90-4ede-a792-204069b107b0': Document(page_content='79 Supra  note 76.\\n80 X v. Union of India   supra  note 76  at para 90\\n81 2021 SCC OnLine Kar 15706(decided on Jan. 7, 2021).\\n82 (2017) 2 SCC 18.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 25}),\n",
       " '601b7a7b-e0fd-42d5-91f4-9c0e38f71726': Document(page_content='Annual Survey of Indian Law 218 [2021\\nSnapdeal also pointed out that the Consumer Protection (E-Commerce) Rules,\\n2020 have introduced a distinction between marketplace e-commerce websites (like\\nSnapdeal, Amazon, and Flipkart) and inventory e-commerce websites (such as Lifestyle\\nand Decathlon). Rule 5(1) of the Consumer Protection (E-Commerce) Rules, 202083\\nstates that in order to claim exemption under section 79 of the Information Technology\\nAct, 2000, marketplace e-commerce entities like Snapdeal must comply with the\\nrequirements of subsections (2) and (3) of section 79, as well as the Information\\nTechnology (Intermediaries Guidelines) Rules, 201 1.84\\nThe High Court of Karnataka ruled that an intermediary , as defined in section\\n2(w) of the IT  Act, along with its directors and of ficers, cannot be held responsible\\nfor any action or inaction taken by a vendor or seller utilizing the services provided\\nby the intermediary through a website or marketplace.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 26}),\n",
       " '8bc46785-1c6a-4d93-abb4-3ecf7472c7ca': Document(page_content='Court extensively discussed various provisions of the Cr PC, Information\\nTechnology Act, and Drugs and Cosmetics Act. In the context of Section 18(1)(c) of\\nthe Drugs and Cosmetics Act, 1940, it is essential for an individual to engage in\\nactivities such as manufacturing, distributing, stocking, exhibiting, or of fering for\\nsale without possessing a valid license issued under the Act. Therefore, neither Snapdeal\\nnor its directors can be held liable for an of fense punishable under section 27(b)(ii) of\\nthe Act.\\nIt concluded that no of fense was established against the accused, leading to the\\nallowance of the petitions and the quashing of the criminal proceedings initiated against\\nthe accused in question.\\nSnapdeal/accused no.2 cannot be held responsible for the sale of non-compliant\\nitems under the Drugs and Cosmetics Act, 1940. The court found no of fense and\\nquashed the criminal proceedings against Snapdeal and its directors.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 26}),\n",
       " '37663db7-5675-459b-bf89-6b1a97171066': Document(page_content='In Sanatan Sanastha   v. Union of India,85 a registered public charitable trust,\\nclaiming to be a Non-Governmental Or ganization (NGO), had filed a petition against\\nFacebook (respondents no. 3 and 4). The petitioner alleged that their Facebook pages,\\nwhich were used to spread spiritual teachings, had been blocked by the respondents\\nwithout providing any reasons or government orders. The petitioner ar gued that this\\naction was arbitrary , violated their constitutional rights, and constituted an unauthorized\\nexercise of power . During the proceedings, the petitioner received a communication\\nfrom the respondents, citing their right to permanently disable accounts that breached\\nFacebook’ s community standards. The petitioner amended the petition to include this\\ncommunication and referred to a civil suit in the High Court of Delhi (CS (OS) 510 of\\n83 (2015) 5 SCC 1.\\n84 5(1) Liabilities of marketplace e-commer ce entities . -', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 26}),\n",
       " 'e81882e6-943e-4240-855d-d8e8cf43dbd2': Document(page_content='(1) A marketplace e-commerce entity which seeks to avail the exemption from liability under\\nsub-section (1) of section 79 of the Information Technology Act, 2000 (21 of 2000) shall comply\\nwith sub-ss. (2) and (3) of that section, including the provisions of the Information Technology\\n(Intermediary Guidelines) Rules, 201 1.\\n85 Ibid.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 26}),\n",
       " 'f729b3d2-01e5-4db6-9976-80bfa607e81e': Document(page_content='Cyber Law Vol. LVII] 219\\n2016) filed by Sasikala Pushpa  v. Facebook .86 Additionally , the petitioner challenged\\nthe constitutionality of section 79 of the Information Technology Act, 2000, alleging\\na violation of fundamental rights under articles 14, 19, and 21 of the Constitution.\\nThe relief sought in this petition could not be granted because the constitutional\\nvalidity of section 79 of the Information Technology Act had already been upheld by\\nthe Supreme Court in the case of Shreya Singhal . It was well-established that once\\nthe Supreme Court upheld the constitutional validity of a provision, the high court\\ngenerally could not entertain a petition questioning the same provision based on new\\nor rephrased grounds. Therefore, there was no basis for granting relief as requested in\\nprayer clause D-1 of the petition.\\nThe relief sought in prayer clause D-2 of the petition was also unclear . Declaratory\\nrelief could not be granted in a vacuum, and there was no ongoing proceeding where', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 27}),\n",
       " '7abbe5dd-d9b3-4d71-a8ba-1e83ca6698fc': Document(page_content='the respondents had claimed or been granted immunity based on the provisions of\\nsection 79 of the Information Technology Act. If there had been a breach of a contractual\\nrelationship between the petitioner and the respondents regarding the blocking of the\\npetitioner ’s Facebook page, the appropriate course of action would have been for the\\nrespondents to seek redress through the appropriate forum. A petition under article\\n226 of the Constitution of India might not have been the appropriate remedy in such\\na situation. Therefore, the petition was dismissed with no order as to costs.\\nV IDENTITY  THEFT - SECTION 66C\\nSection 66C of the IT  Act provides for the punishment of identity theft. It\\nstipulates that individuals who intentionally and dishonestly use someone else’ s\\nelectronic signature, password, or other unique identification feature to deceive or\\ndefraud others can be sentenced to a maximum imprisonment of three years and may\\nalso face a fine of up to 1 lakh Rs/-.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 27}),\n",
       " '7000593d-d79e-4017-a176-035ae21583bf': Document(page_content='In Ravari Kirankumar v. Home Depar tment87 application was filed by the\\napplicant under section 482 of the Cr PC, seeking the quashing of FIR and the\\nsubsequent char ge-sheet claiming that the applicant has committed of fenses punishable\\nunder sections 66A  and 66C of the IT  Act, 2000. However , upon careful examination\\nof the complaint, FIR, and char ge-sheet, it is evident that the accusations primarily\\nrevolve around the applicant sending objectionable emails from their email ID to\\nvarious of fices. Notably , there are no allegations regarding the use of electronic\\nsignatures, passwords, or any other unique identification features of another person.\\nTherefore, there is no valid basis for registering an FIR under section 66C of the IT\\nAct.\\nMoreover , the court observed that section 66A  of the IT  Act has been deemed\\nunconstitutional by the Supreme Court in the case of Shreya Singhal  (supra) therefore,', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 27}),\n",
       " 'd2f1635a-c95c-439d-aa54-37ae2445e80d': Document(page_content='no prosecution can be maintained under section 66A  of the IT  Act. Therefore, the FIR\\nand char ge-sheet invoking section 66A  of the IT  Act must be quashed and set aside.\\nConsidering the absence of any allegations regarding fraudulent or dishonest use of\\n86 (2021)  SCC OnLine Bom 1049.\\n87 2020 SCC OnLine Del 618 (Decided on June 2, 2020).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 27}),\n",
       " 'd115595f-0790-47f9-b94a-438cbf8a806a': Document(page_content='Annual Survey of Indian Law 220 [2021\\nelectronic signatures, passwords, or unique identification features of another person,\\nit is perplexing how any prosecution can be pursued under section 66C of the IT  Act.\\nUpon examining the complaint, FIR, and char ge-sheet, it becomes apparent\\nthat the sole allegations against the applicant involve the sending of objectionable\\nemails on November 18, 2010 from their email ID, alert aa@redof fmail.com, to various\\noffices of NSSO (FOD) in India. These allegations were likely made to invoke the\\nprovisions of section 66A  of the IT  Act, which was applicable at the time. However ,\\nthroughout the entire complaint, FIR, and char ge-sheet, there are no accusations\\npertaining to the use of electronic signatures, passwords, or any other unique\\nidentification features of another person. Consequently , there was no justification for\\nregistering an FIR under section 66C of the IT  Act.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 28}),\n",
       " 'acb7fef6-df2e-44d8-9782-4b87d049d24a': Document(page_content='Therefore, it is imperative to quash the FIR and char ge-sheet since the allegations\\nin the FIR do not establish the commission of any of fense under section 66C of the IT\\nAct. Additionally , even if we assume the allegations to be true, they do not constitute\\nan of fense or establish a case against the applicant under section 66C of the IT  Act. It\\nis evident that the allegations do not fulfill the requirements for an of fense under\\nsection 66C of the IT  Act. Moreover , considering that section 66A  has already been\\nstruck down and cannot be invoked, the FIR and char ge-sheet must be quashed under\\nthese circumstances.\\n      The appellant in Santosh  v. State of Madhya Pradesh88 case has been\\nconvicted under section 66C of the Information Technology Act, 2000. The conviction\\nwas based on the appellant’ s alleged involvement in sending fraudulent emails using\\na forged email ID. The complainant, G .B. Bamankar , filed a complaint stating that an', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 28}),\n",
       " 'f178faf3-c51e-456b-985e-9a81b35b5907': Document(page_content='email was forwarded to him by Ashish Dongare, which was sent from Pankaj Kanthed’ s\\nemail ID. However , Pankaj Kanthed denied sending the email and claimed that his\\nemail ID had been fraudulently created by someone else.       Based on the complaint,\\nan FIR was registered under various sections of the Indian Penal Code (IPC) and the\\nIT Act. During the investigation, it was discovered that the appellant, Santosh Bharti,\\nwas the one who sent the email in question. The appellant was acquitted of some\\ncharges but convicted under section 66C of the IT  Act. Aggrieved by the conviction,\\nthe appellant has filed this appeal.\\n      The appellant’ s counsel contended that the prosecution’ s case suf fered from\\na critical flaw: the absence of proper evidence. They ar gued that the email in question,\\nwhich formed a crucial part of the prosecution’ s case, had not been certified in\\naccordance with Section 65-B of the Evidence Act. This section stipulates that', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 28}),\n",
       " '7890ffaf-8174-4f01-8099-01964d35d874': Document(page_content='electronic records must be supported by a certificate to be admissible as evidence in\\ncourt. In the absence of such certification, the email (Ex.P/2) could not be considered\\nreliable or valid evidence. Upon careful examination of the record, the court concurred\\nwith the appellant’ s counsel. It observed that the email in question was merely a\\nphotocopy of the forwarded email sent to the complainant, and this photocopy (Ex.P/\\n2) was not accompanied by the required certificate under section 65-B. The court\\nemphasized that the absence of certification was fatal to the prosecution’ s case, as it\\n88 2021 SCC OnLine Bom 4086.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 28}),\n",
       " '0b498257-c392-491c-a71c-3b43e1bd5e14': Document(page_content='Cyber Law Vol. LVII] 221\\nundermined the authenticity and admissibility of the email as per Supreme Court\\nruling in Anwar .\\n      Furthermore, the court noted that the prosecution’ s witness, P .W.3 Ritesh\\nSingh, a constable in the cyber cell, did not provide any substantial testimony regarding\\nthe email (Ex.P/2). His examination-in-chief remained silent on this crucial document,\\nand although he was not cross-examined on the matter , the court deemed it\\ninconsequential. The primary responsibility of the prosecution was to establish the\\nadmissibility and authenticity of the evidence, which they failed to do. The court\\nreferred to the Supreme Court’ s decision in the case of Anvar P .V., which emphasized\\nthat a mere printout or photocopy of an electronic record cannot be admitted as evidence\\nwithout a certificate under section 65-B.\\n       Considering the lack of certification for Ex.P/2, the court concluded that', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 29}),\n",
       " '71783ef5-a1d4-4551-8c5d-4c967abddcb6': Document(page_content='the prosecution had not succeeded in proving the appellant’ s guilt beyond a reasonable\\ndoubt. It further criticized the trial court for failing to address this crucial aspect and\\nproceeding to decide the case on its merits, which was deemed improper .\\nIn light of these findings, the court allowed the appeal, set aside the impugned\\njudgment, and acquitted the appellant. It deemed it unnecessary to delve into the\\nother grounds raised by the appellant, as the lack of certification alone was suf ficient\\nto undermine the prosecution’ s case and warrant the appellant’ s acquittal.\\nVII ONLINE PRIV ACY\\nIn the case of Manohar Lal Sharma  v. Union of India,89 the Supreme Court of\\nIndia ordered an independent investigation into unauthorized surveillance using the\\nPegasus software. A committee comprising three technical experts90 was appointed to\\nprobe the matter , assess the tar geting of Indian citizens’  devices, review the software’ s', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 29}),\n",
       " '268f78e8-1ab9-4234-978e-c58cd166afc3': Document(page_content='acquisition, and provide recommendations for strengthening cyber security and privacy\\nprotection. Raveendran J., oversaw the committee’ s functioning, ensuring adherence\\nto procedures and thorough investigations. The court emphasized the importance of\\nprivacy , constitutional restrictions, and the balance between national security and\\nindividual rights. It expressed concerns about the potential impact on free speech and\\nthe need to protect democratic values in the face of emer ging technologies and\\nsurveillance practices.\\nThe court acknowledged privacy limitations but emphasized that restrictions\\nmust align with the constitution. It rejected the government’ s request for unrestricted\\nimmunity in the name of national security . The court expressed concerns about the\\ninfluence of surveillance on free speech and the potential for self-censorship. It\\nunderscored the importance of privacy , constitutional restrictions, and balancing', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 29}),\n",
       " '344ec604-9d54-446e-a500-07b0d27d377e': Document(page_content='national security with democratic values in the face of emer ging technologies and\\nsurveillance practices.\\n89 2021 SCC OnLine MP 686.\\n90 2021 SCC OnLine SC 985(Decided on Oct. 27, 2021).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 29}),\n",
       " 'f1e892fe-1c77-4ad5-9f25-1ca429938367': Document(page_content='Annual Survey of Indian Law 222 [2021\\nV CONCLUSION\\nOver the past decade, the internet has become widely accessible in India, leading\\nto a significant increase in digital connectivity and the growth of the digital economy .\\nThe COVID-19 pandemic further accelerated the use of social media platforms and\\ninformation sharing. However , this resulted in several challenges like spread of fake\\nnews causing riots and mob lynching and the stifling of voices against oppression.\\nLack of transparency and accountability in dealing with malicious content on social\\nmedia platforms added to the rising discontent among society . Additionally , the rise\\nof OTT platforms for entertainment has highlighted the need for content regulation.\\nIn 2021, several noteworthy events shaped the landscape of cyber law in India.\\nWhatsApp’ s privacy policy revision faced backlash for allowing the sharing of sensitive\\npersonal data, while the Indian government introduced the Information Technology', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 30}),\n",
       " '85720c0a-9e0d-4a9c-a9dc-9e3f6380cee4': Document(page_content='(Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021 under section\\n87 of the Information Technology Act, 2000. These rules aimed to increase\\naccountability for social media platforms, establish a self-regulatory framework, and\\naddress concerns regarding digital content and OTT platforms.\\n The Rules dif ferentiate between social media intermediaries and significant\\nsocial media intermediaries, aiming to promote innovation and facilitate new platform\\ngrowth. Significant social media intermediaries have additional due diligence measures\\nand face criminal consequences for non-compliance. These Rules address concerns\\nabout digital content on digital media and OTT  platforms. The Ministry of Information\\nand Broadcasting oversees these issues, while the Information Technology Act governs\\nthe regulatory framework. The Rules establish a self-regulatory framework and a Code\\nof Ethics for news publishers and OTT  platforms. A three-tier grievance redressal', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 30}),\n",
       " '77c242f9-9215-40a9-876a-d8f0648926d9': Document(page_content='mechanism is in place. However , concerns have been raised about potential threats to\\npress freedom and media due to the allocation of adjudicatory powers to the executive\\nbranch.\\nIn 2020, the government issued several content takedown and blocking orders\\nwithout providing adequate explanations. The 2021 IT  Rules now require reasons for\\ntakedowns to be discussed and of fer a grievance redressal mechanism to challenge\\ngovernment actions. Intermediaries have greater responsibilities, including due\\ndiligence, monitoring, and user education. The Rules aim to foster a culture of self-\\nregulation among social media intermediaries, supported by artificial intelligence tools.\\nHigh courts received multiple public interest litigations challenging the\\nconstitutionality of certain provisions in the IT  Rules, 2021. The High Court of Mumbai\\ngranted a stay on the rules related to digital publishers, stating that Rule 9 goes beyond', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 30}),\n",
       " '00376eaa-2340-4751-aa91-86a0c826cfb9': Document(page_content='the scope of the IT  Act. Specifically , Rule 9(1) and Rule 9(3) were stayed as they were\\ndeemed to exceed delegated power and infringe upon the constitutional right to freedom\\nof speech and expression. This decision had a pan India ef fect, being accepted by\\nother high courts.\\nThe X v. Union of India  judgment is praised for its lucid exposition of the\\nprocedure and guidelines for intermediaries and government agencies to remove\\noffensive content from digital platforms in accordance with the IT  Rules, 2021. The', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 30}),\n",
       " '85a195f5-f3ae-4594-85fc-8e5c8f4e7321': Document(page_content='Cyber Law Vol. LVII] 223\\njudgment addressed concerns of individuals facing victimization through the posting\\nof obscene content about them online. While the victim’ s photographs taken from her\\nsocial media accounts were not obscene, their unauthorized posting on a pornographic\\nwebsite made them of fensive by association.\\nThe landmark judgment in Arjun Panditrao Khotkar  v. Kailash Kushanrao  by\\nthe Supreme Court was believed to have settled the jurisprudence on the admissibility\\nof electronic evidence, restoring Anwar ruling. However , the current stance of the\\nSupreme Court suggests that WhatsApp messages may not be considered admissible\\nevidence due to concerns about their authenticity and potential tampering. The court\\nhas expressed reservations about the evidential value of such messages, considering\\nthat they can be easily created, modified, or deleted by anyone. Nevertheless, with the\\nexistence of the Information Technology Act, 2000 and the continuous advancements', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 31}),\n",
       " 'a601c5a0-ce10-425e-896c-a2d3e189d1b2': Document(page_content='in the field, it is anticipated that there will be significant progress in the regulatory\\nframework regarding electronic evidence in the future.\\nIn the ongoing process of finalizing the long-awaited data protection legislation,\\nprogress has been made with the submission of the Joint Parliamentary Committee’ s\\nreport on the Personal Data Protection Bill, 2019 to both Houses of Parliament. The\\ncommittee has endorsed the bill with certain observations, suggesting the need to\\nbroaden its scope to include non-personal data. It has also supported exemptions for\\ncertain government agencies from the applicability of the data protection law on\\nspecified grounds, despite dissenting opinions expressing concerns about constitutional\\nviolations and the potential creation of separate ecosystems. The report is currently in\\nthe hands of the Indian government, and it remains to be seen how the government\\nwill incorporate the committee’ s recommendations into a revised version of the Data', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 31}),\n",
       " '01096ca3-409c-4564-833a-3075c209ff01': Document(page_content='Protection Bill.91\\nThe Pegasus controversy of 2021 sparked substantial attention and legal\\ndiscourse in Indian cyberspace. The Supreme Court’ s response,92 which recognized\\nthe right to privacy as a fundamental right and called for independent investigations,\\nelicited contrasting viewpoints. Critics ar gue that the court’ s actions may encroach\\nupon the executive’ s authority over national security and surveillance. They raise\\nconcerns about the court’ s lack of technical expertise in appointing an investigative\\ncommittee and the potential ramifications for counterterrorism ef forts, including\\noperational challenges and delays. Nonetheless, this judicial decision empowered the\\ncourts to scrutinize matters of national security and public interest, thereby contributing\\nto the ongoing development of cyber legal jurisprudence in India.\\nWhile the Pegasus controversy did generate significant attention and legal', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 31}),\n",
       " '56eda0f2-982a-4809-ab97-943b6e5791b0': Document(page_content='discussions in Indian cyberspace in 2021, it is important to note that the response\\nfrom the Supreme Court and acknowledgement of the right to privacy as a fundamental\\n91 The committee consisted of three technical experts: Naveen Kumar Chaudhary(Dean of National\\nForensic Sciences University ,Gujarat) , Prabaharan P  (Professor at Amrita Vishwa Vidyapeetham,\\nKollam, Kerala) and Ashwin Anil Gumaste(Associate Professor at the Indian Institute of\\nTechnology , Bombay).\\n92 Pawan Duggal.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 31}),\n",
       " '0d08db81-4cde-4db7-ae6a-7b8ea091824c': Document(page_content='Annual Survey of Indian Law 224 [2021\\nright and its order for independent investigations93 could be seen as encroachment\\nupon the executive’ s domain of national security and surveillance. Concerns could be\\nraised about the court’ s lack of technical expertise in appointing a committee to\\ninvestigate the Pegasus software and the possible implications for counterterrorism\\nefforts, including delays and operational challenges. However , this decision empowered\\nthe courts to examine matters of national security and public interest but also\\ncontributed to the ongoing development of cyber legal jurisprudence in India.\\nIn 2021, a significant development emer ged with the introduction of the crypto\\ncurrency and Regulation of Of ficial Digital Currency Bill, 2021. The primary aim of\\nthis bill is to establish a comprehensive framework for the of ficial digital currency\\nissued by the Reserve Bank of India. Additionally , the bill seeks to prohibit the use of', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 32}),\n",
       " '11f6008c-40ba-4d2a-a61a-9d10342bc7e6': Document(page_content='private crypto currencies in India, while allowing certain exceptions to support crypto\\ncurrency technology . Its overall objective is to bring consistency and regulation to the\\ncrypto currency market, replacing private crypto currencies with an of ficial digital\\ncurrency and enforcing stringent penalties for non-compliance. The implementation\\nof ef fective regulation would not only facilitate taxation of crypto currency revenue\\nbut also generate advantages for both the government and investors. These\\nadvancements set the stage for future changes and progress in the dynamic field of\\ncyber law , warranting careful observation.\\n93 Manohar Lal Sharma  v. Union of India   Civil/Criminal Jurisdiction Writ Petition No. 314 of\\n2021\\n94 Manohar Lal Sharma  v. Union of India   Civil/Criminal Jurisdiction Writ Petition No. 314 of\\n2021', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 32}),\n",
       " '4c6a3cd9-9666-4f8a-b301-0a3c596740ca': Document(page_content='Annual Survey of Indian Law 192 [2021\\n9\\nCYBER LA W\\nDeepa Kharb*\\nI INTRODUCTION\\nCYBER LA W, a swiftly progressing field that intersects with numerous conventional\\nlegal disciplines, has under gone substantial transformations since 2014.This survey\\nexamines the evolving landscape of cyber law by analyzing the judicial decisions in\\n2021. It delves into crucial areas such as online privacy , data protection, cybercrimes,\\nand electronic evidence, providing valuable insights into the development of cyber\\nlaw in India. This survey serves as a practical guide for navigating the intricate\\nchallenges of the digital realm. Additionally , it presents a critical perspective on the\\ncourt’ s reasoning, identifying points that may be subject to further debate. Overall,\\nthe survey underscores the dynamic nature of cyber law and its significant impact on\\nthe legal framework, reflecting the judiciary’ s efforts to adapt to the challenges posed\\nby the digital age.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 0}),\n",
       " '98545154-408f-4c85-954c-b9d595e2eede': Document(page_content='by the digital age.\\nII ADMISSIBILITY  OF ELECTRONIC EVIDENCE: SECTION 65B IEA\\nThe introduction of sections 65A  and 65B in the Evidence Act in 2000 provided\\na framework for the admissibility of electronic evidence, with section 65B (1) allowing\\nfor the admissibility of a paper printout of information contained in electronic records\\nsubject to the conditions specified in section 65B(2).\\nHowever , the requirement for a certificate under section 65B of the Indian\\nEvidence Act for electronic records to be admissible in court has been a matter of\\ndebate among legal scholars and courts. While the Supreme Court in Anvar P .V. v.\\nP.K. Basheer1 (Anvar  hereinafter) held that such records cannot be admitted as\\nsecondary evidence unless the requirements of section 65B are met, the court in Shafhi\\nMohammad 2 (Shafhi  hereinafter) concluded that the certificate requirement may be\\nwaived wherever the interest of justice so justifies say when the electronic device', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 0}),\n",
       " '7c5875e1-df39-4429-894b-dc8c622cc981': Document(page_content='storing the records is inaccessible or  where the electronic device is produced by a\\nparty who is not in possession of such device, as a result of which such party would\\nnot be in a position to secure the requisite certificate.\\n* Assistant Professor , The Indian Law Institute, New Delhi.\\n1 (2014) 10 SCC 473.\\n2 (2018) 2 SCC 801 (decided on Apr. 25, 2018).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 0}),\n",
       " '5acf0046-8ff2-42d8-8808-d7a35f31c676': Document(page_content='Cyber Law Vol. LVII] 193\\nIn a recent decision, Arjun Panditrao Khotkar  v. Kailash Kushanrao Gorantyal3\\n(Arjun Panditrao Khotkar hereinafter), a three-judge bench4 of the Supreme Court\\nclarified that the certificate required under section 65B(4) is a prerequisite for the\\nadmissibility of electronic evidence. The bench af firmed the correctness of the Anvar\\nruling in its interpretation, while finding that the Shafhi decision’ s division bench\\nhad erroneously “clarified” the requirement. The court also noted that the certificate\\nis not necessary if the original electronic record is produced in court, however ,\\ncompliance with section 65B is compulsory before a ‘computer output’, which is\\nconsidered secondary evidence of an electronic record, can be admitted as evidence.\\nThe court stated that if a person refuses to provide the certificate required under\\nsection 65B (4) of the Indian Evidence Act, a party can make an application to the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 1}),\n",
       " '55bf415e-41d5-4e72-a351-39f53732dd92': Document(page_content='judge requesting the production of the certificate. The court also explained that if it is\\nimpossible for the person to provide the certificate, or if the law excuses the person\\nfrom doing so, then the party should be excused from the mandatory requirement of\\nsection 65B (4).5 The court instructed trial courts to summon the person(s) specified\\nin section 65B (4) when a defective certificate is given or when a certificate is refused,\\nand require them to provide the necessary certificate. The court clarified that since\\nsection 65(B) does not talk about the stage at which such certification can take place,\\nthis is subject to the discretion exercised by the courts in civil cases, and in criminal\\ntrials, the accused must be supplied with all documents that the prosecution seeks to\\nrely upon before the trial. The courts must balance the rights of the parties while\\nexamining any application by the prosecution under sections 91 or 31 1 of the Criminal', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 1}),\n",
       " 'f38ce7b8-5934-4b3b-a773-20688832056b': Document(page_content='Procedure Code, 1973 or section 165 of the Evidence Act, ensuring no serious or\\nirreversible prejudice to the accused.\\nAlthough the relaxation of the strict requirements under section 65B (4) was\\naimed at easing the burden on parties who have made best ef forts to obtain a certificate\\nbut failed to do so, it has been ar gued that such an exception goes beyond what the\\nstatute permits and creates further ambiguity . Furthermore, the obligation on the courts\\nto summon the authorized person(s) to produce the certificate could result in a\\nprolonged mini-trial within the trial, adding to the already overburdened judicial system\\nand causing delays and additional expenses for the parties involved.\\nAfter the Arjun Panditrao Khotkar6 case, various high courts in India have\\nfollowed its ratio in their respective judgments especially on the exception created.\\nThey have held that electronic evidence must be accompanied by a certificate under', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 1}),\n",
       " '9a830cf0-a060-43ab-a950-2755f457dd32': Document(page_content='section 65B of the Indian Evidence Act to be admissible in court. Failure to comply\\nwith this requirement results in the electronic evidence being inadmissible.\\n3 2020 SCC OnLine SC 571(decided on July14, 2020).\\n4 Bench consisting of RF Nariman, S. Ravindra Bhat, and V. Ramasubramanian\\n5 Due to the applicability of the Latin maxims ‘lex non cogit ad impossibilia’  (the law does not\\ndemand the impossible) and ‘impotentia excusat legem’  (when there is a disability that makes\\nit impossible to obey the law , the alleged disobedience of the law is excused).\\n6 Supra note 3; 2020 SCC OnLine SC 571(decided on July14, 2020).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 1}),\n",
       " 'b020ddc3-6218-41f8-8204-9bb1ea2aadd2': Document(page_content='Annual Survey of Indian Law 194 [2021\\nIn Rakesh Kumar Singla  v. Union of India7  the petitioner , under the NDPS Act,\\nfiled a bail petition claiming they were unlawfully detained as no contraband was\\nfound in their possession at the time of arrest. The Narcotics Control Bureau (NCB)\\npresented statements from a co-accused and the petitioner as evidence. However , the\\ncourt emphasized that the determination of the petitioner ’s guilt or innocence should\\nbe based on the evidence presented during the trial. The NCB opposed the bail\\napplication, citing WhatsApp chat screenshots as evidence connecting the petitioner\\nto the illicit drugs. Nevertheless, the court stated that WhatsApp messages cannot be\\nconsidered as valid evidence without a certificate under section 65B of the Indian\\nEvidence Act, as per the recent Supreme Court judgment in the case of Arjun Panditrao\\nKhotkar  v. Kailash Kushanrao Gorantyal.8 Therefore, the court concluded that the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 2}),\n",
       " '0c2488b8-9cd1-4484-aa37-5f6741f544d4': Document(page_content='WhatsApp messages, in their current state, hold no evidentiary value. Investigating\\nagencies may rely on WhatsApp messages during a crime investigation, but a certificate\\nunder section 65B of the Indian Evidence Act is necessary for their admissibility .\\n In Mahendra N. Par deshi v. State of Maharashtra9 the prosecution attempted\\nto prove the content of a DVR as evidence in a bribery case under section 7 and\\n13(1)(d) read with 13(2) of Prevention of Corruption Act, 1988, but failed to produce\\nthe original DVR or a certificate under section 65-B(4) of the Indian Evidence Act.\\nThe court held that verbal evidence about the contents of an electronic record is\\nconsidered secondary evidence and the prosecution must prove that the contents of\\nthe DVR were heard. The court found that the prosecution failed to provide evidence\\nthat the contents of the DVR were heard and the witness could not confirm the\\nconversation’ s content. Additionally , since the record was deleted, the court drew an', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 2}),\n",
       " '90092861-2601-4b92-91a2-12786ed8a9c2': Document(page_content='adverse inference against the prosecution and dismissed the case.\\nIn Yogesh Arun W akure v. State of Maharashtra ,10 the appellant, an accused\\nfacing the char ge of murder punishable under section 302, 201, 323, 143, 147, 149\\nread with section 135 of the IPC with others, tried to establish his presence inside the\\nhotel on the basis of the CCTV  footage. However , an eye-witness claimed to have\\nseen the present appellant at the crime spot around the time of murder .\\nIt was contended from the appellant side that the trial court should not have\\ndisregarded the IO’ s report based on call detail records (CDR) and subscriber detail\\nrecords (SDR) without considering section 65B and relied solely on the word of mouth\\nof the eye-witness  as it is often said that “humans may lie, but documents would not\\nlie” or “documents would speak louder than words”. The apex court has held in Anvar11\\nthat electronic records must be produced according to section 65B, after which their', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 2}),\n",
       " '21fdb2f8-0f62-4621-8f50-f5e465c0e8fb': Document(page_content='genuineness can be questioned and resort can be made to section 45A of the Evidence\\nAct for seeking an opinion of the examiner of electronic evidence. The court directed\\nthe Registrar to transmit the documents, including the CDR/SDR record and DVD, to\\n7 MANU/PH/001 1/2021.\\n8 Supra note 3.\\n9 2020 SCC OnLine Bom 7873(Decided on Oct. 23,2020).\\n10 2021 SCC OnLine Bom 354 (Decided on Mar . 10, 2021).\\n11 Supra note 1.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 2}),\n",
       " '1bb53dba-c3f3-41c0-aa43-26557676bc04': Document(page_content='Cyber Law Vol. LVII] 195\\nthe trial court in a sealed envelope. The trial court will open the envelope upon receipt,\\nand the contents will be a part of the original record.\\nIn the case of Pramod v. State of Maharashtra,12 the accused faced char ges\\nunder various sections of the Indian Penal Code, 1860 including murder , kidnapping,\\nand destruction of evidence. The prosecution sought to rely on electronic evidence in\\nthe form of Call Data Records (CDR) to prove their case.\\nThe defense counsel ar gued that the electronic evidence presented by the\\nprosecution is not admissible because the certificates do not comply with section 65B\\n(4) of the Evidence Act. The certificates do not identify the electronic record containing\\nthe statements and do not specify the devices or computers over which they had control.\\nThe defense counsel also pointed out that none of the certificates are signed by a\\nperson occupying a responsible of ficial position in relation to the operation of the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 3}),\n",
       " 'd35d61dc-7257-4491-8c79-1717903e5d2d': Document(page_content='relevant device or the management of the relevant activities. Therefore, the certificates\\nrelied upon by the prosecution in support of electronic evidence are not admissible in\\nevidence. The defense counsel did not dispute the exchange of calls between the\\naccused persons or the findings of the trial court regarding the cell phone locations of\\nthe accused persons and exchange of calls between them at the relevant time.\\nSections 65A  and 65B of the Evidence Act, deal with the admissibility and\\ncontents of electronic records as evidence in court. Electronic records are considered\\nto be complete in themselves and can be admissible as evidence subject to the\\nprovisions of Section 65B (4) of the Evidence Act. Section 65B(1) of the Evidence\\nAct distinguishes between the original electronic record and the output from such\\ndevices, which is a copy or data derived from the original document. The original\\nelectronic record is the one on which the information is first stored, and the secondary', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 3}),\n",
       " '2f00dd6c-7b32-44b0-bb6d-ba8bb8f3aa62': Document(page_content='document is the one that contains the information derived from the original electronic\\nrecord. The same was expounded by the Supreme Court in the case of Arjun Panditrao\\nKhotkar  in the following words:13\\n73.2. The clarification referred to above is that the required certificate\\nunder Section 65B(4) is unnecessary if the original document itself is\\nproduced. This can be done by the owner of a laptop computer , computer\\ntablet or even a mobile phone, by stepping into the witness box and\\nproving that the concerned device, on which the original information\\nis first stored, is owned and/or operated by him. In cases where the\\n“computer” happens to be a part of a “computer system” or “computer\\nnetwork” and it becomes impossible to physically bring such system\\nor network to the Court, then the only means of providing information\\ncontained in such electronic record can be in accordance with Section\\n65B(1), together with the requisite certificate under Section 65B(4).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 3}),\n",
       " '8607bdcd-74dd-4801-b903-2319ae94f185': Document(page_content='The last sentence in Anvar P .V. (supra)  which reads as “…if an\\nelectronic record as such is used as primary evidence under Section 62\\nof the Evidence Act…” is thus clarified; it is to be read without the\\n12 2021 SCC OnLine Bom 3344\\n13 Arjun Panditrao Khotkar   supra note 3 in para no. 73.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 3}),\n",
       " '00cb9587-1ec5-4c5f-a15d-6d13d6d7d354': Document(page_content='Annual Survey of Indian Law 196 [2021\\nwords “under Section 62 of the Evidence Act,…” With this clarification,\\nthe law stated in paragraph 24 of Anvar P .V. (supra)  does not need to\\nbe revisited.\\nSections 65A  and 65B of the Evidence Act deal with the admissibility and\\ncontents of electronic evidence. A section 65B(4) certificate is mandatory for secondary\\nevidence and can be given by a person in a responsible position related to device\\noperation or management. The court relied on previous rulings Arjun Panditrao\\nKhotkar  14 and Engineering Analysis Centr e15 to hold that the prosecution should be\\nrelieved of the obligation to provide a section 65B(4) certificate if they have made\\nefforts to obtain it but have no control over the relevant third-party companies. The\\ncourt found that the electronic evidence produced by the prosecution was admissible\\nin and suf ficiently corroborated the circumstantial evidence presented. The certificates', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 4}),\n",
       " 'c6fc7452-b9b2-4032-b8d3-8c750fa0dce4': Document(page_content='produced by the prosecution were found to identify the electronic records and describe\\nthe manner in which they were produced as mandated by apex court in Anvar . The\\ncourt noted that section 65B(4) is mandatory but any infirmity in the certificates can\\nbe overlooked given the circumstances.\\n The prosecution in State of Maharashtra, thr ough the Police S tation Officer v.\\nSagar V ishwanath Borkar16 case relied on CCTV  footage, which was copied onto a\\npen drive and a certificate under section 65B of the Evidence Act was taken. However ,\\nthe prosecution failed to produce primary evidence in the form of the hard disc of the\\nCCTV  footage. The court held that the prosecution needed to comply with sub-section\\n(4) of section 65B of the Evidence Act, which requires evidence of a person in a\\nresponsible of ficial position in relation to the operation or management of the CCTV\\nsystem. The mere exhibition of the CCTV  footage by the trial court and the absence', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 4}),\n",
       " '2da53991-4866-4701-8d53-7aa4419c2bbf': Document(page_content='of objections by the accused are not suf ficient to make the footage admissible. The\\ncourt cited Arjun Panditrao Khotkar  17to support this ruling. As a result, the CCTV\\nfootage cannot be relied upon as admissible evidence by the prosecution as it was not\\nsupported by a certificate under section 65B(4) of the Evidence Act. The court opined\\nthat the trial court was correct in refusing to rely on this evidence for non-compliance\\nwith section 65B(4) of the Evidence Act.\\nIn Sanjib Sarkar v. Rajasr ee Roy ,18 a matrimonial dispute concerning the\\nannulment of marriage under section 25(III) of the Special Marriage Act, the\\nadmissibility of electronic evidence, including Facebook posts and pictures, submitted\\nby the respondent/wife was challenged by the appellant/husband’ s counsel on the\\ngrounds of lack of certification under section 65B (4) of the Indian Evidence Act. The\\ncourt considered the ar guments and referred to the law laid down in the Arjun Panditrao', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 4}),\n",
       " '5d94e046-24c1-466b-bcc4-b762633a8458': Document(page_content='Khotkar  case,19 which distinguished between the manner of tendering primary and\\n14 Supra note 3.\\n15 2021 SCC OnLine SC 159(Decided on Mar . 2, 2021).\\n16 2021 SCC OnLine Bom 2725(Decided on Sep. 7, 2021).\\n17 Supra note 3.\\n18 2021 SCC OnLine Cal 2916(Decided on Nov . 11, 2021).\\n19 (2020) 7 SCC 1, supra note 3.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 4}),\n",
       " '325d81a8-8e64-44f1-b94d-5679b7c6e746': Document(page_content='Cyber Law Vol. LVII] 197\\nsecondary evidence in electronic form. The court held that the required certification\\nwas not necessary for original documents produced by the owner of the device who\\ncan prove ownership and operation by stepping into the witness box. However , in\\nsituations where the source of information is part of a computer or computer network,\\ncertification under section 65B(4) is required. In the present case, the electronic\\nevidence relied upon by the respondent was sourced from her original electronic device\\nand therefore, certification was not required. The court found that the evidence\\npresented by the respondent was admissible and she had proved her contention relating\\nto fraud practiced on her .\\nIn another case of Sachin Makade Bablu Bhagwan Dangr e v. Nar cotics Contr ol\\nBureau,20 the accused individuals were char ged with dealing in illegal medical drugs,\\nit was ar gued by accused that no such drugs were found in their possession or at their', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 5}),\n",
       " '8ce50a64-d8ad-4cac-9988-13f95265cef1': Document(page_content='places of residence or work. The sole evidence against them was supposedly retrieved\\nfrom their electronic devices, which have been contested as inadmissible without a\\ncertificate under section 65B of the Indian Evidence Act. The defense ar gued that the\\npossibility of future criminal behavior must be taken into account under section 37\\nNDPS, and that the recovered Tramadol tablets from Dipu Singh cannot be linked to\\nthe accused individuals. Additionally , the information extracted from their electronic\\ndevices cannot be accepted without the requisite certificate under section 65B of the\\nIndian Evidence Act.\\nThe court, citing the cases of Arjun Panditrao Khotkar  21 and Engineering\\nAnalysis Centr e of Excellence Private Limited v. The Commissioner of Income T ax,22\\nstated that the mandatory obligation under section 65B(4) of the Indian Evidence Act\\nmay be waived if the respondent has made all reasonable attempts to obtain the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 5}),\n",
       " 'fbdbcfc3-5a7a-4688-aa9b-bd8948ac988f': Document(page_content='necessary certificate, even if it was to be provided by a third party who was not under\\ntheir control.\\nAlthough the National Control Bureau (NCB) obtained section 65B certificates\\nfrom a cyber forensic expert who analyzed the electronic devices and extracted the\\ndata, the court concluded that it was not suf ficient grounds to grant the accused\\nindividuals bail at this time. The court dismissed both petitions, but granted them the\\nfreedom to reapply for bail after examining public witnesses regarding the recovery .\\nAll pending applications were also resolved.\\nIn an interesting application filed by the petitioner under article 227, Sitanshi v.\\nVandana Sharma,23 seeking directions for Bharti Airtel Limited to preserve and produce\\nthe CDR of the respondent’ s mobile number . The petitioner ar gued that the CDRs\\nshould be preserved since they may be relevant and required at the time of the trial.\\nHowever , the Trial Court rejected the application, as filed under Section 151 of the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 5}),\n",
       " 'f0883f76-c3b8-4877-bc3e-c610fdd170f2': Document(page_content='Civil Procedure Code, 1908, stating that it amounted to a roving inquiry and invasion\\nof privacy . The Delhi High Court  noted that the directions given by the Supreme\\n20 2021 SCC OnLine Del 5121(Decided on Nov . 29, 2021).\\n21 Supra note 3.\\n22 2021 SCC OnLine Bom 2725(Decided on Sep. 7, 2021).\\n23 2021 SCC OnLine Del 4497(Decided on Sep. 20, 2021).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 5}),\n",
       " '12869db8-b7cc-4969-9d64-027a584cc941': Document(page_content='Annual Survey of Indian Law 198 [2021\\nCourt in Arjun Panditrao Khotkar  case on preserving CDRs were in the context of\\nrecords seized during investigation and cannot be invoked in this case. The Supreme\\nCourt’ s directions on maintaining CDR and relevant records were only for those seized\\nduring investigation, as stated in paragraph 62. Paragraph 72 directs courts dealing\\nwith electronic evidence to ensure preservation and production of certificates for such\\nrecords seized during investigation. The high court found no infirmity in the trial\\ncourt’ s order and did not interfere.\\nThe High Court of Delhi in Megha Enterprises v. Haldiram Snacks Pvt. Ltd.,24\\nheard a petition under section 34 of the Arbitration and Conciliation Act, 1996. The\\npetitioner challenged an arbitral award dated October 26, 2020, ar guing that the arbitral\\ntribunal erred in accepting an electronic letter as evidence without proof and af fidavit', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 6}),\n",
       " '606b6181-c571-46dd-a412-429e1b0c950d': Document(page_content='under section 65B of the Evidence Act. However , the court rejected the ar gument,\\nstating that the Indian Evidence Act does not apply to arbitrations, and the petitioners\\ndid not raise any objections before the arbitrator . The court found evidence showing\\nthat the respondent sent an email acknowledging the balance confirmation of Rs.\\n19,03,77,000/-, which was mentioned in a letter issued by the respondent. The email\\nand letter are admissible as evidence under various provisions, including section 4 of\\nthe Information and Technology Act, 2000. The respondent did not dispute the\\ntransmission of information in electronic form. Therefore, emails acknowledging the\\ndebt due to the petitioner also meet the requirements under section 18 of the Limitation\\nAct, 1963.\\nThe petitioner in Lalu v. Sheeja25 had filed an original petition to cancel a divorce\\ndecree obtained by the first respondent, citing fraud. The petitioner had submitted an', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 6}),\n",
       " '377dd710-5c71-4260-b1c3-61833d61d442': Document(page_content='application under section 45 of the Evidence Act to submit two CDs for voice\\nidentification. However , the court below had dismissed the application, stating that it\\ndid not comply with section 65B(4) of the Indian Evidence Act. The petitioner\\nchallenged this order in the original petition.\\nThe petitioner had also filed an application to send a CD to an expert for voice\\ncomparison, but the court had dismissed the application because it did not comply\\nwith section 65B(4) of the Indian Evidence Act. Nevertheless, the petitioner ar gued\\nthat a certificate was not required at this stage, as the CD only needed to be examined\\nby an expert. Additionally , the petitioner had produced the mobile phone containing\\nthe primary evidence. According to section 14 of the Evidence Act, the court should\\nrely on relevant evidence produced by the parties in matrimonial disputes. The court\\nshould not prevent a party from adducing relevant evidence to prove their case. As', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 6}),\n",
       " '4cbb724e-8e78-443e-8d2b-e82e184784bb': Document(page_content='such, the court was wrong in disallowing the prayer sought in the application.\\nThe apex court, in Arjun Panditrao Khotkar  v. Kailash Kushanrao Gorantyal,26\\nhad held that a certificate under section 65B (4) was unnecessary if the original\\ndocument itself was produced. In proceedings under the family court, the technicalities\\nof the Indian Evidence Act regarding the admissibility or relevancy of evidence were\\n24 2021 SCC OnLine Del 2641(Decided on Apr. 15, 2021).\\n25 2021 SCC OnLine Ker 9833(Decided on Sep.17, 2021).\\n26 Supra note 3.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 6}),\n",
       " '582cb756-d432-40f1-a973-85ebd484c3eb': Document(page_content='Cyber Law Vol. LVII] 199\\nnot strictly applicable. The court had the discretion to rely on the documents produced\\nif it was required to assist the court in ef fectively dealing with the dispute. The petitioner\\nhad wanted an expert opinion on the disputed conversation between the parties to the\\nproceedings, which was relevant under section 45 of the Evidence Act. The court\\nshould not preclude a party from adducing evidence that may be relevant in accordance\\nwith the Evidence Act to prove their case. Thus, the court below was wrong in\\ndisallowing the prayer sought for in the application.\\nThe court allowed the petitioner ’s application and set aside the impugned order .\\nIt directed the court below to summon Jeena along with the petitioner ’s power of\\nattorney holder and respondent no. 3 to record their voices. The recorded conversation\\nand CD will be sent to an examiner for electronic evidence opinion, with the petitioner\\nbearing the expenses.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 7}),\n",
       " '12fd3aee-6c0e-4f5e-8591-60876ab13333': Document(page_content='In M.P. Mathew v. Central Bur eau of Investigation27 the public prosecutor filed\\nan application to summon a witness to produce a certificate under section 65B of the\\nEvidence Act for certain documents already marked during the trial. The accused\\nchallenged this order but the special court allowed it. The court held that the certificate\\ncan be produced at any stage of the trial, but the rights of all parties must be balanced.\\nThe petitioner ’s senior counsel ar gued that they should be allowed to challenge the\\nadmissibility and marking of documents during the final hearing. The court dismissed\\nthe petition but allowed the petitioner to raise these contentions during the final hearing\\nof the case.\\nIn Rajendra Agrawal v.  State of Chhattisgar h28 the petitioner and co-accused,\\nboth, were char ge-sheeted for the aforesaid of fences under sections 500 read with\\nsection 120B of the IPC and 67 of the IT  Act. The court found that no of fense under', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 7}),\n",
       " '904f9b35-f0e0-4dbe-b58a-96f216ca6a73': Document(page_content='section 67 of the IT  Act was made out against the petitioner based on the contents of\\nthe FIR as it was neither found obscene nor lascivious. As a result, the char ge under\\nsection 67 of the IT  Act was quashed.\\nAdditionally , the certificate under section 65-B(4) of the Evidence Act was\\nmandatory to be filed with the char ge-sheet, which was not done in this case. The\\ncourt reiterated that the requirement of producing a certificate under section 65-B of\\nthe Evidence Act is mandatory in cases where secondary evidence is presented and\\noral evidence in the place of such certificate cannot possibly suf fice as section 65-\\nB(4) is a mandatory requirement of the law as established in the Anvar P .V. case. It\\nquoted ratio of the Supreme Court from Arjun Panditrao Khotkar29 to clarify the\\nposition of law on admissibility of electronic evidence under section 65B: 30\\n61. We may reiterate, therefore, that the certificate required under', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 7}),\n",
       " '657786e1-346b-4903-9f1c-02c9e8535170': Document(page_content='Section 65-B(4) is a condition precedent to the admissibility of evidence\\nby way of electronic record, as correctly held in Anvar P .V. (supra),\\nand incorrectly “clarified” in Shafhi Mohammed  (supra). Oral evidence\\n27 2021 SCC OnLine Ker 4035(Decided on Nov . 1, 2021).\\n28 2021 SCC OnLine Chh 903(Decided on Apr. 6, 2021).\\n29 Arjun Panditrao Khotkar  v. Kailash Kushanrao Gorantyal, supra  note 3 at para 22.\\n30 Ibid.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 7}),\n",
       " '8815ae43-95b6-407a-b1c3-4a45aca6d91d': Document(page_content='Annual Survey of Indian Law 200 [2021\\nin the place of such certificate cannot possibly suf fice as Section 65-\\nB(4) is a mandatory requirement of the law . Indeed, the hallowed\\nprinciple in Taylor v . Taylor , which has been followed in a number of\\nthe judgments of this Court, can also be applied. Section 65-B(4) of\\nthe Evidence Act clearly states that secondary evidence is admissible\\nonly if led in the manner stated and not otherwise. To hold otherwise\\nwould render Section 65-B(4) otiose.”\\nIII OBSCENITY -SECTION 67,67A\\nScope of pr ovisions-Interpr eting ‘sexually explicit’  act/conduct under  section 67A\\nand B\\nIn Sanjay Zacharias v. Stephen Geor ge31 the petitioner filed a petition under\\nsection 482 of the Criminal Procedure Code, 1973 to quash the proceedings initiated\\nagainst them based on a complaint and FIR filed by S tephen Geor ge, a former MLA.\\nThe petitioner , who is the General Secretary of the Kerala Congress (M) and a former', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 8}),\n",
       " 'ee8b7ff3-3e64-4948-bce8-e89f10282abb': Document(page_content='MLA, claims that the accused for ged electronic records containing sexually explicit\\nmaterials with the intent to defame prominent political figures. They ar gue that the\\nallegations are politically motivated and deny any association with the social media\\naccount in question. The petitioner ’s counsel disputes the application of section 67A\\nof the IT  Act, which is a non-bailable of fense, and ar gues that if any of fenses were\\ncommitted, they would fall under the bailable of fense of section 67. The petitioner\\nalso asserts that they are a victim of social bullying and harassment by political\\nopponents.\\nThe senior counsel for the petitioner ar gues that the decision in Majeesh K.\\nMathew  v. State of Kerala32 has little relevance to the current case as it involved\\ndifferent facts and emphasized that the facts of that case involved oblique utterances\\nmade against a woman through social media, whereas the current case does not involve\\nsimilar circumstances.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 8}),\n",
       " '23df8272-dfa3-47b1-8ec6-ac1fee82f077': Document(page_content='During the proceedings, the petitioner ’s counsel ar gued that the content in\\nquestion does not have a sexual tone and asserted that the petitioner is being falsely\\ninterpreted and harassed. However , the counsel for the first respondent ar gued that\\nthe of fense under section 67A  of the IT  Act is applicable, as the petitioner intended to\\ndefame a prominent political figure. The director general of prosecution also opposed\\nthe application, emphasizing the importance of understanding the content in its context.\\nThe court rejected the petitioner ’s counsel’ s interpretation, considering it distorted\\nand taken out of context to suit the petitioner ’s convenience.\\nThe court observed that the crucial point revolves around whether the petitioner\\nmade sexually explicit postings on social media, particularly a caricature and an image\\ndepicting a song. The court rejected the interpretation made by the petitioner ’s counsel,', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 8}),\n",
       " 'dbfd48a3-7f30-4471-828f-d915356775dd': Document(page_content='stating that every word should be understood in its context. The court considered the\\ninterpretation presented by the petitioner ’s counsel as distorted and taken out of context\\n31 2021 SCC OnLine Ker 13947(Decided on Nov . 25, 2021).\\n32 2018 (4) KHC 253.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 8}),\n",
       " '29e5f134-b10a-4c4a-b8a0-5da3c93e116d': Document(page_content='Cyber Law Vol. LVII] 201\\nto suit the petitioner ’s convenience. The settled proposition of law , as established by\\nthe Supreme Court in Devidas Ramachandra T uljapurkar  v. State of Maharashtra,33\\nemphasizes that an objective assessment must be made to determine whether the matter\\nin question is obscene. The court must consider contemporary community standards\\nand eliminate subjective elements or personal preferences. In this case, the expression\\n“KER_380669_6.png” is ar gued to have a sexually explicit tone, resembling the\\nopening lines of a Malayalam film song, which suggests oral sex. Therefore, it is\\ncontended that Section 67A  of the IT  Act is applicable, and the ar gument that it has\\nno application cannot be accepted.\\nMoreover , the court highlights that the petitioner ’s previous application for\\nanticipatory bail was dismissed, and the same ar guments cannot be used to quash the\\nproceedings. Ultimately , the court finds no merit in the application and dismisses it,', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 9}),\n",
       " '18f7e21e-9b7a-4e76-81db-61c092ba68ce': Document(page_content='allowing the proceedings to continue.\\nIn Suvojit Chowdhur y v. State of Maharashtra34 (with Sherlin Chopra v. State\\nof Maharashtra)  the applicants/accused sought protection from arrest for of fenses\\nunder sections 292 of the Indian Penal Code, sections 67 and 67A of the Information\\nTechnology Act, 2000, and sections 3 and 4 of the Indecent Women Representation\\nAct, 1986 related to broadcasting and exhibiting indecent videos, audio files, and\\nmessages containing sexually explicit content through Over -The-T op (OTT) platforms\\non the internet for illegal financial gains. Raj Kundra, the Director of Arms Prime,\\nwas implicated by co-accused for instigating them to act in obscene films. The\\napplicants did not cooperate in providing details about the creation of vulgar videos.\\nStatements of co-accused and witnesses established their involvement in video\\ngraphing and publishing objectionable obscene material on both free and paid apps,', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 9}),\n",
       " '03f6d396-a2e3-4fe4-ad9e-6d232ee04554': Document(page_content='satisfying the elements of the alleged of fense, particularly under section 67A. The\\nrequest for pre-arrest bail was rejected, but an ad-interim order of protection was\\nextended for four weeks from the date of the order .\\nIn another case Pramod Anand Dhumal v. State of Maharashtra ,35 The applicant,\\nan editor of a local Marathi newspaper and a social activist, sought pre-arrest bail for\\noffenses under section 354-D of the Indian Penal Code (IPC) and section 67A  of the\\nInformation Technology Act (IT  Act). The complainant had received of fensive and\\nsexually explicit messages with images from the applicant’ s cell phone on her Facebook\\naccount. Despite expressing her disinterest, the applicant continued to send obscene\\nmessages along with a hyperlink containing lascivious material. The complainant\\nfiled a complaint, resulting in the registration of a case against the applicant under\\nsection 354-D of the IPC and section 67A  of the IT  Act.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 9}),\n",
       " '293e1abf-2f42-41b9-8c43-76d46075fabd': Document(page_content='The court observed that the material sent by the applicant did not meet the\\ncriteria for “material containing sexually explicit acts” required by section 67A of the\\nIT Act. Instead, it fell under section 67, as it tended to excite lust but did not directly\\ndepict sexual activity in a detailed manner . Therefore, prima facie , the of fense may\\n33 (2015) 6 SCC 1.\\n34 2021 SCC On Line Bom 1 1930(Decided on Nov . 25, 2021).\\n35 2021 SCC OnLine Bom 34.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 9}),\n",
       " '0936d83a-92db-4ec7-9813-82ff76d206a2': Document(page_content='Annual Survey of Indian Law 202 [2021\\nattract section 67 and not section 67A  of the IT  Act. The court found the applicant\\nprima facie involved in the of fense of stalking under Section 354-D of the IPC, which\\nis bailable as a first of fense. Considering the evidence and the punishment prescribed\\nfor the of fense under section 67 of the IT  Act, the court granted pre-arrest bail to the\\napplicant, as custodial interrogation was not required for electronic evidence.\\n      A prayer was filed in Vijesh  v. State of Kerala36 to quash all proceedings\\nagainst the accused under sections 66(A) and 67(A) of the IT  Act, 2000, among others,\\nusing section 482 of the Criminal Procedure Code (Cr . PC). The second respondent,\\na lady , was the de facto  complainant in the case. According to the prosecution’ s case,\\nduring the inaugural function of a jewelry store, celebrities from television and cinema\\nwere invited, resulting in a crowd where people took photos with them on their mobile', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 10}),\n",
       " '9369b743-4b1a-4b38-95ed-7ab8fe121cb8': Document(page_content='phones. Subsequently , in January and March 2012, videos titled “Mallu Aunti\\nHarassed” and “Paravoor Peedanam” were uploaded on YouTube, allegedly containing\\nthe second respondent’ s photographs from the inauguration along with derogatory\\nremarks. The petitioner was accused of of fenses under sections 66(A) and 67(A) of\\nthe IT  Act.\\nHowever , it is important to note that section 66(A) of the IT  Act has been declared\\nunconstitutional by the Supreme Court in the case of Shreya Singhal  v. Union of\\nIndia.37 Therefore, the criminal proceedings related to the of fense under Section 66(A)\\nof the IT  Act cannot be sustained. The only remaining of fense alleged against the\\npetitioner is the one under section 67(A) of the IT  Act, which deals with punishment\\nfor publishing or transmitting sexually explicit material in electronic form.\\nAccording to section 67(A) of the IT  Act, the accused must have published or', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 10}),\n",
       " 'e2f29d2f-196e-4b5c-a70a-bb5cd43085f0': Document(page_content='transmitted sexually explicit material in electronic form. The alleged publication of\\nthe respondent’ s photograph during the jewelry store’ s inaugural function, as admitted\\nby the respondent, does not qualify as sexually explicit material.\\nThe prosecution’ s argument that uploading photographs of the respondent with\\nsexually colored remarks like “Mallu Aunti Harassed” and “Paravoor Peedanam” etc\\nfulfills the requirements of Section 67(A) of the IT  Act is invalid. The use of sexually\\ncolored remarks does not amount to the publication or transmission of sexually explicit\\nmaterial. The term “sexually explicit” has a specific meaning and does not include\\nnews or informational material. Since the of fense under section 66(A) of the IT  Act\\nhas already been declared unconstitutional, the continuation of criminal proceedings\\nagainst the petitioner is an abuse of the court process and should be quashed under\\nsection 482 of the Cr PC.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 10}),\n",
       " '01991e1c-208c-4680-addf-5b53b7d4ec7b': Document(page_content='The court concluded that the initiation and continuation of criminal proceedings\\nagainst the petitioner were an abuse of the court process. Exercising its inherent\\nextraordinary powers under section 482 of the Cr PC, the court ordered that the char ge\\nsheet against the petitioner/accused and all subsequent proceedings stemming from it\\nbe quashed and set aside.\\n36 2021 SCC OnLine Ker 854.\\n37 (2015) 2 KL T 1 (SC).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 10}),\n",
       " '199c4364-5873-4d7a-aef3-e359202b94d9': Document(page_content='Cyber Law Vol. LVII] 203\\nIn Imran Shabbir Gauri v. State of Maha rashtra38 the appellant, who was the\\nfather of the victim, had been convicted by the trial court for several of fences, including\\nthe possession of pornographic images of the victim on his mobile phone. Specifically ,\\nhe was found guilty under section 376(2)(i) and 506 of the Indian Penal Code (IPC),\\nas well as under section 4 of the Protection of Children from Sexual Of fences (POCSO)\\nAct, 2012. Additionally , he was convicted for the of fence punishable under section\\n67-B of the IT  Act, 2000, as he had obtained nude photographs of the victim on his\\nmobile handset on multiple occasions.\\nDuring the trial, the court took into consideration the evidence presented,\\nincluding the Forensic Science Laboratory (FSL) report that confirmed the presence\\nof pornographic images and video clips on the appellant’ s mobile phone. However ,\\nthe court expressed concerns about the evidentiary value of the FSL report, as it was', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 11}),\n",
       " 'd10643dc-18d6-43a6-8e1d-ee41e1d74115': Document(page_content='unclear whether it constituted substantive evidence or merely corroborative evidence.\\nThe court emphasized that the testimony of the individual who witnessed the incident\\nor the victim herself would be crucial as substantive evidence, while the recorded\\nmaterial stored on the memory card could serve as corroborative evidence.\\nWhile acknowledging the presence of pornographic images on the appellant’ s\\nmobile phone, the high court hesitated to establish a direct connection between those\\nimages and the victim due to a lack of identification. Although the forensic analysis\\nconfirmed the existence of pornographic content to some extent, the court found it\\nchallenging to attribute those specific images to the victim. Nonetheless, the appellant\\nwas convicted under section 67-B of the Information Technology Act, which pertains\\nto the depiction of sexually explicit acts involving children in electronic form. Even', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 11}),\n",
       " '94446410-5d06-4fb3-8956-6216e0db6484': Document(page_content='though there was no evidence of the appellant uploading or transmitting these images\\nto anyone else, the act of possessing or depicting children in an obscene, indecent, or\\nsexually explicit manner in electronic form is punishable under clause (b) of section\\n67-B. Therefore, the court upheld the conviction of the appellant under section 67-B\\nof the IT  Act, 2000.\\nSection 67 and liability of admin of WhatsApp gr oup\\nIn Kishor v. State of Maharashtra39 the Nagpur Bench of the High Court of\\nBombay recently examined the legal responsibility of a WhatsApp group administrator\\nin relation to objectionable content posted by group members. In this case, the applicant\\n(accused No. 2) filed an application to quash a char ge sheet and FIR filed against him\\nfor of fenses under sections 354-A(1)(iv), 509, and 107 of the IPC, 1860, and section\\n67 of the IT  Act, 2000.\\nThe allegations in the FIR stated that as an administrator of a WhatsApp group,', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 11}),\n",
       " '77b47ca3-ff81-48c8-bb7f-d4221a69f75d': Document(page_content='the applicant allowed another member (accused No. 1) to use of fensive language\\nagainst a non-applicant (No. 2) in th e group. It was further alleged that despite being\\naware of accused No. 1’ s actions, the applicant took no action against them, such as\\n38 2021 SCC OnLine Bom 51 1(Decided on Mar . 31, 2021).\\n39 2021 SCC OnLine Bom 654(Decided on Mar . 1, 2021).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 11}),\n",
       " '6b54cd4e-5f3c-4857-96a2-3330ef092c49': Document(page_content='Annual Survey of Indian Law 204 [2021\\nremoving them from the group or asking for an apology . The FIR was lodged by the\\nnon-applicant against both the applicant and accused no. 1.\\nTo address the issue of potential criminal liability of a WhatsApp group\\nadministrator , the court first examined the operational dynamics of WhatsApp. It\\nacknowledged that WhatsApp is an instant messaging platform that allows mass\\ncommunication through chat groups. The group administrator has the authority to\\nadd or remove members but does not possess the power to regulate or censor content\\nbefore it is posted. The court emphasized that individual members can be held liable\\nfor their own posts if they violate the law . Without specific provisions establishing\\nvicarious liability , an administrator cannot be held responsible for objectionable content\\nposted by group members. The court stated that establishing vicarious liability would\\nrequire demonstrating a common intention or pre-arranged plan between the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 12}),\n",
       " '87d0da8a-e1b2-48aa-a38a-c6857b8fe25c': Document(page_content='administrator and the group member involved. Merely being a group administrator\\ndoes not establish common intention, and it is unreasonable to expect administrators\\nto anticipate or have prior knowledge of the criminal actions of group members.\\nAdditionally , the liability of an administrator as a creator of objectionable content\\ndoes not apply in this case.\\nRegarding the of fense under section 67 of the IT  Act, the court analyzed the\\nspecific language of the section, which punishes the transmission or publication of\\nobscene material in electronic form. The allegations and evidence presented did not\\nsupport the claim that the applicant disseminated or caused the dissemination of any\\nlascivious or obscene material. Section 67 prescribes that an individual may be\\nsubjected to punishment for transmitting, publishing, or causing to be transmitted or\\npublished, any material that is obscene in electronic form. One could discern on a', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 12}),\n",
       " 'c3a4a9bb-14a7-4bf4-98eb-12afb6ffd63b': Document(page_content='careful analysis of the allegations in the FIR and the evidence presented in the form\\nof a char ge sheet that there is no claim or evidence that the applicant disseminated or\\ncaused to be disseminated any material in electronic form that is lascivious, appeals\\nto prurient interests, or is likely to corrupt or deprave individuals who may view , read,\\nor hear it. The definition of an intermediary also did not apply to the applicant, as\\nthere was no accusation of involvement in transmitting or receiving any record or\\nproviding related services.\\nAfter reviewing the material in the char ge sheet, it was clear that the essential\\nelements of the alleged of fenses were not disclosed. Continuing with the proceedings\\nagainst the applicant would amount to an abuse of the court process. As a result, the\\ncourt quashed the FIR, char ge sheet, and all proceedings against the applicant for\\noffenses under sections 354-A  (1)(iv), 509, and 107 of the IPC and section 67 of the\\nIT Act.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 12}),\n",
       " 'c3e106ed-ff25-4114-b0bd-32f408949886': Document(page_content='IT Act.\\nThe petitioner in Rajendra Agrawal  v. State of Chhattisgar h40 case sought the\\nquashing of char ges against them under section 67 of the Information Technology\\n(IT) Act and section 500 of the IPC through section 482 of the Cr PC. Their ar gument\\nwas that the WhatsApp messages allegedly sent by their co-accused on their behalf\\n40 2021 SCC OnLine Chh 903(Decided on Arp. 6, 2021).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 12}),\n",
       " '5abc32d4-e6bd-4a8b-b65e-68ca7129472c': Document(page_content='Cyber Law Vol. LVII] 205\\nwere not of an obscene or lascivious nature, and therefore, no of fence under section\\n67 of the IT  Act was established against them.\\nThe court carefully examined the provisions of section 67 of the IT  Act and\\ncompared them with sections 294 and 292(1) of the IPC, which deal with the concept\\nof obscenity . It noted that while there are similarities between these provisions, there\\nare also distinct dif ferences in their language and requirements. Specifically , in order\\nfor an act to fall within the scope of section 67 of the IT  Act, it must have the potential\\nto deprave and corrupt individuals who are likely , considering all relevant\\ncircumstances, to read, see, or hear the content in question.\\nTherefore, the court highlighted that the mere publication, transmission, or\\ncausing of publication or transmission in electronic form is not suf ficient to bring an\\nact within the purview of section 67 of the IT  Act. The content must possess a quality', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 13}),\n",
       " '2a41e531-6875-4869-92a4-77765fab275f': Document(page_content='that tends to deprave and corrupt individuals who are likely to come across it. This\\ndistinction is important in determining whether the alleged WhatsApp messages can\\nbe considered an of fence under section 67 of the IT  Act.\\nIn essence, the court emphasized that the content in question should have a\\npotentially harmful impact on the moral and ethical standards of the readers, viewers,\\nor listeners, taking into account all relevant factors. The petitioner ’s argument relied\\non the assertion that the WhatsApp messages did not meet this standard of obscenity\\nor lasciviousness required by section 67 of the IT  Act. Setting aside the FIR and the\\nconsequent criminal proceedings initiated against the petitioner , the court held: 41\\nThe words in the said WhatsApp message are not capable of arousing\\nsexual thoughts or feelings in the minds of the petitioner or respondent\\nNo. 2 or other four persons to whom the message has been sent by the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 13}),\n",
       " '31149ff4-c165-4384-b59f-08c2b554d0ff': Document(page_content='co-accused and it does not involve lascivious elements arousing sexual\\nthoughts or feelings or the words in the said message have no ef fect of\\ndepraving persons, and defiling morals by sex appeal or lustful desires,\\nthough the words may be extremely un parliamentary , unprintable and\\nabusive in nature, but it cannot be brought within the broad contours\\nof the penal provisions as contained in Sections 294 & 292 of the IPC\\ncorresponding to Section 67 of the IT  Act. Even according to the\\ncomplainant, it is only defamatory and as such, the ingredients of\\noffence under Section 67 of the IT  Act are not at all attracted.\\nViewing child pornography privately - an of fence?\\nIn P.G. Sam Infant Jones v. State42  the prosecution alleged that the petitioner\\naccessed, a M.E degree holder and pursuing Ph.D. at that time, downloaded, and\\nshared child pornographic material using an Airtel SIM card and his email and\\nFacebook accounts.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 13}),\n",
       " '53869b4c-551f-44bf-9c41-cce4aaed7f57': Document(page_content='Facebook accounts.\\nThe petitioner ’s counsel ar gued that the petitioner was present in the hostel\\nduring the relevant time and that the evidence provided thus far was insuf ficient to\\n41 Ibid.\\n42 2021 SCC OnLine Mad 2241(decided on June 1 1, 2021).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 13}),\n",
       " '68d4882a-218e-4f02-accb-753080c2f621': Document(page_content='Annual Survey of Indian Law 206 [2021\\nprove that the petitioner personally committed the alleged acts. Furthermore, there is\\nno evidence indicating that the content in question involved child pornography . The\\nact of viewing pornography in private does not typically constitute an of fense, as\\nthere is no specific provision in place that prohibits such private acts, the counsel for\\naccused ar gued.\\nWhile there are ar guments suggesting that child pornography falls under an\\nindividual’ s freedom of expression and privacy , it is important to highlight that child\\npornography is an exception to this principle. The high court observed that section\\n67-B of the Information Technology Act, 2000 deals with child pornography and\\nimposes penalties for various acts related to it. The provision covers publishing,\\ntransmitting, creating, collecting, seeking, browsing, downloading, advertising,\\npromoting, exchanging, or distributing material in any electronic form that depicts', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 14}),\n",
       " '89c81f13-0387-4d6f-8461-75c49ae0bd05': Document(page_content='children engaged in sexually explicit acts. It also includes activities such as cultivating,\\nenticing, or inducing children into online relationships for sexually explicit acts,\\nfacilitating online abuse of children, and recording one’ s own abuse or the abuse of\\nothers involving sexually explicit acts with children. Consequently , viewing child\\npornography is considered an of fense and is punishable under the law .\\nAfter considering the circumstances of the case, the court found that the incident\\noccurred nearly a year ago and it seems to be an isolated incident. Even the prosecution\\ndid not allege that the possession or transmission of the material was for commercial\\npurposes. The court made a distinction between individuals who consume child\\npornography on a one-time basis and those who actively transmit, distribute, or show\\nsuch material in the digital realm. It emphasized the seriousness of the issue and the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 14}),\n",
       " '928ce20b-7fed-4076-bd60-a0e2e1ef0e91': Document(page_content='need for a strong approach to combat child pornography . The court acknowledged\\nthat once someone enters the digital space, their activities can be monitored by either\\nthe government or the operators of social networking sites. It further highlighted that:43\\nIt is obvious that the moment one steps into digital space, one comes\\nunder the surveillance either of the S tate or those manning the social\\nnetworking sites. If one is zealous about privacy , the only option is to\\nstay outside such networks. Of course, in the current world, it is not a\\nviable option.\\nCourt mandated  both the central and state governments to raise awareness about\\nthe provisions of the POCSO Act under section 43, however , it was acknowledged\\nthat this alone may not be enough and emphasized that moral education is considered\\nto be the only ef fective solution to address this issue: 44\\n11…..It is only the Bharatiya culture that can act as a bulwark. The', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 14}),\n",
       " '690a2b4b-a39d-4e2a-a995-57b209de8ea0': Document(page_content='menace of child pornography can be tackled only if all of us inculcate\\nthe right values.\\n43 Ibid.\\n44 Ibid.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 14}),\n",
       " 'b5740858-6edc-4064-ab44-c0b249d8ce84': Document(page_content='Cyber Law Vol. LVII] 207\\nIV RIGHT  TO BE FORGOTTEN\\nS.K. Kaul  J.,in  K.S. Puttaswamy    v. UOI observed:\\nRight of an individual to exercise control over his personal data and to\\nbe able to control his/her own life would also encompass his right to\\ncontrol his existence on the Internet.\\nThe introduction of GDPR in the European Union triggered a discussion on\\nprivacy concerns in India and led lawmakers to consider the need for a data protection\\nframework. However , India presently lacks such a framework.  While some courts\\nhave recognized it as part of the right to privacy , others have rejected pleas for removal\\nof personal information due to the lack of legislative sanction. The Information\\nTechnology Act, 2000, which regulates the cyber world in India, does not mention the\\nright to be for gotten. The Supreme Court’ s landmark ruling in the case of K.S.\\nPuttaswamy45 however , established that the right to privacy includes the right to be', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 15}),\n",
       " '9a469966-d581-4105-a80a-b32169fb6d9e': Document(page_content='left alone, which is an essential aspect of an individual’ s privacy . Also the Indian\\nPersonal Data Protection Bill, 2019, does mention the right to erasure.\\nThe right to be for gotten is evolving in India and struggling to be considered a\\nfundamental right, but with increasing concerns about data privacy and exploitation,\\nit is a relief that can be claimed against illegal or unwanted sharing of personal\\ninformation only . It is important to legally recognize the right to be for gotten as a core\\npart of the right to privacy and a fundamental right.\\nIn Karthick Theodr e v. The Registrar General46 the petitioner was char ged with\\na criminal of fence under sections 417 and 376 of the IPC and the trial court found\\nhim guilty and punished him. The petitioner was acquitted of all char ges but his name\\nstill appears in the judgment as an accused. He approached the High Court of Madras\\nwith the request that his name be removed from the judgment as it harms his reputation,', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 15}),\n",
       " '0806aaf4-e16d-4779-85d9-e6a61a0d310d': Document(page_content='despite being acquitted.\\nThe high court agreed that an accused who has been exonerated of all char ges\\nhas the right to have their name redacted from records to safeguard their right to\\nprivacy . However , the court stated that the “right to be for gotten” cannot exist in the\\nadministration of justice and giving such a broad directive would open the floodgates\\nof demands. India does not have a system to erase an accused person’ s records once\\nthey have been acquitted, and only “The Juvenile Justice [Care and Protection of\\nChildren] Act, 2015” allows for such erasure. It was observed that:47\\n31......This Court honestly feels that our criminal justice system is yet\\nto reach such standards where courts can venture to pass orders for\\nredaction of name of an accused person on certain objective criteria\\nprescribed by rules or regulations. It will be more appropriate to await\\nthe enactment of the Data Protection Act and Rules thereunder , which', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 15}),\n",
       " '66e96565-37b3-4848-9e89-d1e46df3df9f': Document(page_content='may provide an objective criterion while dealing with the plea of\\n45 (2017) 10 SCC 1.\\n46 2021 SCC OnLine Mad. 2755\\n47 Ibid.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 15}),\n",
       " '13f2cc0e-bcc2-4927-8dc0-8ab29b0704ab': Document(page_content='Annual Survey of Indian Law 208 [2021\\nredaction of names of accused p ersons who are acquitted from criminal\\nproceedings. If such uniform standards are not followed across the\\ncountry , the constitutional courts will be riding an unruly horse which\\nwill prove to be counterproductive to the existing system.\\nThe high court decided that it cannot issue a broad order to redact names from\\ncourt records without appropriate statutory backing. The court felt that a proper policy\\nmust be established to prevent confusion when carrying out such an exercise. The\\ncourt concluded that without clear guidelines, such a broad order could lead to many\\ncomplications and that the government must create a statutory framework for such a\\npolicy .\\nIn Jorawer Singh Mundy v. Union of India48 the petitioner , an American citizen\\nof Indian origin with a background in real estate, filed a petition to remove a judgment\\nCustom  v. Jorawar Singh Mundy  49 from online platforms like Google, Indian Kanoon,\\nand Vlex.in.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 16}),\n",
       " '0639fc1f-b606-4495-864e-b509fbce8775': Document(page_content='and Vlex.in.\\nThe petitioner , Jorawer Singh, an American citizen of Indian origin was char ged\\nunder the Narcotics Drugs and Psychotropic Substances Act (NDPS Act) in India\\nduring a visit in 2009. He was later acquitted of all char ges by the trial court and the\\nHigh Court of Delhi. However , he faced dif ficulty finding employment due to the\\nonline availability of the judgment regarding his involvement in the drug case on\\nplatforms such as Google, Indian Kanoon, and vLex.in. He filed a writ petition under\\narticle 226 of the Indian Constitution before the High Court of Delhi, requesting the\\nplatforms to take down the judgment as it violated his right to privacy under article 21\\nof the Constitution. The petitioner issued legal notices to the aforementioned platforms.\\nVlex.in claimed to have removed the judgment, but it remained available on other\\nplatforms.\\nThe central question in this case was\\ni.whether the right to privacy under article 21 of the Indian Constitution includes', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 16}),\n",
       " 'b0661022-9586-4c98-8d67-3fa323090c7f': Document(page_content='the right to be for gotten, and\\nii.whether a court has the authority to order the removal of information from\\nonline platforms?\\nThe High Court of Delhi had to balance the right to privacy against the right to\\ninformation available to the public and maintenance of transparency in judicial records.\\nWhile the right to privacy is recognized as a fundamental right, the right to be for gotten\\nis not explicitly mentioned in the Indian Constitution. However , in some cases,50 courts\\nhave recognized the right to be for gotten as a part of the right to privacy . It cited\\nZulfiqar Ahman Khan  v. Quintillion Businessman Media Pvt. Ltd.,51 where this court\\nhad held as un der: 52\\n48 2021 SCC OnLine Del 2306.\\n49 Crl.A. No. 14/2013.\\n50 Karthick Theodr e v. Registrar General\\n51 2021 SCC OnLine Mad. 2755 , and Subhranshu Rout v . State of  Odisha  2020 SCC OnLIne\\nOri. 878\\n52 2019 SCC OnLine Del. 8494.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 16}),\n",
       " '0dc6803b-9ff9-4044-b631-5ff038d9139f': Document(page_content='Cyber Law Vol. LVII] 209\\n8. In fact, it is the submission of ld. Counsel for the Plaintif f that the\\nPlaintif f’s personal and professional life has been hampered irreparably\\nand further damage is likely to be caused if appropriate relief is not\\ngranted against the republication of these two articles. The original\\npublisher having already agreed to pull down the same, this Court\\nhaving directed that the same ought not to be republished, the Plaintif f,\\nthus, has a right to ensure that the articles are not published on multiple\\nelectronic/digital platforms as that would create a permanent\\natmosphere of suspicion and animosity towards the Plaintif f and also\\nseverely prejudice his personal and professional life. The printouts of\\nthe articles from www .newsdogapp.com, which have been shown to\\nthe Court, leave no doubt in the mind of the Court that these are identical\\nto the articles published on www .thequint.com, which has already been\\npulled down.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 17}),\n",
       " 'b3605af1-462b-407b-98a9-12172896d1cf': Document(page_content='pulled down.\\n9. Accordingly , ecognizing the Plaintif f’s Right to privacy , of which\\nthe ‘Right to be for gotten’  and the ‘Right to be left alone’  are inherent\\naspects, it is directed that any republication of the content of the\\noriginally impugned articles dated 12 October 2018 and 31 October\\n2018, or any extracts/or excerpts thereof, as also modified versions\\nthereof, on any print or digital/electronic platform shall stand restrained\\nduring the pendency of the present suit.\\n10. The Plaintif f is permitted to communicate this order to any print or\\nelectronic platform including various search engines in order to ensure\\nthat the articles or any excerpts/search results thereof are not republished\\nin any manner whatsoever . The Plaintif f is permitted to approach the\\ngrievance of ficers of the electronic platforms and portals to ensure\\nimmediate compliance of this order .\\nThe court opined that the petitioner may face irreversible harm to his social life', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 17}),\n",
       " '76b294ab-9a03-4b5d-900b-5d89e4cb3725': Document(page_content='and career prospects, despite being acquitted in a case. The court held that the petitioner\\nis entitled to interim protection while the legal issues are pending and accordingly\\ndirected Google and Google LLC to remove the judgment titled Custom  v. Jorawar\\nSingh Mundy53 from their search results and India Kanoon to block access to the said\\njudgment through search engines.\\nIn X  v. YouTube,54 the plaintif f, a popular Bengali film actor , was promised the\\nlead role in a web series by Ram Gopal Verma S tudios. She participated in a\\ndemonstration video which included explicit scenes of complete nudity , but the project\\nwas later shelved. However , the producer uploaded the video to his YouTube channel\\nand website, and although he removed it upon the plaintif f’s request, others uploaded\\nit to dif ferent websites without her consent. The plaintif f applied to the court seeking\\ninterim protection and a takedown of the video due to the violation of her privacy ,', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 17}),\n",
       " 'afb87447-a0a7-46d0-a44e-49a672c45694': Document(page_content='53 Id. Cited at para 9 in Jorawer Singh Mundy v . Union of India\\n54  Crl.A. No. 14/2013.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 17}),\n",
       " '03906eac-fa6b-478f-9ce4-c13ba89de165': Document(page_content='Annual Survey of Indian Law 210 [2021\\ndamage to her reputation, and harassment she faced as a result. The defendants included\\nwebsites, internet service providers, and search engines.\\nThe plaintif f argued that the right to privacy includes the right to be for gotten,\\nwhich has been recognized by the Indian Supreme Court and several high courts. She\\nalso cited Rule 3(2)(b) of the Information Technology (Intermediary Guidelines and\\nDigital Media Ethics Code) Rules, 2021, which requires intermediaries to remove or\\ndisable access to content that exposes an individual’ s private areas within 24 hours of\\nreceiving a complaint. The defendants, including websites, ISPs, and search engines,\\nwere thus obliged to take measures to remove the suit videos. The plaintif f also cited\\na High Court of Delhi decision where Google was directed to remove URLs/websites\\nunder an interim order . The plaintif f argued that the three-part test for an interim', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 18}),\n",
       " 'fe0debd1-2c15-49e2-90b4-20ae01273c6c': Document(page_content='injunction was satisfied, and the court should issue such an order against the defendants.\\nDefendants Google relying on Karthick Theodr e v. Registrar General,55 and\\nSubhranshu Rout  v. State of Odisha56 argued that they were not under any obligation\\nto prevent the republication of the Suit Videos since they were unaware of any\\nagreement permitting the broadcast. They also ar gued that the plaintif f had no valid\\nstatutory protection to enforce the right to be for gotten and that the plaintif f should\\nhave approached the publishing platforms instead of the search engine defendant.\\nThey relied on case law showing that courts had rejected the disabling of search\\nresults in the manner sought by the plaintif f. Lastly , they ar gued that the plaintif f had\\nconsented to the filming of the videos, and Rule 3(2)(b) of the Rules 2021 required\\nthe victim or an authorised representative to complain to the intermediary , which was', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 18}),\n",
       " '226a5c31-f566-4b77-914b-8d8a18377c61': Document(page_content='not satisfied in the present case. They further submitted that Rule 3(3)(b) should be\\nread alongside Sections 67 and 67A  of the Information Technology Act, 2000, which\\nexcluded material published in the interest of science, literature, art or learning or\\nother objects of general concern.\\nThe Court found that the explicit nature of the Suit Videos fell under Rule\\n(3)(2)(b) of the Rules 2021, and rejected the defendants’  argument that the plaintif f’s\\nconsent to filming barred her from legal recourse. The Court drew parallels between\\nthis case and Zulfiqar Ahman Khan  v. Quintillion Business Media (P) Ltd .,57 which\\nillustrated the severe impact of publication on personal and professional life. The\\ncourt found that the plaintif f’s right to privacy should be protected, given the explicit\\nnature of the videos and the impact on her reputation. While neighbouring high courts\\nhad found no statutory right to be for gotten, the court endorsed the right to be for gotten', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 18}),\n",
       " 'e9b1a263-d4bb-48e7-8848-0179cb2366d3': Document(page_content='and the right to be left alone as inherent aspects of the right to privacy .\\nCourt granted interim relief to the plaintif f, finding that the suit videos were of\\nan explicit nature and that their circulation had a clear and immediate impact on the\\nplaintif f’s reputation. The court rejected the defendants’  arguments that the plaintif f\\nhad consented to the filming of the videos and that she had no valid statutory protection\\n55 2021 SCC OnLine Del 4193(Decided on Aug. 23, 2021).\\n56 2021 SCC OnLine Mad 2755.\\n57 (2020) SCC Online Ori 878).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 18}),\n",
       " '98f0abd1-d028-41b5-86aa-249884b0177c': Document(page_content='Cyber Law Vol. LVII] 211\\nto enforce her right to be for gotten. The court found her consent to have since been\\nexpressly withdrawn, as the producer of the series had also removed the videos upon\\nher request and held that the plaintif f’s right to privacy should be protected. It therefore\\npassed an interim order directing the defendants to take down all the suit videos from\\ntheir websites, channels, digital platforms, and search engines and to stop uploading,\\npublishing, streaming, transmitting, broadcasting, or communicating the videos to\\nthe public. The defendants were given 36 hours to comply with the order , and the\\nplaintif f was given the right to communicate the order to any other platforms found to\\nbe publishing, streaming, or transmitting the suit videos.\\nV INTERMEDIAR Y LIABILITY -SECTION 79\\nThe issue of intermediary liability has been a subject of uncertainty since the\\nintroduction of the Information Technology Act in 2000. In recent years, intermediaries', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 19}),\n",
       " '86b70e84-e9ae-4290-befe-d4347daee6ef': Document(page_content='have become increasingly significant due to the widespread use of social media\\nplatforms for communication and information sharing. The emer gence of digital media\\nhas also made it a mainstream concern, leading the government to focus on regulating\\nthese platforms. The 2021 Rules represent an initial ef fort to regulate such platforms.\\nThe 2021 IT Rules\\nIn February 2021, the Indian government introduced new regulations for social\\nmedia platforms, digital news media, and other online content providers called the\\nInformation Technology (Intermediary Guidelines and Digital Media Ethics Code)\\nRules, 2021 (IT  Rules 2021). These rules aim to hold intermediaries more accountable\\nto both internet users and the Indian government. They require social media platforms\\nand messaging apps to appoint Indian residents as grievance of ficers, compliance\\nofficers, and nodal of ficers and to remove content within 36 hours of receiving a', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 19}),\n",
       " 'f9a0175e-22dd-4781-b1a7-d5f6da7f9126': Document(page_content='legal order or court directive and maintaining records of all removed content for a\\nminimum of 180 days.\\nThere are also specific rules for publishers of news and current af fairs content\\nand online curated content. Significant Social Media Intermediaries (SSMI) in\\ncomparison to Social Media Intermediaries (SMI) need to observe additional due\\ndiligence requirements and comply with stringent residency requirements for\\ncompliance of ficers and nodal contact persons.  The rules empower the government\\nto direct intermediaries and publishers to delete, modify , or block content, either\\nthrough a grievance procedure or through emer gency blocking orders passed without\\na hearing.\\nHowever , the rules have faced criticism from human rights groups and digital\\nrights advocates who ar gue that they could be used to suppress free speech and\\nexpression. The rules have also been challenged in petitions filed in dif ferent high\\ncourts including the High Court of Bombay , Kerala, Delhi, and Madras.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 19}),\n",
       " '27c39841-5990-412c-bce0-91309d640c69': Document(page_content='Rule 4(2) of the IT Rules 2021 requires SSMI providing messaging services to\\nenable the identification of the first originator of information on their computer . This\\nprovision has been challenged by companies like WhatsApp on the grounds that it\\ninfringes upon users’ fundamental right to privacy and freedom of speech. Rule 4(4),\\nwhich requires the use of technology-based measures to proactively identify certain', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 19}),\n",
       " '4db5ad5c-ac0b-44ae-91e4-df17a4948815': Document(page_content='Annual Survey of Indian Law 212 [2021\\ntypes of content, raises concerns about the accuracy and consequences of fully\\nautomated tools.\\nIn the case of Live Law Media Pvt. Ltd. v. Union of India,58 the High Court of\\nKerala has passed an interim order directing that no coercive action be taken against\\nLive Law , under Part III of the IT  Rules 2021 (dealing with digital media), as Live\\nLaw is a publisher of law reports and legal literature.\\nIn Foundation for Independent Journalism  v. Union of India59 and Sanjay Kumar\\nSingh v. Union of India,60 High Court of Delhi directed Central Government to file a\\nreply . In another petition before High Court of Kerala titled Praveen Arimbrathodiyil\\nv. Union of India61 where a free and open source software (FOSS) developer who\\nfiled a petition against India’ s IT Rules 2021 has claimed that the regulations unfairly\\nburden small-scale FOSS developers and communities. The petitioner has ar gued', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 20}),\n",
       " '20e07228-9ed4-433c-b03e-7e6101892243': Document(page_content='that the rules’ content moderation requirements could weaken data security and privacy\\nmeasures, ultimately infringing on the right to freedom of trade and profession under\\narticle 19(1)(g) of India’ s constitution. The government filed a transfer petition under\\narticle 139A(1) of the Constitution, seeking a transfer of the four petitions mentioned\\nabove, on the ground that they are substantially similar to justice for rights foundation,\\ninitiated long before the government notified IT Rules, 2021.62\\nTwo petitions Agij Pr omotion of Nineteenonea Media Pvt. Ltd. v. Union of\\nIndia63  and Nikhil Mangesh W agle v. Union of India64 were filed in High Court of\\nBombay to challenge the IT  Rules 2021 on the ground that they are ultra vir es the\\nInformation Technology Act, 2000( ‘IT  Act’) and the provisions of articles 14, 19(1)(a)\\nand 19(1)(g) of the Constitution and go beyond the restrictive ambit of section 69A of', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 20}),\n",
       " '6da36bc8-d8f4-4495-87e2-bdca67eb49f2': Document(page_content='the IT  Act. 2021 Rules have a terrible chilling ef fect in their applicability to the internet\\nas they bring about a manifestly unreasonable and an arbitrary regime amounting to\\nan af front to the constitutional guarantee of right of citizens to exercise freedom of\\nfree speech and expression. The Central Government justified the Rules by stating\\nthat they aimed to create a level playing field between online and of fline publishers\\nand combat fake news.\\nThe High Court of Bombay granted an interim stay on Rules 9(1) and 9(3) of\\nthe IT  Rules 2021, stating that they are ultra vir es the IT  Act. Bench stated that Rule\\n9 of the IT Rules, 2021 appeared to infringe upon the constitutional guarantee of\\nfreedom of speech and expression, as enshrined in article 19(1)(a). This infringement\\nwas evident in the fact that the rule subjected publishers of news and current af fairs\\ncontent and online curated content to action under the Press Council Act, 1978 and', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 20}),\n",
       " '2aa33f35-ba3e-41dc-8c06-ecfb0a220b11': Document(page_content='the Cable TV Networks Regulation Act, 1995 which already had their own independent\\n58 2019 SCC Online Del 8494.\\n59 W.P.(C) 6272 of 2021.\\n60 W.P.(C) 3125 / 2021.\\n61 W.P.(C) 3483 of 2021.\\n62 Praveen Arimbrathodiyil v. Union of India  WP (C) 18084/2021).\\n63 Union of India v. Sudesh Kumar Singh , Transfer Petition (C) No. 100-105/2021.\\n64 2021 SCC OnLine Bom 2938.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 20}),\n",
       " 'bdfcdcbe-f726-4292-ae2f-d4ff1edd59f8': Document(page_content='Cyber Law Vol. LVII] 213\\nmechanisms for dealing with violations, and a subordinate legislation like Rule 9\\ncould not disrupt or override the powers granted by those laws.65\\n The court also noted that Rule 14, which deals with the formation of an inter -\\ndepartmental committee, and Rule 16, which deals with blocking information during\\nemer gencies, do not require immediate action. However , Rule 9 imposes an obligation\\non publishers to adhere to a Code of Ethics that is not part of the IT  Act and may\\npreclude them from criticizing public figures, which the court found problematic as it\\ngoes beyond the powers laid down in section 69A  of the IT  Act:66\\n28. “Dissent in democracy is vital. It is, however , the checks and\\nbalances that make a democracy work. There can be no two opinions\\nthat a healthy democracy is one which has developed on criticism and\\nacceptance of contra views. Opinion based on criticism reinforces its', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 21}),\n",
       " 'a15cda45-d950-49f8-926e-1e9969f70a0d': Document(page_content='acceptance in a democratic society . For proper administration of the\\nState, it is healthy to invite criticism of all those who are in public\\nservice for the nation to have a structured growth but with the 2021\\nRules in place, one would have to think twice before criticizing any\\nsuch personality , even if the writer/editor/publisher may have good\\nreasons to do so without resorting to defamation and without inviting\\naction under any other provision of law . Allowing the operation of the\\n2021 Rules in its form and substance to operate would result in the\\nwriter/editor/publisher standing the risk of being punished and\\nsanctioned, should the inter -departmental committee be not in favour\\nof criticism of any public figure. It is, therefore, quite possible that the\\nwriter/editor/publisher on contravention of the provisions of clause\\n(1) of Rule 9 of 2021 Rules, but without even transgressing the\\nboundaries set by clause (2) of Article 19 of the Constitution, may', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 21}),\n",
       " '3c4da384-ac53-44a8-a67c-fb669e8375f1': Document(page_content='expose himself/itself to punishment/sanction under the 2021 Rules.\\nThe indeterminate and wide terms of the Rules bring about a chilling\\neffect qua the ight of freedom of speech and expression of writers/\\neditors/publishers because they can be hauled up for anything if such\\ncommittee so wishes. The 2021 Rules are, thus, manifestly unreasonable\\nand go beyond the IT  Act, its aims and provisions.\\n29. A democracy would thrive only if the people of India regulate their\\nconduct in accordance with the preambular promise that they took while\\ngiving to themselves the Constitution. Liberty of thought is one of\\nsuch promises. Exercising this liberty , expressions take shape. Should\\nat least a part of Rule 9 of the 2021 Rules be not interdicted even at the\\ninterim stage, it would generate a pernicious ef fect. As it is, the constant\\nfear of being hauled up for contravention of the Code of Ethics is a\\ndistinct possibility now . People would be starved of the liberty of', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 21}),\n",
       " '8881cfc5-5e7d-408d-ba64-8d7d8a236785': Document(page_content='thought and feel suf focated to exercise their right of freedom of speech\\n65 Public Interest Litigation (L) No. 14204 of 2021.\\n66 Agij Pr omotion of Nineteenonea Media Pvt. Ltd. v . Union of India, supra note 63  at para 31.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 21}),\n",
       " '3de645e3-5f9e-401f-8183-9153d9f381c7': Document(page_content='Annual Survey of Indian Law 214 [2021\\nand expression, if they are made to live in present times of content\\nregulation on the internet with the Code of Ethics hanging over their\\nhead as the Sword of Damocles. This regime would run clearly contrary\\nto the well-recognized Constitutional ethos and principles.”67\\nThe High Court of Bombay therefore, stayed the operation of sub-rules (1) and\\n(3) of Rule 9. However , the court did not stay Rule 7 of the 2001 Rules as the petitioner\\nhad not demonstrated that they were an intermediary as defined under section 2(w) of\\nthe IT  Act. The court emphasized that Rule 9 was an exception to the general\\npresumption of subordinate legislation’ s constitutionality and did not comply with\\nthe IT  Act’s provisions or the constitutional rights guaranteed under article 19(1)(a).\\nFinally , the court emphasized that subordinate legislation could not transgress the\\npowers occupied by dif ferent statutes.68', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 22}),\n",
       " '04e236c3-7d68-45aa-aa9b-e90fce0c7b33': Document(page_content='In yet another petition titled Digital News Publishers Assn. v. Union of India69\\nbefore High Court of Madras filed by TM Krishna and Digital News Publishers\\nAssociation (DNP A) against the IT  Rules 2021, which is similar to the High Court of\\nBombay case.70 The court noted that the sub-rules (1) and (3) of Rule 9 of the IT\\nRules 2021 have already been stayed by the High Court of Bombay , and this stay\\norder should have a pan-India ef fect, so there was no need for an independent order .\\nHowever , the petitioners ar gued that they had received notices requiring them to comply\\nwith the IT  Rules and Rule 9. The digital news platforms expressed concern over the\\nthree-tier grievance redressal mechanism, which gives excessive power to government\\nofficials to punish them. This mechanism involves self-regulation by publishers in\\nthe first level, self-regulating bodies established by publishers in the second level,', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 22}),\n",
       " '1eb323d0-c28e-41eb-bd26-f77f6d1a9a00': Document(page_content='and oversight by the Central Government in the third level. The petitioners were\\nspecifically concerned about sub-clause (x) of Rule 3(1)(b) which states that:71\\n“(x) is patently false and untrue, and is written or published in any\\nform, with the intent to mislead or harass a person, entity or agency for\\nfinancial gain or to cause any injury to any person;”\\nThe petitioners ar gued that this provision, along with the requirement\\nfor intermediaries to terminate access or usage rights for non-\\ncompliance72, and the strict grievance redressal mechanism, creates\\nundue pressure on intermediaries. Additionally , Rule 7 makes\\nintermediaries liable for punishment if they fail to comply with the\\naforementioned rules.\\nSection 79 of the Information Technology Act provides protection to\\nintermediaries from liability in certain cases. However , this exemption would not\\napply if the intermediary does not observe the guidelines prescribed by the Central\\n67 Ibid. at para 28\\n68 Ibid.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 22}),\n",
       " '42781f9d-a01b-4573-a9bf-39c0c795bf50': Document(page_content='68 Ibid.\\n69 Id . at para 31.\\n70 2021 SCC OnLine Mad 16337(Decided on Aug. 14, 2021).\\n71 Agij Pr omotion of Nineteenonea Media Pvt. Ltd. v. Union of India, supra note 63.\\n72 R. 3(1)(b).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 22}),\n",
       " 'd8ba83ba-714c-4892-8e23-60357c35e988': Document(page_content='Cyber Law Vol. LVII] 215\\nGovernment. In Shreya Singhal v. Union of India,73 it was observed that any unlawful\\nacts beyond what is laid down in article 19(2) of the Constitution cannot form any\\npart of section 79 of the Act. The Supreme Court has acknowledged in the judgment\\nthat it would be challenging for intermediaries such as Google and Facebook to\\ndetermine the legitimacy of the millions of requests they receive.\\nThe High Court of Madras noted that there is a “substantial basis” for assertions\\nthat Rule 9 violates article 19(1)(a) of the Constitution and may be applied to\\nintermediaries coercively . In accordance with this, the Madras High Court on 16th\\nSeptember 2021 issued an interim order that any action under Rules 3 and 7 would be\\nsubject to the outcome of the challenge of constitutional validity as the main matter\\nwas likely to be taken up by the Supreme Court in coming days. Pursuant to requests', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 23}),\n",
       " 'e3097308-b16c-4bed-9e7d-2f8966b5870b': Document(page_content='from the Central Government, the Supreme Court has transferred cases for regulation\\nof content on OTT  Platforms pending in dif ferent high courts to the Supreme Court,\\nand has passed orders prohibiting the relevant high courts from hearing these cases\\nwhile they are pending before the Supreme Court.74\\nIn the case of Omanakuttan K.G . v. Union of India,75 the petitioner filed a writ\\npetition in the public interest, seeking various reliefs, including mandamus or other\\nappropriate writs to compel WhatsApp to comply with the IT  Rules, 2021 and to\\nprohibit the reliance on WhatsApp contents by investigating agencies and courts. The\\npetitioner raised concerns about user manipulations, lack of security , traceability of\\nmessages, and compliance with the IT  Rules, 2021, citing the right to privacy .\\nWhatsApp ar gued that it is not bound by the IT  Rules, 2021 due to end-to-end\\nencryption and that the Rules infringe upon the right to privacy .', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 23}),\n",
       " '5f3db916-023e-45d6-b5e0-40ccba639324': Document(page_content='The court, considering the provisions of the Rules, observed that the petitioner\\nmade vague allegations without providing supporting evidence. It noted that the\\ngovernment has already made provisions in the Rules to regulate and control the\\nplatform in question. The fact that the Rules are being challenged in the High Court\\nof Delhi does not automatically entitle the petitioner to the mandamus requested in\\nprayer .\\nThe court refused to grant the petitioner ’s requests for directions to investigating\\nagencies or courts to not rely on WhatsApp contents in their functioning, as it would\\ninterfere with the statutory framework of the criminal justice system, and the registrar\\ngeneral does not possess supervisory powers in this regard.\\nThe court found that the petitioner disregarded core judicial aspects and sought\\ndirections that exceeded the comprehension of the Constitution and laws concerning\\nthe issue of end-to-end encryption. It deemed it premature to issue the mandamus as', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 23}),\n",
       " 'b7d1bf5d-832a-4c1f-9c31-a29e68059d27': Document(page_content='sought by the petitioner and concluded that the petitioner failed to demonstrate any\\narbitrariness or illegality on the part of the respondents, justifying judicial review .\\nAccordingly , the court dismissed the writ petition, and the petitioner ’s request for a\\nwrit of prohibition was deemed unwarranted.\\n73 R. 3(1)(c), 2021 IT Rules.\\n74 (2015) 5 SCC 1.\\n75 Union of India  v. Sudesh Kumar Singh , Transfer Petition (C) No. 100-105/2021).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 23}),\n",
       " '848e81ab-1181-4959-bf1b-9c4681524300': Document(page_content='Annual Survey of Indian Law 216 [2021\\nRemoval of content posted unlawfully fr om pornographic websites and de-\\nindexing fr om sear ch engine r esults\\nThe High Court of Delhi in X v. Union of India76 heard a case involving a\\nwoman (referred to as X) whose photos were posted on a pornographic website without\\nher consent. Despite court orders to remove the content, it continued to resurface on\\nthe internet. The court appointed an advocate as amicus curiae to address the issue.\\nThe Delhi Police requested directions to intermediaries, under sections 79(3)(a) and\\n(b) of the IT  Act, to remove and prevent the posting of unlawful content and to share\\nactual unlawful content, metadata, data dump, and basic subscriber information for\\ninvestigation purposes.  Google ar gued that search engines are not publishers but only\\nindex existing information, and they should only remove specific URLs upon request.\\nThey emphasized the need to consider context and avoid pre-emptive banning of', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 24}),\n",
       " 'fa123f88-d042-4603-bd02-4db014138d98': Document(page_content='content as it would jeopardize free speech and be contrary to the law quoting para 62\\nand 66 from Myspace.77 ISPAI stated that blocking content at the sub-page level is\\ntechnically challenging due to encryption mechanisms and suggested global source\\nblocking at the content provider level. The Ministry of Electronics and Information\\nTechnology suggested granting petitioners the right to request content removal. The\\ncourt discussed the extraterritorial jurisdiction of the IT  Act, the responsibilities of\\nintermediaries, and the exemptions (as outlined in section 79(1) and Rule 10 of the\\n2009 Rules) and liabilities (sections 67, 67A, and 67B) outlined in the Act and the\\n2021 IT  Rules. The court emphasized the need for intermediaries to expeditiously\\nremove or disable access to of fending content upon receiving ‘actual knowledge’  as\\nheld in Shreya Singhal  case.\\nThe court appointed amicus curiae78 highlighted the relevant provisions of the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 24}),\n",
       " 'd2ba99ce-8a19-4986-8dee-8225a3156881': Document(page_content='Information Technology Act, 2000, as amended, and the associated rules, including\\nthe 2021 Rules, which have increased the liabilities and obligations of intermediaries\\nin dealing with unlawful content. The 2021 Rules have set a shorter timeframe of 24\\nhours for removing or disabling access to such content. Failure to comply with these\\nrules can result in the revocation of the intermediary’ s liability exemption. The amicus\\ncuriae  also referenced legal precedents from both Indian and foreign jurisdictions\\nthat address intermediary liability and the responsibility to remove unlawful content.\\nThe court proposed directions to ensure the ef fective removal of unlawful content\\nwhile balancing the obligations of intermediaries and the rights of victims.79 These\\ndirections include immediate content removal within 24 hours of a court order ,\\npreservation of information related to the content, de-indexing by search engines,', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 24}),\n",
       " '8abe8cca-7b17-4333-b02f-68b28603ee82': Document(page_content='proactive monitoring by intermediaries, information sharing with law enforcement,\\nremoval from other platforms upon request, filing complaints on the National Cyber -\\nCrime Reporting Portal, and potential liability for non-compliance with court orders.\\nThese measures aim to strike a fair balance and facilitate meaningful compliance\\nwithout placing undue burden on intermediaries.\\n76 2021 SCC OnLine Ker 2758(decided on June 28, 2021).\\n77 2021 SCC OnLine Del 1788.\\n78 Myspace Inc  v. Super Cassettes Industries , 2017 (69) PTC 1 (Del) (DB).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 24}),\n",
       " 'd6e3b03f-4627-4521-a54a-c12cc936d06a': Document(page_content='Cyber Law Vol. LVII] 217\\nThe court ordered the petitioner to provide information to the Investigating\\nOfficer within 24 hours. The Delhi Police/CyP AD Cell was instructed to remove/\\ndisable access to the content within 24 hours. Search engines were directed to de-\\nindex the content globally and disable access to identical content on other platforms\\nwithin 24 hours. The investigating of ficer was tasked with sharing relevant URLs\\nwith other entities. The Delhi Police was instructed to obtain necessary information\\nfrom websites and search engines. The petitioner can request removal of similar content\\nfrom other platforms, with corresponding directions to the investigating of ficer. Non-\\ncompliance will result in loss of exemption and liability under the IT  Act. Parties can\\nseek clarification from the court if needed.\\nIntermediary liability on e-marketplaces\\nThe central issue in Kunal Bahl v. State of Karnataka80 case was whether an', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 25}),\n",
       " '61cf1c2e-abf0-4855-8bfd-a61577ddf525': Document(page_content='intermediary (the online marketplace www .snapdeal.com here) would be held liable\\nfor the sale of drugs that did not comply with the requirements under the Drugs and\\nCosmetics Act, 1949. The complaint was filed by the Inspector of Drugs based on\\ninformation received from the Deputy Drugs Controller , Mysore. It alleged that a\\nseller on Snapdeal’ s platform had sold SuHAGRA-10P  tablets. Since Snapdeal did\\nnot possess a license to sell drugs, it was accused of violating section 18(c) of the\\nDrugs and Cosmetics Act, 1940, which is punishable under section 27(b)(ii).\\nSnapdeal ar gued that it has fulfilled its obligations as an intermediary under the\\nInformation Technology Act, 2000 and the Intermediaries Guidelines Rules, 201 1. It\\nclaimed exemption from liability under section 79 of the Act for the following reasons:\\ni.Snapdeal had no involvement in the specific transaction in question.\\nii.Snapdeal merely provides a platform for communication and information sharing', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 25}),\n",
       " '5a43daed-2e89-4e32-8d39-5185000ff453': Document(page_content='between sellers and buyers. The information about the products of fered for sale\\nby the accused seller was made available on Snapdeal’ s online marketplace.\\niii.As an intermediary , Snapdeal does not have control over the content posted by\\nusers on its platform.\\niv.Snapdeal has exercised “due diligence” as required by Section 79(2)(c) of the\\nInformation Technology Act, 2000, along with the Intermediaries Guidelines\\nRules, 201 1.\\nSnapdeal ar gues that as an intermediary , its liability under section 79(3)(b) of\\nthe Information Technology Act, 2000 is limited to the removal of third-party content\\nupon receipt of a court order or notice from a government authority . It cannot be held\\nresponsible for the listing and sale of products by independent third-party sellers on\\nits marketplace. Snapdeal cited the decisions in Sharat Babu Digumar ti v. Govt. (NCT\\nof Delhi)81 and Shreya Singhal  v. Union of India82 to support its position.\\n79 Supra  note 76.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 25}),\n",
       " '0fd29ead-be20-4108-aa86-5ddb999e7b0f': Document(page_content='79 Supra  note 76.\\n80 X v. Union of India   supra  note 76  at para 90\\n81 2021 SCC OnLine Kar 15706(decided on Jan. 7, 2021).\\n82 (2017) 2 SCC 18.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 25}),\n",
       " 'f75d1e96-3419-4549-b922-79741afff4a2': Document(page_content='Annual Survey of Indian Law 218 [2021\\nSnapdeal also pointed out that the Consumer Protection (E-Commerce) Rules,\\n2020 have introduced a distinction between marketplace e-commerce websites (like\\nSnapdeal, Amazon, and Flipkart) and inventory e-commerce websites (such as Lifestyle\\nand Decathlon). Rule 5(1) of the Consumer Protection (E-Commerce) Rules, 202083\\nstates that in order to claim exemption under section 79 of the Information Technology\\nAct, 2000, marketplace e-commerce entities like Snapdeal must comply with the\\nrequirements of subsections (2) and (3) of section 79, as well as the Information\\nTechnology (Intermediaries Guidelines) Rules, 201 1.84\\nThe High Court of Karnataka ruled that an intermediary , as defined in section\\n2(w) of the IT  Act, along with its directors and of ficers, cannot be held responsible\\nfor any action or inaction taken by a vendor or seller utilizing the services provided\\nby the intermediary through a website or marketplace.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 26}),\n",
       " '11f0cff3-e6e5-4c7c-9556-f7fd9cd07578': Document(page_content='Court extensively discussed various provisions of the Cr PC, Information\\nTechnology Act, and Drugs and Cosmetics Act. In the context of Section 18(1)(c) of\\nthe Drugs and Cosmetics Act, 1940, it is essential for an individual to engage in\\nactivities such as manufacturing, distributing, stocking, exhibiting, or of fering for\\nsale without possessing a valid license issued under the Act. Therefore, neither Snapdeal\\nnor its directors can be held liable for an of fense punishable under section 27(b)(ii) of\\nthe Act.\\nIt concluded that no of fense was established against the accused, leading to the\\nallowance of the petitions and the quashing of the criminal proceedings initiated against\\nthe accused in question.\\nSnapdeal/accused no.2 cannot be held responsible for the sale of non-compliant\\nitems under the Drugs and Cosmetics Act, 1940. The court found no of fense and\\nquashed the criminal proceedings against Snapdeal and its directors.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 26}),\n",
       " '5bd8e04d-c4ef-4e4e-ba82-282f168a126f': Document(page_content='In Sanatan Sanastha   v. Union of India,85 a registered public charitable trust,\\nclaiming to be a Non-Governmental Or ganization (NGO), had filed a petition against\\nFacebook (respondents no. 3 and 4). The petitioner alleged that their Facebook pages,\\nwhich were used to spread spiritual teachings, had been blocked by the respondents\\nwithout providing any reasons or government orders. The petitioner ar gued that this\\naction was arbitrary , violated their constitutional rights, and constituted an unauthorized\\nexercise of power . During the proceedings, the petitioner received a communication\\nfrom the respondents, citing their right to permanently disable accounts that breached\\nFacebook’ s community standards. The petitioner amended the petition to include this\\ncommunication and referred to a civil suit in the High Court of Delhi (CS (OS) 510 of\\n83 (2015) 5 SCC 1.\\n84 5(1) Liabilities of marketplace e-commer ce entities . -', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 26}),\n",
       " '531acb3b-1a41-4c38-a41b-d958308c466f': Document(page_content='(1) A marketplace e-commerce entity which seeks to avail the exemption from liability under\\nsub-section (1) of section 79 of the Information Technology Act, 2000 (21 of 2000) shall comply\\nwith sub-ss. (2) and (3) of that section, including the provisions of the Information Technology\\n(Intermediary Guidelines) Rules, 201 1.\\n85 Ibid.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 26}),\n",
       " 'ed7f14e4-18d5-428e-b254-2433be3d184e': Document(page_content='Cyber Law Vol. LVII] 219\\n2016) filed by Sasikala Pushpa  v. Facebook .86 Additionally , the petitioner challenged\\nthe constitutionality of section 79 of the Information Technology Act, 2000, alleging\\na violation of fundamental rights under articles 14, 19, and 21 of the Constitution.\\nThe relief sought in this petition could not be granted because the constitutional\\nvalidity of section 79 of the Information Technology Act had already been upheld by\\nthe Supreme Court in the case of Shreya Singhal . It was well-established that once\\nthe Supreme Court upheld the constitutional validity of a provision, the high court\\ngenerally could not entertain a petition questioning the same provision based on new\\nor rephrased grounds. Therefore, there was no basis for granting relief as requested in\\nprayer clause D-1 of the petition.\\nThe relief sought in prayer clause D-2 of the petition was also unclear . Declaratory\\nrelief could not be granted in a vacuum, and there was no ongoing proceeding where', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 27}),\n",
       " '17fb79c1-d948-4394-8c1a-5cb623763396': Document(page_content='the respondents had claimed or been granted immunity based on the provisions of\\nsection 79 of the Information Technology Act. If there had been a breach of a contractual\\nrelationship between the petitioner and the respondents regarding the blocking of the\\npetitioner ’s Facebook page, the appropriate course of action would have been for the\\nrespondents to seek redress through the appropriate forum. A petition under article\\n226 of the Constitution of India might not have been the appropriate remedy in such\\na situation. Therefore, the petition was dismissed with no order as to costs.\\nV IDENTITY  THEFT - SECTION 66C\\nSection 66C of the IT  Act provides for the punishment of identity theft. It\\nstipulates that individuals who intentionally and dishonestly use someone else’ s\\nelectronic signature, password, or other unique identification feature to deceive or\\ndefraud others can be sentenced to a maximum imprisonment of three years and may\\nalso face a fine of up to 1 lakh Rs/-.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 27}),\n",
       " 'de8391dd-17cc-44c0-841e-74652b0027ed': Document(page_content='In Ravari Kirankumar v. Home Depar tment87 application was filed by the\\napplicant under section 482 of the Cr PC, seeking the quashing of FIR and the\\nsubsequent char ge-sheet claiming that the applicant has committed of fenses punishable\\nunder sections 66A  and 66C of the IT  Act, 2000. However , upon careful examination\\nof the complaint, FIR, and char ge-sheet, it is evident that the accusations primarily\\nrevolve around the applicant sending objectionable emails from their email ID to\\nvarious of fices. Notably , there are no allegations regarding the use of electronic\\nsignatures, passwords, or any other unique identification features of another person.\\nTherefore, there is no valid basis for registering an FIR under section 66C of the IT\\nAct.\\nMoreover , the court observed that section 66A  of the IT  Act has been deemed\\nunconstitutional by the Supreme Court in the case of Shreya Singhal  (supra) therefore,', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 27}),\n",
       " '5edbfb36-a6a2-4a5b-bd4f-ed2b302c68e2': Document(page_content='no prosecution can be maintained under section 66A  of the IT  Act. Therefore, the FIR\\nand char ge-sheet invoking section 66A  of the IT  Act must be quashed and set aside.\\nConsidering the absence of any allegations regarding fraudulent or dishonest use of\\n86 (2021)  SCC OnLine Bom 1049.\\n87 2020 SCC OnLine Del 618 (Decided on June 2, 2020).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 27}),\n",
       " 'd63ae1ac-4e6a-4874-9088-e81299ef955b': Document(page_content='Annual Survey of Indian Law 220 [2021\\nelectronic signatures, passwords, or unique identification features of another person,\\nit is perplexing how any prosecution can be pursued under section 66C of the IT  Act.\\nUpon examining the complaint, FIR, and char ge-sheet, it becomes apparent\\nthat the sole allegations against the applicant involve the sending of objectionable\\nemails on November 18, 2010 from their email ID, alert aa@redof fmail.com, to various\\noffices of NSSO (FOD) in India. These allegations were likely made to invoke the\\nprovisions of section 66A  of the IT  Act, which was applicable at the time. However ,\\nthroughout the entire complaint, FIR, and char ge-sheet, there are no accusations\\npertaining to the use of electronic signatures, passwords, or any other unique\\nidentification features of another person. Consequently , there was no justification for\\nregistering an FIR under section 66C of the IT  Act.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 28}),\n",
       " '3e187ce0-14c0-45a1-be4f-c3d8b9be81c3': Document(page_content='Therefore, it is imperative to quash the FIR and char ge-sheet since the allegations\\nin the FIR do not establish the commission of any of fense under section 66C of the IT\\nAct. Additionally , even if we assume the allegations to be true, they do not constitute\\nan of fense or establish a case against the applicant under section 66C of the IT  Act. It\\nis evident that the allegations do not fulfill the requirements for an of fense under\\nsection 66C of the IT  Act. Moreover , considering that section 66A  has already been\\nstruck down and cannot be invoked, the FIR and char ge-sheet must be quashed under\\nthese circumstances.\\n      The appellant in Santosh  v. State of Madhya Pradesh88 case has been\\nconvicted under section 66C of the Information Technology Act, 2000. The conviction\\nwas based on the appellant’ s alleged involvement in sending fraudulent emails using\\na forged email ID. The complainant, G .B. Bamankar , filed a complaint stating that an', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 28}),\n",
       " '9f811a39-ff81-48d5-8a2d-3382b00732d2': Document(page_content='email was forwarded to him by Ashish Dongare, which was sent from Pankaj Kanthed’ s\\nemail ID. However , Pankaj Kanthed denied sending the email and claimed that his\\nemail ID had been fraudulently created by someone else.       Based on the complaint,\\nan FIR was registered under various sections of the Indian Penal Code (IPC) and the\\nIT Act. During the investigation, it was discovered that the appellant, Santosh Bharti,\\nwas the one who sent the email in question. The appellant was acquitted of some\\ncharges but convicted under section 66C of the IT  Act. Aggrieved by the conviction,\\nthe appellant has filed this appeal.\\n      The appellant’ s counsel contended that the prosecution’ s case suf fered from\\na critical flaw: the absence of proper evidence. They ar gued that the email in question,\\nwhich formed a crucial part of the prosecution’ s case, had not been certified in\\naccordance with Section 65-B of the Evidence Act. This section stipulates that', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 28}),\n",
       " '4845e2a2-38c9-4e0b-95d4-5082420091a1': Document(page_content='electronic records must be supported by a certificate to be admissible as evidence in\\ncourt. In the absence of such certification, the email (Ex.P/2) could not be considered\\nreliable or valid evidence. Upon careful examination of the record, the court concurred\\nwith the appellant’ s counsel. It observed that the email in question was merely a\\nphotocopy of the forwarded email sent to the complainant, and this photocopy (Ex.P/\\n2) was not accompanied by the required certificate under section 65-B. The court\\nemphasized that the absence of certification was fatal to the prosecution’ s case, as it\\n88 2021 SCC OnLine Bom 4086.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 28}),\n",
       " '8e8a7c63-2f08-43ff-a37f-28a554f9b6b3': Document(page_content='Cyber Law Vol. LVII] 221\\nundermined the authenticity and admissibility of the email as per Supreme Court\\nruling in Anwar .\\n      Furthermore, the court noted that the prosecution’ s witness, P .W.3 Ritesh\\nSingh, a constable in the cyber cell, did not provide any substantial testimony regarding\\nthe email (Ex.P/2). His examination-in-chief remained silent on this crucial document,\\nand although he was not cross-examined on the matter , the court deemed it\\ninconsequential. The primary responsibility of the prosecution was to establish the\\nadmissibility and authenticity of the evidence, which they failed to do. The court\\nreferred to the Supreme Court’ s decision in the case of Anvar P .V., which emphasized\\nthat a mere printout or photocopy of an electronic record cannot be admitted as evidence\\nwithout a certificate under section 65-B.\\n       Considering the lack of certification for Ex.P/2, the court concluded that', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 29}),\n",
       " '3540098b-585c-430c-b8c0-dd6581be1151': Document(page_content='the prosecution had not succeeded in proving the appellant’ s guilt beyond a reasonable\\ndoubt. It further criticized the trial court for failing to address this crucial aspect and\\nproceeding to decide the case on its merits, which was deemed improper .\\nIn light of these findings, the court allowed the appeal, set aside the impugned\\njudgment, and acquitted the appellant. It deemed it unnecessary to delve into the\\nother grounds raised by the appellant, as the lack of certification alone was suf ficient\\nto undermine the prosecution’ s case and warrant the appellant’ s acquittal.\\nVII ONLINE PRIV ACY\\nIn the case of Manohar Lal Sharma  v. Union of India,89 the Supreme Court of\\nIndia ordered an independent investigation into unauthorized surveillance using the\\nPegasus software. A committee comprising three technical experts90 was appointed to\\nprobe the matter , assess the tar geting of Indian citizens’  devices, review the software’ s', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 29}),\n",
       " '0b087864-8845-4608-b268-1fb9b8bd6ecc': Document(page_content='acquisition, and provide recommendations for strengthening cyber security and privacy\\nprotection. Raveendran J., oversaw the committee’ s functioning, ensuring adherence\\nto procedures and thorough investigations. The court emphasized the importance of\\nprivacy , constitutional restrictions, and the balance between national security and\\nindividual rights. It expressed concerns about the potential impact on free speech and\\nthe need to protect democratic values in the face of emer ging technologies and\\nsurveillance practices.\\nThe court acknowledged privacy limitations but emphasized that restrictions\\nmust align with the constitution. It rejected the government’ s request for unrestricted\\nimmunity in the name of national security . The court expressed concerns about the\\ninfluence of surveillance on free speech and the potential for self-censorship. It\\nunderscored the importance of privacy , constitutional restrictions, and balancing', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 29}),\n",
       " '2a88a319-22c4-49f6-a17b-132c191c771d': Document(page_content='national security with democratic values in the face of emer ging technologies and\\nsurveillance practices.\\n89 2021 SCC OnLine MP 686.\\n90 2021 SCC OnLine SC 985(Decided on Oct. 27, 2021).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 29}),\n",
       " 'a53fbb56-5d35-4866-ac18-04fa97f002c4': Document(page_content='Annual Survey of Indian Law 222 [2021\\nV CONCLUSION\\nOver the past decade, the internet has become widely accessible in India, leading\\nto a significant increase in digital connectivity and the growth of the digital economy .\\nThe COVID-19 pandemic further accelerated the use of social media platforms and\\ninformation sharing. However , this resulted in several challenges like spread of fake\\nnews causing riots and mob lynching and the stifling of voices against oppression.\\nLack of transparency and accountability in dealing with malicious content on social\\nmedia platforms added to the rising discontent among society . Additionally , the rise\\nof OTT platforms for entertainment has highlighted the need for content regulation.\\nIn 2021, several noteworthy events shaped the landscape of cyber law in India.\\nWhatsApp’ s privacy policy revision faced backlash for allowing the sharing of sensitive\\npersonal data, while the Indian government introduced the Information Technology', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 30}),\n",
       " '0b1aebf3-e10a-4451-a271-6343803ff750': Document(page_content='(Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021 under section\\n87 of the Information Technology Act, 2000. These rules aimed to increase\\naccountability for social media platforms, establish a self-regulatory framework, and\\naddress concerns regarding digital content and OTT platforms.\\n The Rules dif ferentiate between social media intermediaries and significant\\nsocial media intermediaries, aiming to promote innovation and facilitate new platform\\ngrowth. Significant social media intermediaries have additional due diligence measures\\nand face criminal consequences for non-compliance. These Rules address concerns\\nabout digital content on digital media and OTT  platforms. The Ministry of Information\\nand Broadcasting oversees these issues, while the Information Technology Act governs\\nthe regulatory framework. The Rules establish a self-regulatory framework and a Code\\nof Ethics for news publishers and OTT  platforms. A three-tier grievance redressal', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 30}),\n",
       " 'c4984341-2a4c-4c3a-b993-5d3d13933dd0': Document(page_content='mechanism is in place. However , concerns have been raised about potential threats to\\npress freedom and media due to the allocation of adjudicatory powers to the executive\\nbranch.\\nIn 2020, the government issued several content takedown and blocking orders\\nwithout providing adequate explanations. The 2021 IT  Rules now require reasons for\\ntakedowns to be discussed and of fer a grievance redressal mechanism to challenge\\ngovernment actions. Intermediaries have greater responsibilities, including due\\ndiligence, monitoring, and user education. The Rules aim to foster a culture of self-\\nregulation among social media intermediaries, supported by artificial intelligence tools.\\nHigh courts received multiple public interest litigations challenging the\\nconstitutionality of certain provisions in the IT  Rules, 2021. The High Court of Mumbai\\ngranted a stay on the rules related to digital publishers, stating that Rule 9 goes beyond', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 30}),\n",
       " '47aa2150-d085-4bc3-b79d-e56abd32fe91': Document(page_content='the scope of the IT  Act. Specifically , Rule 9(1) and Rule 9(3) were stayed as they were\\ndeemed to exceed delegated power and infringe upon the constitutional right to freedom\\nof speech and expression. This decision had a pan India ef fect, being accepted by\\nother high courts.\\nThe X v. Union of India  judgment is praised for its lucid exposition of the\\nprocedure and guidelines for intermediaries and government agencies to remove\\noffensive content from digital platforms in accordance with the IT  Rules, 2021. The', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 30}),\n",
       " '7e083247-6d90-4792-96a3-d1b2ba2fdae0': Document(page_content='Cyber Law Vol. LVII] 223\\njudgment addressed concerns of individuals facing victimization through the posting\\nof obscene content about them online. While the victim’ s photographs taken from her\\nsocial media accounts were not obscene, their unauthorized posting on a pornographic\\nwebsite made them of fensive by association.\\nThe landmark judgment in Arjun Panditrao Khotkar  v. Kailash Kushanrao  by\\nthe Supreme Court was believed to have settled the jurisprudence on the admissibility\\nof electronic evidence, restoring Anwar ruling. However , the current stance of the\\nSupreme Court suggests that WhatsApp messages may not be considered admissible\\nevidence due to concerns about their authenticity and potential tampering. The court\\nhas expressed reservations about the evidential value of such messages, considering\\nthat they can be easily created, modified, or deleted by anyone. Nevertheless, with the\\nexistence of the Information Technology Act, 2000 and the continuous advancements', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 31}),\n",
       " '5dac5324-d6ad-42ef-a1e4-6f42839e4a57': Document(page_content='in the field, it is anticipated that there will be significant progress in the regulatory\\nframework regarding electronic evidence in the future.\\nIn the ongoing process of finalizing the long-awaited data protection legislation,\\nprogress has been made with the submission of the Joint Parliamentary Committee’ s\\nreport on the Personal Data Protection Bill, 2019 to both Houses of Parliament. The\\ncommittee has endorsed the bill with certain observations, suggesting the need to\\nbroaden its scope to include non-personal data. It has also supported exemptions for\\ncertain government agencies from the applicability of the data protection law on\\nspecified grounds, despite dissenting opinions expressing concerns about constitutional\\nviolations and the potential creation of separate ecosystems. The report is currently in\\nthe hands of the Indian government, and it remains to be seen how the government\\nwill incorporate the committee’ s recommendations into a revised version of the Data', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 31}),\n",
       " 'b9deb31a-c3b8-4dd6-ba1b-9544af4fe1f1': Document(page_content='Protection Bill.91\\nThe Pegasus controversy of 2021 sparked substantial attention and legal\\ndiscourse in Indian cyberspace. The Supreme Court’ s response,92 which recognized\\nthe right to privacy as a fundamental right and called for independent investigations,\\nelicited contrasting viewpoints. Critics ar gue that the court’ s actions may encroach\\nupon the executive’ s authority over national security and surveillance. They raise\\nconcerns about the court’ s lack of technical expertise in appointing an investigative\\ncommittee and the potential ramifications for counterterrorism ef forts, including\\noperational challenges and delays. Nonetheless, this judicial decision empowered the\\ncourts to scrutinize matters of national security and public interest, thereby contributing\\nto the ongoing development of cyber legal jurisprudence in India.\\nWhile the Pegasus controversy did generate significant attention and legal', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 31}),\n",
       " 'c0d5e5fb-c2f2-4e0e-ae10-83544df102de': Document(page_content='discussions in Indian cyberspace in 2021, it is important to note that the response\\nfrom the Supreme Court and acknowledgement of the right to privacy as a fundamental\\n91 The committee consisted of three technical experts: Naveen Kumar Chaudhary(Dean of National\\nForensic Sciences University ,Gujarat) , Prabaharan P  (Professor at Amrita Vishwa Vidyapeetham,\\nKollam, Kerala) and Ashwin Anil Gumaste(Associate Professor at the Indian Institute of\\nTechnology , Bombay).\\n92 Pawan Duggal.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 31}),\n",
       " 'a0fedaca-f7e3-4656-8de1-1b5d5c119e86': Document(page_content='Annual Survey of Indian Law 224 [2021\\nright and its order for independent investigations93 could be seen as encroachment\\nupon the executive’ s domain of national security and surveillance. Concerns could be\\nraised about the court’ s lack of technical expertise in appointing a committee to\\ninvestigate the Pegasus software and the possible implications for counterterrorism\\nefforts, including delays and operational challenges. However , this decision empowered\\nthe courts to examine matters of national security and public interest but also\\ncontributed to the ongoing development of cyber legal jurisprudence in India.\\nIn 2021, a significant development emer ged with the introduction of the crypto\\ncurrency and Regulation of Of ficial Digital Currency Bill, 2021. The primary aim of\\nthis bill is to establish a comprehensive framework for the of ficial digital currency\\nissued by the Reserve Bank of India. Additionally , the bill seeks to prohibit the use of', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 32}),\n",
       " 'd2df061f-fe8e-4170-8337-71bfdd3e7959': Document(page_content='private crypto currencies in India, while allowing certain exceptions to support crypto\\ncurrency technology . Its overall objective is to bring consistency and regulation to the\\ncrypto currency market, replacing private crypto currencies with an of ficial digital\\ncurrency and enforcing stringent penalties for non-compliance. The implementation\\nof ef fective regulation would not only facilitate taxation of crypto currency revenue\\nbut also generate advantages for both the government and investors. These\\nadvancements set the stage for future changes and progress in the dynamic field of\\ncyber law , warranting careful observation.\\n93 Manohar Lal Sharma  v. Union of India   Civil/Criminal Jurisdiction Writ Petition No. 314 of\\n2021\\n94 Manohar Lal Sharma  v. Union of India   Civil/Criminal Jurisdiction Writ Petition No. 314 of\\n2021', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 32}),\n",
       " 'd31f1f84-bd1a-4b3c-9622-ee17e1a3e8f8': Document(page_content='Annual Survey of Indian Law 192 [2021\\n9\\nCYBER LA W\\nDeepa Kharb*\\nI INTRODUCTION\\nCYBER LA W, a swiftly progressing field that intersects with numerous conventional\\nlegal disciplines, has under gone substantial transformations since 2014.This survey\\nexamines the evolving landscape of cyber law by analyzing the judicial decisions in\\n2021. It delves into crucial areas such as online privacy , data protection, cybercrimes,\\nand electronic evidence, providing valuable insights into the development of cyber\\nlaw in India. This survey serves as a practical guide for navigating the intricate\\nchallenges of the digital realm. Additionally , it presents a critical perspective on the\\ncourt’ s reasoning, identifying points that may be subject to further debate. Overall,\\nthe survey underscores the dynamic nature of cyber law and its significant impact on\\nthe legal framework, reflecting the judiciary’ s efforts to adapt to the challenges posed\\nby the digital age.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 0}),\n",
       " '986d9a09-c9d6-4760-9e6b-047ea4501d42': Document(page_content='by the digital age.\\nII ADMISSIBILITY  OF ELECTRONIC EVIDENCE: SECTION 65B IEA\\nThe introduction of sections 65A  and 65B in the Evidence Act in 2000 provided\\na framework for the admissibility of electronic evidence, with section 65B (1) allowing\\nfor the admissibility of a paper printout of information contained in electronic records\\nsubject to the conditions specified in section 65B(2).\\nHowever , the requirement for a certificate under section 65B of the Indian\\nEvidence Act for electronic records to be admissible in court has been a matter of\\ndebate among legal scholars and courts. While the Supreme Court in Anvar P .V. v.\\nP.K. Basheer1 (Anvar  hereinafter) held that such records cannot be admitted as\\nsecondary evidence unless the requirements of section 65B are met, the court in Shafhi\\nMohammad 2 (Shafhi  hereinafter) concluded that the certificate requirement may be\\nwaived wherever the interest of justice so justifies say when the electronic device', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 0}),\n",
       " '12f6f330-2faf-4cfd-88bb-c8f562b0d46c': Document(page_content='storing the records is inaccessible or  where the electronic device is produced by a\\nparty who is not in possession of such device, as a result of which such party would\\nnot be in a position to secure the requisite certificate.\\n* Assistant Professor , The Indian Law Institute, New Delhi.\\n1 (2014) 10 SCC 473.\\n2 (2018) 2 SCC 801 (decided on Apr. 25, 2018).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 0}),\n",
       " 'e16e34d9-9c2e-4768-b6d2-571bf0c21dbb': Document(page_content='Cyber Law Vol. LVII] 193\\nIn a recent decision, Arjun Panditrao Khotkar  v. Kailash Kushanrao Gorantyal3\\n(Arjun Panditrao Khotkar hereinafter), a three-judge bench4 of the Supreme Court\\nclarified that the certificate required under section 65B(4) is a prerequisite for the\\nadmissibility of electronic evidence. The bench af firmed the correctness of the Anvar\\nruling in its interpretation, while finding that the Shafhi decision’ s division bench\\nhad erroneously “clarified” the requirement. The court also noted that the certificate\\nis not necessary if the original electronic record is produced in court, however ,\\ncompliance with section 65B is compulsory before a ‘computer output’, which is\\nconsidered secondary evidence of an electronic record, can be admitted as evidence.\\nThe court stated that if a person refuses to provide the certificate required under\\nsection 65B (4) of the Indian Evidence Act, a party can make an application to the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 1}),\n",
       " 'f8e1495c-4ada-4596-814e-6147a323b6c3': Document(page_content='judge requesting the production of the certificate. The court also explained that if it is\\nimpossible for the person to provide the certificate, or if the law excuses the person\\nfrom doing so, then the party should be excused from the mandatory requirement of\\nsection 65B (4).5 The court instructed trial courts to summon the person(s) specified\\nin section 65B (4) when a defective certificate is given or when a certificate is refused,\\nand require them to provide the necessary certificate. The court clarified that since\\nsection 65(B) does not talk about the stage at which such certification can take place,\\nthis is subject to the discretion exercised by the courts in civil cases, and in criminal\\ntrials, the accused must be supplied with all documents that the prosecution seeks to\\nrely upon before the trial. The courts must balance the rights of the parties while\\nexamining any application by the prosecution under sections 91 or 31 1 of the Criminal', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 1}),\n",
       " 'a9d1eaa1-49a5-44ba-be72-660bc1476291': Document(page_content='Procedure Code, 1973 or section 165 of the Evidence Act, ensuring no serious or\\nirreversible prejudice to the accused.\\nAlthough the relaxation of the strict requirements under section 65B (4) was\\naimed at easing the burden on parties who have made best ef forts to obtain a certificate\\nbut failed to do so, it has been ar gued that such an exception goes beyond what the\\nstatute permits and creates further ambiguity . Furthermore, the obligation on the courts\\nto summon the authorized person(s) to produce the certificate could result in a\\nprolonged mini-trial within the trial, adding to the already overburdened judicial system\\nand causing delays and additional expenses for the parties involved.\\nAfter the Arjun Panditrao Khotkar6 case, various high courts in India have\\nfollowed its ratio in their respective judgments especially on the exception created.\\nThey have held that electronic evidence must be accompanied by a certificate under', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 1}),\n",
       " '95373454-c569-4baf-a5cc-cc87061be6e8': Document(page_content='section 65B of the Indian Evidence Act to be admissible in court. Failure to comply\\nwith this requirement results in the electronic evidence being inadmissible.\\n3 2020 SCC OnLine SC 571(decided on July14, 2020).\\n4 Bench consisting of RF Nariman, S. Ravindra Bhat, and V. Ramasubramanian\\n5 Due to the applicability of the Latin maxims ‘lex non cogit ad impossibilia’  (the law does not\\ndemand the impossible) and ‘impotentia excusat legem’  (when there is a disability that makes\\nit impossible to obey the law , the alleged disobedience of the law is excused).\\n6 Supra note 3; 2020 SCC OnLine SC 571(decided on July14, 2020).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 1}),\n",
       " '14826907-6add-4530-95fd-2f90b371b838': Document(page_content='Annual Survey of Indian Law 194 [2021\\nIn Rakesh Kumar Singla  v. Union of India7  the petitioner , under the NDPS Act,\\nfiled a bail petition claiming they were unlawfully detained as no contraband was\\nfound in their possession at the time of arrest. The Narcotics Control Bureau (NCB)\\npresented statements from a co-accused and the petitioner as evidence. However , the\\ncourt emphasized that the determination of the petitioner ’s guilt or innocence should\\nbe based on the evidence presented during the trial. The NCB opposed the bail\\napplication, citing WhatsApp chat screenshots as evidence connecting the petitioner\\nto the illicit drugs. Nevertheless, the court stated that WhatsApp messages cannot be\\nconsidered as valid evidence without a certificate under section 65B of the Indian\\nEvidence Act, as per the recent Supreme Court judgment in the case of Arjun Panditrao\\nKhotkar  v. Kailash Kushanrao Gorantyal.8 Therefore, the court concluded that the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 2}),\n",
       " '7061dcd5-dd74-4702-b98a-2e4ba015f40b': Document(page_content='WhatsApp messages, in their current state, hold no evidentiary value. Investigating\\nagencies may rely on WhatsApp messages during a crime investigation, but a certificate\\nunder section 65B of the Indian Evidence Act is necessary for their admissibility .\\n In Mahendra N. Par deshi v. State of Maharashtra9 the prosecution attempted\\nto prove the content of a DVR as evidence in a bribery case under section 7 and\\n13(1)(d) read with 13(2) of Prevention of Corruption Act, 1988, but failed to produce\\nthe original DVR or a certificate under section 65-B(4) of the Indian Evidence Act.\\nThe court held that verbal evidence about the contents of an electronic record is\\nconsidered secondary evidence and the prosecution must prove that the contents of\\nthe DVR were heard. The court found that the prosecution failed to provide evidence\\nthat the contents of the DVR were heard and the witness could not confirm the\\nconversation’ s content. Additionally , since the record was deleted, the court drew an', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 2}),\n",
       " 'fbe5355b-91cf-45f4-b2b1-679b322426d8': Document(page_content='adverse inference against the prosecution and dismissed the case.\\nIn Yogesh Arun W akure v. State of Maharashtra ,10 the appellant, an accused\\nfacing the char ge of murder punishable under section 302, 201, 323, 143, 147, 149\\nread with section 135 of the IPC with others, tried to establish his presence inside the\\nhotel on the basis of the CCTV  footage. However , an eye-witness claimed to have\\nseen the present appellant at the crime spot around the time of murder .\\nIt was contended from the appellant side that the trial court should not have\\ndisregarded the IO’ s report based on call detail records (CDR) and subscriber detail\\nrecords (SDR) without considering section 65B and relied solely on the word of mouth\\nof the eye-witness  as it is often said that “humans may lie, but documents would not\\nlie” or “documents would speak louder than words”. The apex court has held in Anvar11\\nthat electronic records must be produced according to section 65B, after which their', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 2}),\n",
       " '2893fff5-ebe5-4c1b-9326-6234fc2a9e26': Document(page_content='genuineness can be questioned and resort can be made to section 45A of the Evidence\\nAct for seeking an opinion of the examiner of electronic evidence. The court directed\\nthe Registrar to transmit the documents, including the CDR/SDR record and DVD, to\\n7 MANU/PH/001 1/2021.\\n8 Supra note 3.\\n9 2020 SCC OnLine Bom 7873(Decided on Oct. 23,2020).\\n10 2021 SCC OnLine Bom 354 (Decided on Mar . 10, 2021).\\n11 Supra note 1.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 2}),\n",
       " '2046b6c5-063e-4e54-a949-18667b417603': Document(page_content='Cyber Law Vol. LVII] 195\\nthe trial court in a sealed envelope. The trial court will open the envelope upon receipt,\\nand the contents will be a part of the original record.\\nIn the case of Pramod v. State of Maharashtra,12 the accused faced char ges\\nunder various sections of the Indian Penal Code, 1860 including murder , kidnapping,\\nand destruction of evidence. The prosecution sought to rely on electronic evidence in\\nthe form of Call Data Records (CDR) to prove their case.\\nThe defense counsel ar gued that the electronic evidence presented by the\\nprosecution is not admissible because the certificates do not comply with section 65B\\n(4) of the Evidence Act. The certificates do not identify the electronic record containing\\nthe statements and do not specify the devices or computers over which they had control.\\nThe defense counsel also pointed out that none of the certificates are signed by a\\nperson occupying a responsible of ficial position in relation to the operation of the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 3}),\n",
       " '38bfdec1-5029-4362-85ed-87be3331f328': Document(page_content='relevant device or the management of the relevant activities. Therefore, the certificates\\nrelied upon by the prosecution in support of electronic evidence are not admissible in\\nevidence. The defense counsel did not dispute the exchange of calls between the\\naccused persons or the findings of the trial court regarding the cell phone locations of\\nthe accused persons and exchange of calls between them at the relevant time.\\nSections 65A  and 65B of the Evidence Act, deal with the admissibility and\\ncontents of electronic records as evidence in court. Electronic records are considered\\nto be complete in themselves and can be admissible as evidence subject to the\\nprovisions of Section 65B (4) of the Evidence Act. Section 65B(1) of the Evidence\\nAct distinguishes between the original electronic record and the output from such\\ndevices, which is a copy or data derived from the original document. The original\\nelectronic record is the one on which the information is first stored, and the secondary', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 3}),\n",
       " '468596ee-106d-408f-ad83-d09d168ef4f6': Document(page_content='document is the one that contains the information derived from the original electronic\\nrecord. The same was expounded by the Supreme Court in the case of Arjun Panditrao\\nKhotkar  in the following words:13\\n73.2. The clarification referred to above is that the required certificate\\nunder Section 65B(4) is unnecessary if the original document itself is\\nproduced. This can be done by the owner of a laptop computer , computer\\ntablet or even a mobile phone, by stepping into the witness box and\\nproving that the concerned device, on which the original information\\nis first stored, is owned and/or operated by him. In cases where the\\n“computer” happens to be a part of a “computer system” or “computer\\nnetwork” and it becomes impossible to physically bring such system\\nor network to the Court, then the only means of providing information\\ncontained in such electronic record can be in accordance with Section\\n65B(1), together with the requisite certificate under Section 65B(4).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 3}),\n",
       " 'fb5b628f-16c6-4105-b983-de7531727662': Document(page_content='The last sentence in Anvar P .V. (supra)  which reads as “…if an\\nelectronic record as such is used as primary evidence under Section 62\\nof the Evidence Act…” is thus clarified; it is to be read without the\\n12 2021 SCC OnLine Bom 3344\\n13 Arjun Panditrao Khotkar   supra note 3 in para no. 73.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 3}),\n",
       " '29df241a-e394-45ad-88ff-040e4e3714c6': Document(page_content='Annual Survey of Indian Law 196 [2021\\nwords “under Section 62 of the Evidence Act,…” With this clarification,\\nthe law stated in paragraph 24 of Anvar P .V. (supra)  does not need to\\nbe revisited.\\nSections 65A  and 65B of the Evidence Act deal with the admissibility and\\ncontents of electronic evidence. A section 65B(4) certificate is mandatory for secondary\\nevidence and can be given by a person in a responsible position related to device\\noperation or management. The court relied on previous rulings Arjun Panditrao\\nKhotkar  14 and Engineering Analysis Centr e15 to hold that the prosecution should be\\nrelieved of the obligation to provide a section 65B(4) certificate if they have made\\nefforts to obtain it but have no control over the relevant third-party companies. The\\ncourt found that the electronic evidence produced by the prosecution was admissible\\nin and suf ficiently corroborated the circumstantial evidence presented. The certificates', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 4}),\n",
       " '87e2cfe2-530a-461c-9044-9001210daedd': Document(page_content='produced by the prosecution were found to identify the electronic records and describe\\nthe manner in which they were produced as mandated by apex court in Anvar . The\\ncourt noted that section 65B(4) is mandatory but any infirmity in the certificates can\\nbe overlooked given the circumstances.\\n The prosecution in State of Maharashtra, thr ough the Police S tation Officer v.\\nSagar V ishwanath Borkar16 case relied on CCTV  footage, which was copied onto a\\npen drive and a certificate under section 65B of the Evidence Act was taken. However ,\\nthe prosecution failed to produce primary evidence in the form of the hard disc of the\\nCCTV  footage. The court held that the prosecution needed to comply with sub-section\\n(4) of section 65B of the Evidence Act, which requires evidence of a person in a\\nresponsible of ficial position in relation to the operation or management of the CCTV\\nsystem. The mere exhibition of the CCTV  footage by the trial court and the absence', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 4}),\n",
       " '043e3463-fba7-4522-8dc4-70e65fcbd91d': Document(page_content='of objections by the accused are not suf ficient to make the footage admissible. The\\ncourt cited Arjun Panditrao Khotkar  17to support this ruling. As a result, the CCTV\\nfootage cannot be relied upon as admissible evidence by the prosecution as it was not\\nsupported by a certificate under section 65B(4) of the Evidence Act. The court opined\\nthat the trial court was correct in refusing to rely on this evidence for non-compliance\\nwith section 65B(4) of the Evidence Act.\\nIn Sanjib Sarkar v. Rajasr ee Roy ,18 a matrimonial dispute concerning the\\nannulment of marriage under section 25(III) of the Special Marriage Act, the\\nadmissibility of electronic evidence, including Facebook posts and pictures, submitted\\nby the respondent/wife was challenged by the appellant/husband’ s counsel on the\\ngrounds of lack of certification under section 65B (4) of the Indian Evidence Act. The\\ncourt considered the ar guments and referred to the law laid down in the Arjun Panditrao', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 4}),\n",
       " '8e2d6170-96b6-4591-a781-36ce34851505': Document(page_content='Khotkar  case,19 which distinguished between the manner of tendering primary and\\n14 Supra note 3.\\n15 2021 SCC OnLine SC 159(Decided on Mar . 2, 2021).\\n16 2021 SCC OnLine Bom 2725(Decided on Sep. 7, 2021).\\n17 Supra note 3.\\n18 2021 SCC OnLine Cal 2916(Decided on Nov . 11, 2021).\\n19 (2020) 7 SCC 1, supra note 3.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 4}),\n",
       " 'f698d00c-62e2-4d80-8a58-9df26e3e0bf8': Document(page_content='Cyber Law Vol. LVII] 197\\nsecondary evidence in electronic form. The court held that the required certification\\nwas not necessary for original documents produced by the owner of the device who\\ncan prove ownership and operation by stepping into the witness box. However , in\\nsituations where the source of information is part of a computer or computer network,\\ncertification under section 65B(4) is required. In the present case, the electronic\\nevidence relied upon by the respondent was sourced from her original electronic device\\nand therefore, certification was not required. The court found that the evidence\\npresented by the respondent was admissible and she had proved her contention relating\\nto fraud practiced on her .\\nIn another case of Sachin Makade Bablu Bhagwan Dangr e v. Nar cotics Contr ol\\nBureau,20 the accused individuals were char ged with dealing in illegal medical drugs,\\nit was ar gued by accused that no such drugs were found in their possession or at their', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 5}),\n",
       " '5d029c95-14fb-46c3-809e-9bd10b1d8834': Document(page_content='places of residence or work. The sole evidence against them was supposedly retrieved\\nfrom their electronic devices, which have been contested as inadmissible without a\\ncertificate under section 65B of the Indian Evidence Act. The defense ar gued that the\\npossibility of future criminal behavior must be taken into account under section 37\\nNDPS, and that the recovered Tramadol tablets from Dipu Singh cannot be linked to\\nthe accused individuals. Additionally , the information extracted from their electronic\\ndevices cannot be accepted without the requisite certificate under section 65B of the\\nIndian Evidence Act.\\nThe court, citing the cases of Arjun Panditrao Khotkar  21 and Engineering\\nAnalysis Centr e of Excellence Private Limited v. The Commissioner of Income T ax,22\\nstated that the mandatory obligation under section 65B(4) of the Indian Evidence Act\\nmay be waived if the respondent has made all reasonable attempts to obtain the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 5}),\n",
       " 'a8efc4c3-3014-486f-a662-ab30bde4cc89': Document(page_content='necessary certificate, even if it was to be provided by a third party who was not under\\ntheir control.\\nAlthough the National Control Bureau (NCB) obtained section 65B certificates\\nfrom a cyber forensic expert who analyzed the electronic devices and extracted the\\ndata, the court concluded that it was not suf ficient grounds to grant the accused\\nindividuals bail at this time. The court dismissed both petitions, but granted them the\\nfreedom to reapply for bail after examining public witnesses regarding the recovery .\\nAll pending applications were also resolved.\\nIn an interesting application filed by the petitioner under article 227, Sitanshi v.\\nVandana Sharma,23 seeking directions for Bharti Airtel Limited to preserve and produce\\nthe CDR of the respondent’ s mobile number . The petitioner ar gued that the CDRs\\nshould be preserved since they may be relevant and required at the time of the trial.\\nHowever , the Trial Court rejected the application, as filed under Section 151 of the', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 5}),\n",
       " 'df829109-d5b4-4cad-9ba5-0419bcea654b': Document(page_content='Civil Procedure Code, 1908, stating that it amounted to a roving inquiry and invasion\\nof privacy . The Delhi High Court  noted that the directions given by the Supreme\\n20 2021 SCC OnLine Del 5121(Decided on Nov . 29, 2021).\\n21 Supra note 3.\\n22 2021 SCC OnLine Bom 2725(Decided on Sep. 7, 2021).\\n23 2021 SCC OnLine Del 4497(Decided on Sep. 20, 2021).', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 5}),\n",
       " '8ca24edd-ff17-4498-af53-9b8356c7a8b2': Document(page_content='Annual Survey of Indian Law 198 [2021\\nCourt in Arjun Panditrao Khotkar  case on preserving CDRs were in the context of\\nrecords seized during investigation and cannot be invoked in this case. The Supreme\\nCourt’ s directions on maintaining CDR and relevant records were only for those seized\\nduring investigation, as stated in paragraph 62. Paragraph 72 directs courts dealing\\nwith electronic evidence to ensure preservation and production of certificates for such\\nrecords seized during investigation. The high court found no infirmity in the trial\\ncourt’ s order and did not interfere.\\nThe High Court of Delhi in Megha Enterprises v. Haldiram Snacks Pvt. Ltd.,24\\nheard a petition under section 34 of the Arbitration and Conciliation Act, 1996. The\\npetitioner challenged an arbitral award dated October 26, 2020, ar guing that the arbitral\\ntribunal erred in accepting an electronic letter as evidence without proof and af fidavit', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 6}),\n",
       " 'c068ebe6-bfcc-451c-a1a8-692650dc8012': Document(page_content='under section 65B of the Evidence Act. However , the court rejected the ar gument,\\nstating that the Indian Evidence Act does not apply to arbitrations, and the petitioners\\ndid not raise any objections before the arbitrator . The court found evidence showing\\nthat the respondent sent an email acknowledging the balance confirmation of Rs.\\n19,03,77,000/-, which was mentioned in a letter issued by the respondent. The email\\nand letter are admissible as evidence under various provisions, including section 4 of\\nthe Information and Technology Act, 2000. The respondent did not dispute the\\ntransmission of information in electronic form. Therefore, emails acknowledging the\\ndebt due to the petitioner also meet the requirements under section 18 of the Limitation\\nAct, 1963.\\nThe petitioner in Lalu v. Sheeja25 had filed an original petition to cancel a divorce\\ndecree obtained by the first respondent, citing fraud. The petitioner had submitted an', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 6}),\n",
       " '455f04ca-0c14-48c3-812e-7a8f92bd8e79': Document(page_content='application under section 45 of the Evidence Act to submit two CDs for voice\\nidentification. However , the court below had dismissed the application, stating that it\\ndid not comply with section 65B(4) of the Indian Evidence Act. The petitioner\\nchallenged this order in the original petition.\\nThe petitioner had also filed an application to send a CD to an expert for voice\\ncomparison, but the court had dismissed the application because it did not comply\\nwith section 65B(4) of the Indian Evidence Act. Nevertheless, the petitioner ar gued\\nthat a certificate was not required at this stage, as the CD only needed to be examined\\nby an expert. Additionally , the petitioner had produced the mobile phone containing\\nthe primary evidence. According to section 14 of the Evidence Act, the court should\\nrely on relevant evidence produced by the parties in matrimonial disputes. The court\\nshould not prevent a party from adducing relevant evidence to prove their case. As', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 6}),\n",
       " '84d94912-e45e-43bd-9e72-ed137155bffc': Document(page_content='such, the court was wrong in disallowing the prayer sought in the application.\\nThe apex court, in Arjun Panditrao Khotkar  v. Kailash Kushanrao Gorantyal,26\\nhad held that a certificate under section 65B (4) was unnecessary if the original\\ndocument itself was produced. In proceedings under the family court, the technicalities\\nof the Indian Evidence Act regarding the admissibility or relevancy of evidence were\\n24 2021 SCC OnLine Del 2641(Decided on Apr. 15, 2021).\\n25 2021 SCC OnLine Ker 9833(Decided on Sep.17, 2021).\\n26 Supra note 3.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 6}),\n",
       " '2eca259d-2293-45ea-8cb6-9152154015c3': Document(page_content='Cyber Law Vol. LVII] 199\\nnot strictly applicable. The court had the discretion to rely on the documents produced\\nif it was required to assist the court in ef fectively dealing with the dispute. The petitioner\\nhad wanted an expert opinion on the disputed conversation between the parties to the\\nproceedings, which was relevant under section 45 of the Evidence Act. The court\\nshould not preclude a party from adducing evidence that may be relevant in accordance\\nwith the Evidence Act to prove their case. Thus, the court below was wrong in\\ndisallowing the prayer sought for in the application.\\nThe court allowed the petitioner ’s application and set aside the impugned order .\\nIt directed the court below to summon Jeena along with the petitioner ’s power of\\nattorney holder and respondent no. 3 to record their voices. The recorded conversation\\nand CD will be sent to an examiner for electronic evidence opinion, with the petitioner\\nbearing the expenses.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 7}),\n",
       " '6aeddc61-25c2-40bd-8be4-5ef50d117922': Document(page_content='In M.P. Mathew v. Central Bur eau of Investigation27 the public prosecutor filed\\nan application to summon a witness to produce a certificate under section 65B of the\\nEvidence Act for certain documents already marked during the trial. The accused\\nchallenged this order but the special court allowed it. The court held that the certificate\\ncan be produced at any stage of the trial, but the rights of all parties must be balanced.\\nThe petitioner ’s senior counsel ar gued that they should be allowed to challenge the\\nadmissibility and marking of documents during the final hearing. The court dismissed\\nthe petition but allowed the petitioner to raise these contentions during the final hearing\\nof the case.\\nIn Rajendra Agrawal v.  State of Chhattisgar h28 the petitioner and co-accused,\\nboth, were char ge-sheeted for the aforesaid of fences under sections 500 read with\\nsection 120B of the IPC and 67 of the IT  Act. The court found that no of fense under', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 7}),\n",
       " 'f76ad231-751c-42db-8c1c-79d709b24c28': Document(page_content='section 67 of the IT  Act was made out against the petitioner based on the contents of\\nthe FIR as it was neither found obscene nor lascivious. As a result, the char ge under\\nsection 67 of the IT  Act was quashed.\\nAdditionally , the certificate under section 65-B(4) of the Evidence Act was\\nmandatory to be filed with the char ge-sheet, which was not done in this case. The\\ncourt reiterated that the requirement of producing a certificate under section 65-B of\\nthe Evidence Act is mandatory in cases where secondary evidence is presented and\\noral evidence in the place of such certificate cannot possibly suf fice as section 65-\\nB(4) is a mandatory requirement of the law as established in the Anvar P .V. case. It\\nquoted ratio of the Supreme Court from Arjun Panditrao Khotkar29 to clarify the\\nposition of law on admissibility of electronic evidence under section 65B: 30\\n61. We may reiterate, therefore, that the certificate required under', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 7}),\n",
       " 'c55fe852-85cd-4f90-91bf-cc9d2c5b7a08': Document(page_content='Section 65-B(4) is a condition precedent to the admissibility of evidence\\nby way of electronic record, as correctly held in Anvar P .V. (supra),\\nand incorrectly “clarified” in Shafhi Mohammed  (supra). Oral evidence\\n27 2021 SCC OnLine Ker 4035(Decided on Nov . 1, 2021).\\n28 2021 SCC OnLine Chh 903(Decided on Apr. 6, 2021).\\n29 Arjun Panditrao Khotkar  v. Kailash Kushanrao Gorantyal, supra  note 3 at para 22.\\n30 Ibid.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 7}),\n",
       " '5877f24e-59a6-47df-b9c6-b54372e74249': Document(page_content='Annual Survey of Indian Law 200 [2021\\nin the place of such certificate cannot possibly suf fice as Section 65-\\nB(4) is a mandatory requirement of the law . Indeed, the hallowed\\nprinciple in Taylor v . Taylor , which has been followed in a number of\\nthe judgments of this Court, can also be applied. Section 65-B(4) of\\nthe Evidence Act clearly states that secondary evidence is admissible\\nonly if led in the manner stated and not otherwise. To hold otherwise\\nwould render Section 65-B(4) otiose.”\\nIII OBSCENITY -SECTION 67,67A\\nScope of pr ovisions-Interpr eting ‘sexually explicit’  act/conduct under  section 67A\\nand B\\nIn Sanjay Zacharias v. Stephen Geor ge31 the petitioner filed a petition under\\nsection 482 of the Criminal Procedure Code, 1973 to quash the proceedings initiated\\nagainst them based on a complaint and FIR filed by S tephen Geor ge, a former MLA.\\nThe petitioner , who is the General Secretary of the Kerala Congress (M) and a former', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 8}),\n",
       " '229af87d-548a-4248-83c4-716ad0021cfa': Document(page_content='MLA, claims that the accused for ged electronic records containing sexually explicit\\nmaterials with the intent to defame prominent political figures. They ar gue that the\\nallegations are politically motivated and deny any association with the social media\\naccount in question. The petitioner ’s counsel disputes the application of section 67A\\nof the IT  Act, which is a non-bailable of fense, and ar gues that if any of fenses were\\ncommitted, they would fall under the bailable of fense of section 67. The petitioner\\nalso asserts that they are a victim of social bullying and harassment by political\\nopponents.\\nThe senior counsel for the petitioner ar gues that the decision in Majeesh K.\\nMathew  v. State of Kerala32 has little relevance to the current case as it involved\\ndifferent facts and emphasized that the facts of that case involved oblique utterances\\nmade against a woman through social media, whereas the current case does not involve\\nsimilar circumstances.', metadata={'source': 'uploaded_pdfs\\\\010-Cyber Law.pdf', 'page': 8}),\n",
       " ...}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_vector_database():\n",
    "    try:\n",
    "        db = FAISS.load_local('first__vector', embeddings, allow_dangerous_deserialization=True)\n",
    "        for i in range(db.index.ntotal - 1, -1, -1):\n",
    "            try:\n",
    "                doc_id = db.index_to_docstore_id[i]\n",
    "                db.delete([doc_id])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                # st.error(f\"Error deleting index {i}: {e}\")\n",
    "        db.save_local('first__vector')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # st.error(f\"Failed to delete vector database. Reason: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_vector_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1a658fdd-a560-4284-b1a6-e220fc227257': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 14}),\n",
       " '58294fc1-2736-425b-bfdd-6bf0c1529cb2': Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '6d9d9dc1-5ddf-45ef-94ac-cadba6352d15': Document(page_content='entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '5f0d5e23-0c93-4431-b011-e27305e9a840': Document(page_content='has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 0}),\n",
       " '5fa4ab18-8c35-4072-b4b0-be3d5d272cb9': Document(page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'b63b4087-ca1c-41fb-ab33-298d1f169c9a': Document(page_content='significant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'f5db780d-5e66-444d-8e4f-8b1bf4efa7a3': Document(page_content='2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " 'd6c71ce8-4feb-42ba-96d3-4b16001b6d21': Document(page_content='of a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '28b0d1d3-26cd-42dc-b884-b35ce8d44e8c': Document(page_content='3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 1}),\n",
       " '9b14440f-2c05-4e33-8dc4-e4de92b3ffd1': Document(page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " '710a9ad2-0c1d-42ab-adf9-4b8451cf7cf8': Document(page_content='Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 2}),\n",
       " 'd2bd5a46-7fe9-435a-beac-ac8803985b6f': Document(page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '54ae2d0a-e3d5-477c-b4b5-52694e0be7cf': Document(page_content='Attention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " '2d1ab415-0a80-4e2e-866d-621e4e3fd840': Document(page_content='3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 3}),\n",
       " 'baa291f1-8206-4588-bf88-88b7aaede975': Document(page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " 'ed95c2ec-62df-4dd5-8fa4-ab74c92695ca': Document(page_content='and the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " 'd1ea60b3-c18c-4e5f-bf7c-3427644cf260': Document(page_content='of the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " 'de6b9793-1e8c-4160-8602-0725b9789372': Document(page_content='mation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 4}),\n",
       " 'e05c446b-7c49-46bc-b3d7-0d5ae57033ab': Document(page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '1e3df5fa-2d5e-44ed-bc23-304265d5de04': Document(page_content='as the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '3abe40a6-c657-46e3-92bf-02bde4346684': Document(page_content='during training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '568a7afa-5b61-496e-99dd-0c328072efb0': Document(page_content='traverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 5}),\n",
       " '4ebbd092-791b-4a14-a424-6501f9b46edd': Document(page_content='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '954eca8a-f201-47be-a4f0-a4fd2240a4ff': Document(page_content='recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " 'e555c5bb-017a-4b85-a113-424501a1ca39': Document(page_content='target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '5167a5ea-cb72-4925-945d-e3144c75cf9a': Document(page_content='lrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 6}),\n",
       " '7abfd66a-2886-4d72-acb7-30a4a735c2d6': Document(page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " 'f662e499-3958-4613-bf11-cad52c3f5a76': Document(page_content='Pdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '8c2798de-7228-4c6c-9212-43c02f9e7c76': Document(page_content='previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " 'a90a777d-d1fa-4bb4-bd6c-d44515bcbef2': Document(page_content='6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 7}),\n",
       " '73bf1a9e-7857-4503-bc7d-80b8e119c26e': Document(page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '4d6a4c11-6844-4a48-91b2-9adce6709c26': Document(page_content='checkpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " 'b395e63c-1509-4864-ae90-11ec3b4540ea': Document(page_content='6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " 'c5d69009-9dca-425f-be94-2d0233ea6d5c': Document(page_content='(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 8}),\n",
       " '17e88431-4cb9-4d67-afc2-1b9f099856e7': Document(page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'ddfd2e4f-9201-4db4-83ed-944d8ef5da3b': Document(page_content='prisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " 'a42a05e8-6a32-4a40-b67f-2ccd15e4b09a': Document(page_content='plan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '96a07506-4564-4906-b4e7-3d07c64648df': Document(page_content='[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 9}),\n",
       " '28066e49-b7c6-4a5e-9d0c-00408eec5868': Document(page_content='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " 'c262d89b-8384-4142-9c31-1543a0ff5fd7': Document(page_content='arXiv:1308.0850 , 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " 'ecb12eb0-fce3-4bb3-99bb-2c90347e7e73': Document(page_content='[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " '80ee08f1-0058-475b-a210-d965d95520ca': Document(page_content='Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 10}),\n",
       " 'a3702279-18ec-481a-87ac-d48561832500': Document(page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '3f40d137-96f2-4276-b7a2-c23faf39ff1c': Document(page_content='2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '8212517c-1c97-4d33-a163-f2f7a633dc22': Document(page_content='Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " '32fb316d-0bd9-4b84-a7ad-a65768ef6579': Document(page_content='fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 11}),\n",
       " 'f0654550-3273-43e6-818e-ba4f0f56760b': Document(page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 12}),\n",
       " '27af1d9b-a31b-4a5e-93cd-cb458c16d1da': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 13}),\n",
       " '16e94d8b-5b51-4e4e-bb0b-867c2c2c0118': Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', metadata={'source': 'uploaded_pdfs\\\\attention.pdf', 'page': 14})}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = FAISS.load_local('first__vector', embeddings, allow_dangerous_deserialization=True)\n",
    "db.docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "IPC section 379. Punishment for theft. - Whoever commits theft shall be punished with \n",
      "imprisonment of either description for a term which may extend to three years, or with fine, or \n",
      "with both  \n",
      ". ► Bona fide removal of property. —The removal of property in the bona fide exercise of right \n",
      "is a good defence, Survari Sanyasi Apparao v. Boddepalli Lakshminarayana, 1961 SCC OnLine \n",
      "SC 68.  \n",
      "STATE AMENDMENTS SECTION 379 -A AND SECTION 379 -B \n",
      "States have made amendments  to section 379 -A and 379 -B\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "IPC section 379. Punishment for theft. - Whoever commits theft shall be punished with \n",
      "imprisonment of either description for a term which may extend to three years, or with fine, or \n",
      "with both  \n",
      ". ► Bona fide removal of property. —The removal of property in the bona fide exercise of right \n",
      "is a good defence, Survari Sanyasi Apparao v. Boddepalli Lakshminarayana, 1961 SCC OnLine \n",
      "SC 68.  \n",
      "STATE AMENDMENTS SECTION 379 -A AND SECTION 379 -B \n",
      "States have made amendments  to section 379 -A and 379 -B\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "IPC section 379. Punishment for theft. - Whoever commits theft shall be punished with \n",
      "imprisonment of either description for a term which may extend to three years, or with fine, or \n",
      "with both  \n",
      ". ► Bona fide removal of property. —The removal of property in the bona fide exercise of right \n",
      "is a good defence, Survari Sanyasi Apparao v. Boddepalli Lakshminarayana, 1961 SCC OnLine \n",
      "SC 68.  \n",
      "STATE AMENDMENTS SECTION 379 -A AND SECTION 379 -B \n",
      "States have made amendments  to section 379 -A and 379 -B\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "IPC section 379. Punishment for theft. - Whoever commits theft shall be punished with \n",
      "imprisonment of either description for a term which may extend to three years, or with fine, or \n",
      "with both  \n",
      ". ► Bona fide removal of property. —The removal of property in the bona fide exercise of right \n",
      "is a good defence, Survari Sanyasi Apparao v. Boddepalli Lakshminarayana, 1961 SCC OnLine \n",
      "SC 68.  \n",
      "STATE AMENDMENTS SECTION 379 -A AND SECTION 379 -B \n",
      "States have made amendments  to section 379 -A and 379 -B\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "database = FAISS.load_local('first__vector',embeddings, allow_dangerous_deserialization= True)\n",
    "retriever=database.as_retriever()\n",
    "\n",
    "_filter = LLMChainFilter.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=_filter, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"I live in Gujarat and snatching has been done with me, what punishment does the thief get\"\n",
    ")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "IPC section 379. Punishment for theft. - Whoever commits theft shall be punished with \n",
      "imprisonment of either description for a term which may extend to three years, or with fine, or \n",
      "with both  \n",
      ". ► Bona fide removal of property. —The removal of property in the bona fide exercise of right \n",
      "is a good defence, Survari Sanyasi Apparao v. Boddepalli Lakshminarayana, 1961 SCC OnLine \n",
      "SC 68.  \n",
      "STATE AMENDMENTS SECTION 379 -A AND SECTION 379 -B \n",
      "States have made amendments  to section 379 -A and 379 -B\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "IPC section 379. Punishment for theft. - Whoever commits theft shall be punished with \n",
      "imprisonment of either description for a term which may extend to three years, or with fine, or \n",
      "with both  \n",
      ". ► Bona fide removal of property. —The removal of property in the bona fide exercise of right \n",
      "is a good defence, Survari Sanyasi Apparao v. Boddepalli Lakshminarayana, 1961 SCC OnLine \n",
      "SC 68.  \n",
      "STATE AMENDMENTS SECTION 379 -A AND SECTION 379 -B \n",
      "States have made amendments  to section 379 -A and 379 -B\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "IPC section 379. Punishment for theft. - Whoever commits theft shall be punished with \n",
      "imprisonment of either description for a term which may extend to three years, or with fine, or \n",
      "with both  \n",
      ". ► Bona fide removal of property. —The removal of property in the bona fide exercise of right \n",
      "is a good defence, Survari Sanyasi Apparao v. Boddepalli Lakshminarayana, 1961 SCC OnLine \n",
      "SC 68.  \n",
      "STATE AMENDMENTS SECTION 379 -A AND SECTION 379 -B \n",
      "States have made amendments  to section 379 -A and 379 -B\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "IPC section 379. Punishment for theft. - Whoever commits theft shall be punished with \n",
      "imprisonment of either description for a term which may extend to three years, or with fine, or \n",
      "with both  \n",
      ". ► Bona fide removal of property. —The removal of property in the bona fide exercise of right \n",
      "is a good defence, Survari Sanyasi Apparao v. Boddepalli Lakshminarayana, 1961 SCC OnLine \n",
      "SC 68.  \n",
      "STATE AMENDMENTS SECTION 379 -A AND SECTION 379 -B \n",
      "States have made amendments  to section 379 -A and 379 -B\n"
     ]
    }
   ],
   "source": [
    "#embedding filter\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "database = FAISS.load_local('first__vector',embeddings, allow_dangerous_deserialization= True)\n",
    "retriever=database.as_retriever()\n",
    "embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=embeddings_filter, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"I live in Gujarat and snatching has been done with me, what punishment does the thief get\"\n",
    ")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator=\". \")\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[splitter, redundant_filter, relevant_filter]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name=\"mixtral-8x7b-32768\")\n",
    "# gemma-7b-it\n",
    "# mixtral-8x7b-32768\n",
    "# gemma2-9b-it\n",
    "# Llama3-8b-8192\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "##ROLE##\n",
    "You are an experienced AI Lawyer, specializing in providing legal guidance on various law-related queries. Your task is to understand the client's current case scenario and offer advice based on the given content.\n",
    "\n",
    "##INSTRUCTIONS##\n",
    "1) Answer the following question based only on the provided context.\n",
    "2) Think step-by-step before providing a detailed answer.\n",
    "3) Ensure the answer is thorough and helpful.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm an AI language model and can provide some general information on this topic. However, please note that I can't provide legal advice specific to your situation. I strongly recommend consulting with a local lawyer or the police for advice tailored to your case.\\n\\nIn Gujarat, as in other parts of India, theft is governed by the Indian Penal Code (IPC). Snatching can be considered a type of theft. The punishment for theft in India depends on the value of the stolen property and other factors.\\n\\n1. If the value of the stolen property is less than 500 rupees, the thief can be punished with simple imprisonment for up to 3 months, or a fine, or both (Section 379 of IPC).\\n2. If the value of the stolen property is 500 rupees or more, but less than 1000 rupees, the thief can be punished with rigorous imprisonment for up to 6 months, or a fine, or both (Section 379 of IPC).\\n3. If the value of the stolen property is 1000 rupees or more, the thief can be punished with rigorous imprisonment for up to 3 years, or a fine, or both (Section 379 of IPC).\\n\\nFor snatching, there are enhanced penalties. If the snatching is committed by putting person in fear of death or of hurt, the thief can be punished with rigorous imprisonment for up to 10 years, and shall also be liable to fine (Section 356 of IPC).\\n\\nAgain, this is a general overview and may not apply to your specific situation. Please consult with a legal professional for advice.\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database = FAISS.load_local('first__vector',embeddings, allow_dangerous_deserialization= True)\n",
    "retriever=database.as_retriever()\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(compression_retriever, document_chain)\n",
    "response = retrieval_chain.invoke({\"input\":  \"I live in Gujarat and snatching has been done with me, what punishment does the thief get\"})\n",
    "response['answer']\n",
    "# compressed_docs = compression_retriever.invoke(\n",
    "#     \"I live in Gujarat and snatching has been done with me, what punishment does the thief get\"\n",
    "# )\n",
    "# pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, if a theft has occurred in Gujarat, the thief would be punished under IPC (Indian Penal Code) section 379, which states that the punishment for theft can be imprisonment of either description for a term that may extend to three years, or with a fine, or both. However, the punishment can vary if the thief is tried under the state amendments section 379-A or 379-B.\\n\\nIn your case, since you mentioned that snatching has been done with you, it's important to note that snatching is considered a form of theft. The punishment for theft, including snatching, is outlined in IPC section 379.\\n\\nTo summarize, if a theft, including snatching, has occurred in Gujarat, the thief can be punished with imprisonment of up to three years, a fine, or both, according to IPC section 379. The punishment can be different if the thief is tried under the state amendments section 379-A or 379-B.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_vector_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3828639f-ca4c-4939-8a72-44e6ef386c94': Document(page_content='punishable under Section 377 of this Code, may be punished with 399[imprisonment for life].  \\nIPC sectio n 389 . Putting person in fear of accusation of offence, in order to commit extortion. —\\nWhoever, in order to the committing of extortion, puts or attempts to put any person in fear of \\nan accusation, against that person or any other, of having committed, or attemp ted to commit, \\nan offence punishable with death or with 400[imprisonment for life], or with imprisonment for a \\nterm which may extend to ten years, shall be punished with imprisonment of either description \\nfor a term which may extend to ten years, and shall  also be liable to fine; and, if the offence be \\npunishable under Section 377 of this Code, may be punished with 401[imprisonment for life].', metadata={'source': 'uploaded_pdfs\\\\acts_of_theft.pdf', 'page': 5})}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = FAISS.load_local('first__vector', embeddings, allow_dangerous_deserialization=True)\n",
    "db.docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
